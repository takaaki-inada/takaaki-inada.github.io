---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on/audio/株式会社ずんだもん技術室AI放送局_podcast_20240909.mp3
audio_file_size: 0
date: 2024-09-09 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
死語になったネットスラング、『プロジェクトKV』の中止が発表。問題と騒動で心配をかけたことを深くお詫び、『プロジェクトKV』を応援したファンならびに配信中のゲームファンにも迷惑をかけたとして関連の資料はすべて削除へ、Reflection Llama-3.1 70B を試す｜ぬこぬこ'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20240909
information: 
---

## 関連リンク


- [死語になったネットスラング](https://anond.hatelabo.jp/20240907225609)  


この記事では、かつてインターネット上で広く使われていたネットスラングが、時間の経過とともに使われなくなり、死語となった例を紹介しています。

具体的には、「キボンヌ」「あぼーん」「香具師」「リア充」「うp」など、2ちゃんねるやブログ全盛期に流行した言葉が挙げられています。これらの言葉は、当時のインターネット文化を象徴するものでしたが、利用者の変化や新しいプラットフォームの登場などにより、次第に姿を消していきました。

また、記事では「乙」「〜なう」「ぬるぽ」など、現在でも一部で使われているものの、かつてほど一般的ではなくなったスラングも紹介されています。

ネットスラングは、時代やコミュニティによって流行や廃りが激しく、常に変化していることが分かります。この記事は、かつてのインターネット文化を振り返り、言葉の流行と変化について考えるきっかけを与えてくれます。

エンジニアの皆さんも、日々の業務やコミュニケーションの中で、新しい言葉や表現に触れる機会があるかと思います。この記事で紹介されているような、かつて流行した言葉を知ることで、より幅広い世代やコミュニティとのコミュニケーションを円滑に進めることができるかもしれません。




引用元: https://anond.hatelabo.jp/20240907225609


- [『プロジェクトKV』の中止が発表。問題と騒動で心配をかけたことを深くお詫び、『プロジェクトKV』を応援したファンならびに配信中のゲームファンにも迷惑をかけたとして関連の資料はすべて削除へ](https://news.denfaminicogamer.jp/news/240908b)  



Dynamis Oneは、開発中の新作ゲーム『プロジェクトKV』の中止を発表しました。これは、開発中に発生した問題や騒動により、ファンや他のゲームプレイヤーに迷惑をかけたことを深くお詫びし、これ以上の迷惑をかけないための決断とのことです。

『プロジェクトKV』は、『ブルーアーカイブ』の製作に携わったメンバーが設立したDynamis Oneの新作で、注目を集めていました。コミケへの出展も予定されていましたが、発表からわずか1週間での中止発表となりました。

Dynamis Oneは、公式発表で問題と騒動への謝罪と、未熟さゆえの中止を決断したことを表明しています。さらに、関連資料はすべて削除し、ファンへの迷惑を最小限に抑える対応をとるとしています。

今後については、今回の反省を活かし、ファンに期待に応えられるよう精進していくとメッセージで述べています。公式サイトは現時点では閲覧可能ですが、公式Xアカウントの過去投稿は削除され、YouTubeの関連動画は非公開となっています。

今回の発表は、ゲーム業界において注目を集めており、今後のDynamis Oneの動向が注目されています。新人エンジニアの皆さんも、今回の件を通して、開発における責任や、ユーザーへの配慮の重要性を学ぶ良い機会になるのではないでしょうか。 


引用元: https://news.denfaminicogamer.jp/news/240908b


- [Reflection Llama-3.1 70B を試す｜ぬこぬこ](https://note.com/schroneko/n/nae86e5d487f1)  


Reflection Llama-3.1 70Bは、HyperWriteのCEO Matt Shumer氏によって公開された、オープンソースの大規模言語モデルです。Llama 3.1 70Bをベースに、Reflection-Tuningという手法で事後学習されており、世界最高性能のオープンモデルと謳われています。

**Reflection-Tuning**は、LLM自身が自身の推論結果の誤りを修正できるようにする技術です。推論過程では、`<thinking>`タグで推論内容、`<output>`タグで最終的な回答、そして必要に応じて`<reflection>`タグで自己修正内容を出力します。これにより、LLMの推論過程を可視化し、より信頼性の高い回答を得ることが期待できます。

このモデルは、様々なベンチマークにおいて、GPT-4oを含むトップクラスのプロプライエタリモデルに匹敵する、あるいは上回る性能を示しています。特に、数学問題や論理的推論、常識的な知識を問うタスクで高い精度を達成しています。

Hugging Faceで公開されているモデルは、Llama 3.1 70B Instructをベースとし、チャット形式で利用できます。Ollamaなどのツールを用いれば、比較的容易にローカル環境で推論を実行可能です。

**特徴**

* 世界最高性能のオープンソースLLM
* Reflection-Tuningによる自己修正機能
* `<thinking>`、`<output>`、`<reflection>`タグによる推論過程の可視化
* GPT-4oに匹敵する、または上回る性能
* 日本語を含む多言語対応（ただし、日本語出力は必ずしも得意ではない）

Reflection Llama-3.1 70Bは、その高い性能と推論過程の透明性から、LLMの研究開発や、様々なアプリケーションへの組み込みに役立つ可能性を秘めています。今後、405Bパラメータのモデルも公開される予定であり、さらなる進化が期待されます。




引用元: https://note.com/schroneko/n/nae86e5d487f1



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
