---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on/audio/株式会社ずんだもん技術室AI放送局_podcast_20240902.mp3
audio_file_size: 0
date: 2024-09-02 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
東京大学松尾・岩澤研究室 GENIACプロジェクトにおいて、大規模言語モデル「Tanuki-8×8B」を開発・公開、Llama.cpp で Command-R-plus-08-2024 を試す｜npaka、ソースコードをリポジトリ丸ごとLLMに読んでもらう方法'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20240902
information: 
---

## 関連リンク


- [東京大学松尾・岩澤研究室 GENIACプロジェクトにおいて、大規模言語モデル「Tanuki-8×8B」を開発・公開](https://weblab.t.u-tokyo.ac.jp/2024-08-30/)  


東京大学松尾・岩澤研究室は、経済産業省とNEDOが推進する生成AI基盤モデル開発プロジェクト「GENIAC」において、大規模言語モデル「Tanuki-8×8B」を開発し、公開しました。

**Tanuki-8×8B**は、日本語の対話や作文能力に特化した、80億パラメータのモデルです。開発には、松尾研究室の大規模言語モデル講座の修了生や一般公募で集まった有志が参加し、コンペティション形式で開発が進められました。

**特徴**として、フルスクラッチで開発されたにも関わらず、対話や作文能力を評価する指標「Japanese MT-Bench」において、OpenAIの「GPT-3.5 Turbo」と同等以上の性能を達成しています。さらに、ユーザーとの対話評価においても、GPT-4やGeminiといった海外の先進モデルに匹敵する高い性能を示しました。

**公開内容**としては、Tanuki-8×8B本体に加え、軽量版の「Tanuki-8B」のチャットデモも公開されています。Apache License 2.0のライセンスに基づき、研究・商業目的を問わず自由に利用できます。

**開発の背景**には、国内の生成AI基盤モデル開発の強化と、日本独自の強みを備えたLLMの育成という目的があります。Tanuki-8×8Bは、その取り組みの成果の一つであり、今後の更なる発展が期待されています。

**留意点**として、Tanuki-8×8Bは、総合的な推論能力では海外の最先端モデルにはまだ及ばない点があります。一方で、共感性や思いやりを表現する能力に強みを持つことも明らかになっています。

今回のプロジェクトは、オープンな開発体制で進められ、その過程や知見も公開されています。日本のエンジニアが、生成AI分野の研究開発に貢献できる環境が整備されつつあると言えるでしょう。 


引用元: https://weblab.t.u-tokyo.ac.jp/2024-08-30/


- [Llama.cpp で Command-R-plus-08-2024 を試す｜npaka](https://note.com/npaka/n/n3237af6ce190)  


この記事では、Llama.cppを使ってCohereが開発した大規模言語モデル「Command-R-plus-08-2024」と「Command-R-08-2024」を試した結果をまとめられています。

Command-R-plus-08-2024は、104Bパラメータのモデルで、Command-Rシリーズの最新モデルです。一方、Command-R-08-2024は35Bパラメータのモデルです。

記事では、Llama.cppのビルド方法、モデルのダウンロード方法、そして実際にモデルを実行して質問し、回答を得る様子が紹介されています。

**Command-R-plus-08-2024**と**Command-R-08-2024**はどちらも日本語で質問に答えることができ、異なる回答を生成することが確認できます。

例えば、「まどか☆マギカで誰が一番かわいいか」という質問に対して、Command-R-plus-08-2024は「個人の好みによる」という回答を返した一方、Command-R-08-2024は「鹿目まどか」という回答を返しています。

記事では、それぞれのモデルの実行時間なども掲載されており、モデルの性能を比較する材料にもなっています。

Llama.cppは、大規模言語モデルをローカル環境で実行するためのツールであり、今回の記事のように、様々なモデルを試す際に役立ちます。この記事は、Llama.cppでCommand-Rシリーズのモデルを試したいエンジニアにとって、参考になる情報が詰まっていると言えるでしょう。 


引用元: https://note.com/npaka/n/n3237af6ce190


- [ソースコードをリポジトリ丸ごとLLMに読んでもらう方法](https://zenn.dev/karaage0703/articles/0a546df8941057)  


GitHubなどのリポジトリにある複数のソースコードファイルを、LLM（大規模言語モデル）でまとめて読み込みたい場合、`gpt-repository-loader`や`generate-project-summary`といったツールを使うことで、リポジトリ全体をテキスト化できます。これにより、LLMにプロジェクトの全体像を理解させ、コードに関する質問に答えたり、要約を作成したりすることが可能になります。

**これらのツールは、リポジトリ内のファイル構造やコードをテキストに変換し、LLMが理解しやすい形式で出力します。** 例えば、ファイルパスやファイル名、コードの内容を分かりやすく区切って表示することで、LLMがコードのコンテキストを把握しやすくなります。

**具体的な用途としては、以下のようなものが挙げられます。**

* **LLMへの入力:** ChatGPTやClaudeなどのLLMに、テキスト化されたリポジトリの内容を入力することで、コードに関する質問に答えてもらったり、READMEを自動生成してもらったりできます。
* **NotebookLMによる要約・QA:** GoogleのNotebookLMにテキストを入力することで、リポジトリの内容を要約したり、コードに関する質問に答えたりできます。
* **RAGによるQA:** リポジトリが非常に大きい場合、RAG（Retrieval-Augmented Generation）を用いて、LLMが関連するコード部分を検索し、質問に答えることができます。

**制約としては、NotebookLMは50万文字を超えるテキストの入力に対応していない点や、非常に大きなリポジトリをテキスト化するとLLMのプロンプトに収まらない場合がある点などが挙げられます。**

これらのツールを利用することで、これまで難しかったリポジトリ全体のコードをLLMで解析し、活用することが容易になります。LLMの活用範囲が広がることで、ソフトウェア開発の効率化や、コード理解の促進に役立つことが期待されます。新人エンジニアの皆さんも、ぜひこれらのツールを試して、LLMを活用した開発に挑戦してみて下さい。 


引用元: https://zenn.dev/karaage0703/articles/0a546df8941057



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
