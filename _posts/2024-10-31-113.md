---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20241031.mp3
audio_file_size: 0
date: 2024-10-31 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
Gemini Models on GitHub Copilot、Creating a LLM-as-a-Judge That Drives Business Results –、LlamaPReview - GitHub Marketplace'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20241031
information: 
---

## 関連リンク


- [Gemini Models on GitHub Copilot](https://cloud.google.com/blog/products/ai-machine-learning/gemini-models-on-github-copilot)  



Google CloudとGitHubのパートナーシップにより、GitHub CopilotにGeminiモデルが導入されます。  まず、Gemini 1.5 Proが利用可能になり、今後数週間以内にCopilotユーザーは利用できるようになります。

Gemini 1.5 Proは、コード生成、分析、最適化といった開発者にとって一般的なユースケースに優れています。最大200万トークンという長いコンテキストウィンドウを備えているため、10万行以上のコードを処理し、修正案の提案やコードの動作説明を行うことが可能です。

GitHub Copilotの新しいモデル選択機能を通じて、Gemini 1.5 Proを選択できるようになります。利用可能なプラットフォームは、github.com、Visual Studio Code、Visual Studio向けのCopilot拡張機能などです。  Copilot ChatでもGemini 1.5 Proを使用できます。

これは、開発者がニーズに最適なモデルを選択できる柔軟性と制御性を重視する姿勢を示しており、AIによるコード生成の次の段階は、単一モデルではなく、複数のモデルから選択できるようになることを意味します。  Geminiモデルは、Gemini API、Google AI Studio、Vertex AI、Google Cloud、Workspace、Android Studio、Firebase、Colabなど、多くの主要なプラットフォームや環境で開発者エクスペリエンスをサポートしています。Google独自のコード支援ツールであるGemini Code Assistも、Visual Studio CodeやJetBrains IDEなど、一般的な統合開発環境(IDE)で利用可能です。


本発表は、開発者にとって多様な選択肢を提供する重要な一歩であり、今後、より高度で効率的なコード開発を支援する技術革新が期待されます。 新人エンジニアにとっても、様々な環境で利用可能なGeminiモデルとGitHub Copilotの連携は、開発効率の向上に大きく貢献するでしょう。


引用元: https://cloud.google.com/blog/products/ai-machine-learning/gemini-models-on-github-copilot


- [Creating a LLM-as-a-Judge That Drives Business Results –](https://hamel.dev/blog/posts/llm-judge/)  



本記事は、LLM（大規模言語モデル）を用いてAIシステムの評価を行うための実践的なガイドです。AI開発チームが抱えるデータ量増加や、曖昧な評価指標による非効率性といった問題を解決する手法として、「Critique Shadowing（批評による影付け）」という手法を紹介しています。これは、LLMを「判定者」として活用し、AI出力の評価を効率化する方法です。

**問題点**: 多くのAIチームは、多数の指標を用いた複雑な評価システムに苦戦しています。指標が多すぎたり、評価尺度が曖昧であったり、ドメイン専門家の知見が活用されていなかったりすることで、信頼できるデータが得られず、開発の進捗が滞ることが多々あります。

**解決策**:  Critique Shadowingでは、以下のステップでLLMを「判定者」として活用します。

**ステップ1：主要ドメイン専門家の特定**: AI製品の成功に不可欠な専門知識を持つ人物（複数でも可）を特定します。この専門家は、技術的な側面だけでなく、ユーザーニーズの観点からも評価を行います。

**ステップ2：データセットの作成**:  AIが本番環境で遭遇する様々な状況を網羅したデータセットを作成します。これは、AIの機能（Feature）、想定される状況（Scenario）、ユーザー像（Persona）といった次元で構成されます。既存データとLLMを用いた合成データの両方を活用することが推奨されます。

**ステップ3：パス/フェイル判定と批評**: 主要ドメイン専門家は、AIの出力に対して「パス」または「フェイル」の判定を行い、その理由を詳細に記述した批評を作成します。複雑な数値評価ではなく、単純な二値判定が、重要な点を明確にする上で有効です。批評はLLM判定器のプロンプト作成に必須です。

**ステップ4：エラー修正**:  データセットのレビューを通じてAIシステムのエラーが発見された場合は、修正してから次のステップに進みます。

**ステップ5：LLM判定器の構築**: 主要ドメイン専門家による判定結果と批評を基に、LLM判定器のプロンプトを構築します。専門家の判定とLLM判定器の判定結果の一致率を高めるために、プロンプトを繰り返し改良します。

**ステップ6：エラー分析**: LLM判定器を用いて評価した結果、エラー率の高い領域を特定し、その原因を分析します。これにより、AIシステムの改善に集中すべき点を明確にできます。

**ステップ7：専門的なLLM判定器の作成（必要に応じて）**: エラー分析の結果に基づき、必要に応じて特定のエラーに焦点を当てた専門的なLLM判定器を作成します。

**重要なポイント**:  この手法の真の価値は、データの精査と分析にあります。LLM判定器は、データ分析を促すためのツールと捉えるべきです。  また、本手法は、状況に応じて柔軟に適用できることを理解しておくことが重要です。


本記事では、各ステップの詳細な手順やLLMプロンプト例、よくある質問への回答などが示されていますが、本要約では簡潔に概要を説明しました。より詳細な情報は、元記事を参照ください。


引用元: https://hamel.dev/blog/posts/llm-judge/


- [LlamaPReview - GitHub Marketplace](https://github.com/marketplace/llamapreview)  



LlamaPReviewは、GitHubのプルリクエスト(PR)を自動的にレビューするAIアシスタントです。GitHub Marketplaceで提供されており、ワンクリックインストールで簡単に導入できます。現在、無料で利用可能です。

主な機能は以下の通りです。

* **完全自動化されたレビュー:**  PR作成時に自動的にレビューを行い、包括的なコメントを投稿します。設定不要で、すぐに利用できます。
* **高度なコード理解:**  単なるコードスキャンではなく、リポジトリ全体のコンテキストを理解し、構文エラーから複雑な論理的な欠陥まで、潜在的な問題を特定します。コード間の関係性や依存関係も考慮に入れた分析を行います。
* **多言語対応:** JavaScript、C++、Python、PHP、Java、Scala、Go、Rust、Kotlin、TypeScriptなど、主要なプログラミング言語をサポートしています。
* **無料利用:** クレジットカード情報などの入力は不要で、全ての機能を無料で利用できます。


LlamaPReviewは、シニアエンジニアによるレビューのような詳細なフィードバックを提供することで、コードの品質向上に貢献します。GitHub Actionsとの統合も可能で、CI/CDパイプラインへのシームレスな組み込みを実現できます。  新人エンジニアにとっても、学習コストが低く、効率的なコードレビューを行う上で非常に有用なツールと言えます。  無料であるため、気軽に試してその効果を実感できます。


引用元: https://github.com/marketplace/llamapreview



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
