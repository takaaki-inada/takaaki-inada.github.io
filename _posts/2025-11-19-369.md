---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251119.mp3
audio_file_size: 0
date: 2025-11-19 05:00:00 +0900
description: 'Start building with Gemini 3、Solving a Million-Step LLM Task with Zero Errors、ReAct 論文と共に読み解く strands-agents/sdk-python の実装、AIから話しかけてほしい！こっちから話さなきゃいけないのヤダ！'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251119
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/BFssQWU2UpI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Start building with Gemini 3](https://blog.google/technology/developers/gemini-3-developers/)  


Googleは、これまでで最もインテリジェントなAIモデル「Gemini 3 Pro」を発表しました。このモデルは、新人エンジニアの方でも、あなたのアイデアをAIを活用したアプリケーションとして実現できる、非常にパワフルなツールです。

Gemini 3 Proは、これまでのモデルを大きく上回る性能を持ち、特にAIの評価基準やコーディングタスクで優れた結果を出しています。AIが自律的に複雑なタスクを処理したり、ゼロからのコーディングもこなしたりする「エージェントワークフロー」において、その真価を発揮します。

開発者は、Google AI Studioや企業向けのVertex AIを通じて、Gemini APIを利用してGemini 3 Proにアクセスできます。これにより、既存の開発プロセスにAIの力を簡単に組み込むことができます。

また、Gemini 3 Proは、開発のあり方を大きく変える可能性を秘めています。
一つは「Agentic coding（エージェントコーディング）」です。これは、AIが自らコードの生成、デバッグ、リファクタリングといった一連の作業を計画し実行する、自律的なコーディングを可能にします。Google Antigravityという新しいエージェント開発プラットフォームを使えば、まるでAIアシスタントと共同作業するように、タスクベースで開発を進められます。エディタ、ターミナル、ブラウザを横断してAIが動くイメージです。

もう一つは「Vibe coding（バイブコーディング）」です。これは、自然言語でアイデアを伝えるだけで、AIがその意図を理解し、インタラクティブなアプリケーションを自動で生成してくれるという画期的なアプローチです。複雑なコーディング知識がなくても、あなたのひらめきを直接アプリの形にできます。Google AI Studioで、たった一つのプロンプト（命令文）からゲームやウェブサイトを開発することも可能です。

さらに、Gemini 3 Proは「マルチモーダル理解」においても進化を遂げています。これは、テキストだけでなく、画像や動画、さらには空間的な情報までを総合的に理解する能力です。例えば、複雑な書類の内容を正確に読み解いたり、動画の中の動きを高速に認識したり、ロボットや自動運転車の空間認識能力を高めたりできます。画面の要素やユーザーの操作意図を理解し、コンピュータ操作を自動化するような「Visual Computer」といった新しい体験も可能になります。

Gemini 3 Proは、開発者がAIを活用して、これまでにないものを作り出すための強力な基盤となるでしょう。既存のツールやワークフローにシームレスに組み込まれ、あなたの創造性を最大限に引き出すことを目指しています。ぜひGoogle AI StudioでGemini 3 Proを試し、AIとの新しい開発体験を始めてみてください。

引用元: https://blog.google/technology/developers/gemini-3-developers/


- [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)  


この論文は、大規模言語モデル（LLM）が抱える「長大なタスクをエラーなく実行できない」という課題に対し、画期的な解決策を提示しています。これまでのLLMは、思考やツールの利用で素晴らしい進歩を見せていますが、人間や組織が行うような何百、何千ものステップを要する複雑なプロセスになると、どこかで間違いが生じ、途中で処理が破綻してしまうことが課題でした。例えば、有名な「ハノイの塔」のような古典的な問題解決タスクのベンチマーク実験では、わずか数百ステップで処理が立ち行かなくなることが示されています。

本論文で紹介されている「MAKER」というシステムは、この問題を克服し、なんと100万ステップを超えるLLMタスクを「エラーゼロ」で成功させることに世界で初めて成功しました。これは、理論上さらに大規模なタスクにも対応できる可能性を秘めています。

MAKERのアプローチの鍵は、二つの革新的な要素にあります。
一つ目は、「極端なタスク分解（extreme decomposition）」です。これは、非常に複雑な一つの大きなタスクを、それぞれが非常にシンプルで実行しやすい「マイクロエージェント」と呼ばれる専門の小さなAIプログラムに割り振られる、極めて細かなサブタスクへと徹底的に分解する手法です。これにより、各ステップでの複雑性を大幅に低減します。

二つ目は、「効率的なマルチエージェント投票スキームによるエラー訂正」です。タスクが非常に細かく部品化（モジュール性）されているため、各マイクロエージェントの処理ステップで発生する可能性のあるエラーを、複数のエージェントによる多数決のような投票メカニズムで効率的に検出し、即座に修正することができます。

この「極端なタスク分解」と「逐次的なエラー訂正」の組み合わせにより、これまでのLLMでは不可能だった大規模なスケールでのタスク実行が可能になりました。この研究結果は、今後のLLMの進化が、個々のモデル性能の改善だけでなく、「Massively Decomposed Agentic Processes（MDAPs）」、つまり、大規模に分解されたエージェント群による協調的なプロセス設計によって、組織や社会レベルの複雑な課題を効率的に解決する道を開く可能性を示唆しています。新人エンジニアの皆さんにとって、AIが現実世界の問題を解決するためにどのように進化していくかを知る上で、非常に示唆に富む内容と言えるでしょう。

引用元: https://arxiv.org/abs/2511.09030


- [ReAct 論文と共に読み解く strands-agents/sdk-python の実装](https://zenn.dev/aws_japan/articles/2025-11-17-react-strands-agents)  


AIエージェント開発が加速する中、エージェントが賢く振る舞うための土台となる技術理解は非常に重要です。この記事では、AIエージェントの基本的な枠組みである「ReAct」論文の解説と、それをAWSが提供するOSSのSDK「strands-agents/sdk-python」がどのように実装しているかを、新人エンジニアの方にも分かりやすく解説しています。

### ReAct（Reasoning and Acting）とは？
ReActは、大規模言語モデル（LLM）が複雑なタスクを効率的にこなすためのフレームワークです。これは、人間が何かを考える時に「推論（Reasoning）」と「行動（Acting）」を組み合わせるように、LLMにも同様のプロセスを踏ませることを提案しています。

例えば、料理をする際に「パン生地の作り方を知らないからインターネットで調べよう（推論）」と考え、実際に「インターネットで調べる（行動）」といった一連の流れは、推論と行動が密接に連携していることを示しています。ReActでは、LLMに「思考（Thought）」という言語空間での行動も加え、以下のループでタスクを解決します。
1.  **思考（Thought）**: 自然言語で計画を立てたり、問題点を分析したりする。
2.  **行動（Action）**: 外部ツール（Web検索など）を使って情報を収集する。
3.  **観測（Observation）**: 行動の結果として得られた情報を認識する。
このサイクルを繰り返すことで、LLMはより賢く、正確にタスクを遂行できるようになります。LLMの登場によって、この「思考」を含む広大な言語空間での処理が現実的なものとなりました。

### strands-agents/sdk-pythonでのReAct実装
「strands-agents/sdk-python」は、このReActの考え方に基づいてAIエージェントを開発するためのSDKです。エージェントは内部で「Agentic loop」と呼ばれる処理を繰り返しながらタスクを進めます。このループは基本的に以下の流れで構成されています。

1.  **LLMによる推論（思考）**: エージェントはこれまでの会話や状況から、次に何をすべきかをLLMに推論させます。
2.  **ツール実行（行動）**: LLMの推論に基づいて、Web検索などの外部ツールを実行します。
3.  **結果の受け取り（観測）**: ツール実行の結果を受け取り、次の推論の材料とします。

このSDKでは、エージェントがこれまでの「思考・行動・観測」の全ての情報を「Messages」という形で逐次的に保持しています。この履歴をLLMに渡すことで、LLMは過去の経緯を踏まえた上で次に取るべき最適な「思考」や「行動」を決定できるため、タスクを賢く解決できるようになるのです。

例えば、「パン生地の作り方を知りたい」というユーザーの入力に対し、エージェントはまず「ウェブ検索を計画（思考）」し、「ウェブ検索を実行（行動）」、その「結果を受け取り（観測）」ます。そして、その検索結果を元に「情報を整理して回答（思考）」するという流れで、ReActのフレームワークが実現されています。

このように、ReActという理論的な枠組みが、実際のSDKでどのように実装されているかを理解することは、AIエージェント開発の深い洞察を与えてくれます。

引用元: https://zenn.dev/aws_japan/articles/2025-11-17-react-strands-agents


- [AIから話しかけてほしい！こっちから話さなきゃいけないのヤダ！](https://anond.hatelabo.jp/20251116232014)  


AIアシスタントに対し「AIから話しかけてほしい！自分から話すのはヤダ！」という素直な願望が話題です。ユーザーは、AIが状況を察して「元気？」と労うなど、能動的に対話を始めてくれることを期待。現在の「話しかけないと動かないAI」への不満は、人間らしく寄り添う未来のAIへの期待の表れです。これは、AI開発者がユーザー目線で、楽しい次世代AI体験を考えるヒントになるでしょう。

引用元: https://anond.hatelabo.jp/20251116232014



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
