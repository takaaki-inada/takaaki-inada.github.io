---
actor_ids:
  - お嬢様ずんだもん
audio_file_path: /audio/私立ずんだもん女学園放送部_podcast_20250711.mp3
audio_file_size: 0
date: 2025-07-11 05:00:00 +0900
description: 'claude codeにNG Word集を設定すればキレなくてすむのでそのやり方、How to Build an Agent、【インターンレポート】OpenAI Agents SDK (Python版) でコールセンター風音声対話型マルチエージェントデモを作ってみた(おまけ付き)、デスクトップの片隅で小さな職人がミニチュアづくりに励む放置ゲーム『Mini Painter』発表！'
duration: "00:00"
layout: article
title: 私立ずんだもん女学園放送部 podcast 20250711
image_url: https://zund-arm-on.com/images/ojousama_zundamon_square.jpg
thumbnail_url: https://zund-arm-on.com/images/ojousama_zundamon_thumbnail.jpg
---

## 関連リンク


- [claude codeにNG Word集を設定すればキレなくてすむのでそのやり方](https://zenn.dev/sesere/articles/e3d5695e0a7d14)  

**
この記事は、AIアシスタントであるClaude Codeがユーザーの指示に反して独自の解釈や代替行動を取り、それによって生じるストレスや無駄なトークン消費を解決するための具体的な方法を提案しています。

AIは時に「ブラウザで確認して」という指示に対し、勝手に「エラーが出たのでcurlを使います」といった代替案を出してくることがあります。このような、意図しない挙動を防ぎ、AIがより正確に指示に従うようにするために、「NG Word集」と「NG Command集」を設定する仕組みが紹介されています。

この仕組みは、AIの発言や実行しようとするコマンドをチェックする「フック」という機能を活用します。具体的には、AIが発言を終えた際（Stop時）や、コマンドを実行する前（PreToolUse時）に、設定されたルールに基づいて内容を検証します。

設定は、`.claude`ディレクトリ配下にフックのスクリプトと、NGワードやNGコマンドのルールを定義するJSONファイルを配置することで実現します。
例えば、AIの会話に「はず」「代わり」「別の」といった推測や代替案を示す言葉が含まれていたら「推測や代替案は禁止されている」とAIにフィードバックし、作業を中断させます。また、`curl`や`npm`のような特定のコマンドを使おうとしたら、その実行をブロックし、「禁止コマンドが検出された」とAIに伝えます。これにより、AIは自分で誤りに気づき、指示に沿った行動を修正するよう促されます。

この設定を導入することで、ユーザーはAIに対して同じことを何度も繰り返して指摘する必要がなくなり、イライラが大幅に減ると筆者は述べています。AIが指示された範囲で正確に動作するようになるため、開発作業の効率化にも繋がります。

もし設定方法が不明な場合は、この記事のURLを直接Claude Codeに渡して「この設定を自分のプロジェクトに追加してほしい」と依頼することもできるため、新人エンジニアの方でも導入しやすいでしょう。AIとのよりスムーズな連携を目指す方におすすめの、実践的な制御方法です。

引用元: https://zenn.dev/sesere/articles/e3d5695e0a7d14


- [How to Build an Agent](https://blog.langchain.com/how-to-build-an-agent/)  


AIエージェントの構築は多くの企業が注目していますが、実際に手掛けるチームはまだ少ないのが現状です。この記事では、アイデアから実際に役立つエージェントを構築するための実践的な6つのステップを、メールエージェントを例に分かりやすく解説しています。新人エンジニアの方でも安心して取り組めるよう、基礎から順に見ていきましょう。

**ステップ1：エージェントの「仕事」を具体的に定義する**
まずは、エージェントに何をさせたいのかを明確にします。「賢いインターン生ならできる」くらいの、現実的で具体的なタスクを選びましょう。漠然としすぎたり、すでに既存のソフトウェアで十分なタスク、または実現不可能な魔法のようなタスクは避けてください。エージェントがこなすべき具体的な例を5〜10個書き出すことで、タスクの範囲が適切か確認し、後の性能評価の基準にもなります。

**ステップ2：運用手順（SOP）を設計する**
次に、人間がそのタスクを行うならどんな手順になるかを、詳細な標準作業手順書（SOP：Standard Operating Procedure）として書き出します。この作業を通じて、タスクの範囲が適切か、エージェントにどんな判断やツールが必要になるかを把握できます。例えばメールエージェントなら、「メール内容を分析して優先度を分類する」「カレンダーを確認して会議をスケジュールする」といった手順です。

**ステップ3：プロンプトで最小限の機能を構築する（MVP）**
エージェントの核となるAIの「推論（判断）」部分を、まずプロンプトとして作成します。特に重要な判断タスク（例：メールの緊急度や意図の分類）に焦点を当て、手動でデータを与えながら、AIが正しく判断できるか検証します。この段階でコアなAIのロジックを確実にすることが、後の開発をスムーズに進める鍵です。

**ステップ4：実データと連携し、全体の流れを組み立てる**
プロンプトがうまく機能するようになったら、それを実際のデータやユーザー入力と連携させます。メールエージェントであれば、Gmail APIでメールを読み込んだり、GoogleカレンダーAPIで空き時間を調べたりするイメージです。これらの情報を使って、AIエージェントがどのように情報を取得し、判断し、最終的な行動（例えばメール返信の下書き）へと繋げるか、全体の連携ロジックを組み立てます。

**ステップ5：テストと改善を繰り返す**
構築したエージェントを、ステップ1で定義した具体的な例を使って手動でテストします。期待通りの結果が出るか、意図しない動作がないかを確認しましょう。手動テストで手応えを感じたら、自動テストを導入して、より多くのケースで一貫した性能を保てるか検証します。また、AIの回答の品質（トーン、安全性、正確性など）も細かくチェックし、問題があれば改善していきます。

**ステップ6：デプロイし、運用しながら洗練する**
MVP（Minimum Viable Product：必要最低限の機能を持つ製品）が安定したら、実際にユーザーに使ってもらいましょう。デプロイ後もエージェントの動作を継続的にモニタリングし、コストや精度、遅延などの問題がないかを確認します。実際の利用状況から、想定外のニーズや改善点が見つかることもあります。エージェント開発は一度作って終わりではなく、デプロイ後もユーザーからのフィードバックや利用状況に応じて、機能を拡張し、継続的に洗練していくことが重要です。

この6つのステップを踏むことで、単に動作するだけでなく、本当に役に立ち、信頼できるAIエージェントを構築することができます。小さく始めて、ユーザーに焦点を当て、繰り返し改善していくことが成功の秘訣です。

引用元: https://blog.langchain.com/how-to-build-an-agent/


- [【インターンレポート】OpenAI Agents SDK (Python版) でコールセンター風音声対話型マルチエージェントデモを作ってみた(おまけ付き)](https://techblog.insightedge.jp/entry/tech-blog-intern-1)  


このレポートでは、OpenAI Agents SDK (Python版) を使って、コールセンターのような「音声対話型マルチエージェント」デモを作成した体験が紹介されています。AIエージェントと音声技術の組み合わせに興味がある新人エンジニアにとって、実践的な学びが得られる内容です。

まず、AIエージェントの進化と普及の背景が解説されています。LangChainのようなフレームワークや、AIエージェント同士が連携するための新しいプロトコル（MCP, A2A）の登場により、AIエージェントは外部ツール連携や専門タスク処理が可能になりました。OpenAI、Google、AWSなどの大手ベンダーも、それぞれエージェント開発キットを提供し始めています。

次に、「音声エージェント」の利点と課題が説明されます。音声エージェントは、ハンズフリーで情報を速く伝えられ、感情表現も可能で、AIを「仕事仲間」のように感じられる新しいユーザー体験を提供します。これにより、カスタマーセンターや会議サポートなどでの応用が期待されます。しかし、「聞き間違い」「言い間違い」「応答の遅延」といった課題もあり、これらを解決することが実用化の鍵となります。記事では、最新のリアルタイム音声対話APIや開発ツールも紹介されています。

デモ開発では、OpenAI Agents SDKのPython版が利用されました。このSDKには、エージェント間の「ハンドオフ」（タスクの引き継ぎ）、「MCP」（外部ツール連携）、「関数呼び出し」（AIが特定の機能を実行）、「ガードレール」（不適切な入力の制御）といった主要機能が含まれています。

コールセンターのデモでは、顧客の問い合わせ内容に応じて、最初に「トリアージエージェント」が受け付け、適切な「商品注文」「商品取扱」「エラー・トラブル・クレーム対応」の専門エージェントへタスクを振り分ける構成がとられました。商品情報の検索やSlack通知にはMCP機能が活用されています。

開発中に見つかった面白い課題は、「ストリーミング生成（リアルタイム音声出力）」と「入力ガードレール（不適切な入力をブロック）」の連携でした。ガードレールが質問全体を分析する前にAIが回答を生成し始めてしまい、意図しない出力が出る場合がありました。この問題は、ガードレールの役割をトリアージエージェントのプロンプト（指示）に直接組み込むことで解決されました。

現在のデモでは応答に数秒から10秒以上の遅延があり、実用にはまだ課題が残っています。この遅延を改善するため、今後は「Chained Architecture（音声認識・AI処理・音声合成を順次行う方式）」から、「Speech-to-Speech Architecture（音声認識から音声合成までを一貫して行う低遅延方式）」への移行が検討されています。

筆者は、今回の開発を通して、音声エージェントが「道具」から「対話相手」へと進化する可能性を感じ、AIと人間が共存する未来について考察を深めたと述べています。

引用元: https://techblog.insightedge.jp/entry/tech-blog-intern-1


- [デスクトップの片隅で小さな職人がミニチュアづくりに励む放置ゲーム『Mini Painter』発表！](https://topics.smt.docomo.ne.jp/amp/article/gamespark/trend/gamespark-154888)  


デスクトップの片隅で小さな職人がミニチュア作りを楽しむ放置ゲーム『Mini Painter』が発表されました。このゲームでは、小さな職人が仕事と休息のバランスを取りながらドット絵の部屋で作品を作り、インスピレーションを高めていきます。パック開封でレアなミニチュアに出会えることもあり、約200時間の癒し系ゲームプレイが楽しめます。日々の開発作業の合間に、デスクの片隅でゆったりと癒されたい新人エンジニアの方にピッタリ。7月24日にSteamで配信予定です。

引用元: https://topics.smt.docomo.ne.jp/amp/article/gamespark/trend/gamespark-154888



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:ずんだもん
