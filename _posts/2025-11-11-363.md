---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251111.mp3
audio_file_size: 0
date: 2025-11-11 05:00:00 +0900
description: 'GPT-5超え？最強の無料ローカルAI「Kimi K2 Thinking」、中国から登場 - すまほん!!、Apple MLXを利用したiPhone/iPad用ローカルAIクライアント「Locally AI」がMacに対応。  AAPL Ch.、How to Achieve 4x Faster Inference for Math Problem Solving'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251111
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/l5asU0LmQPY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [GPT-5超え？最強の無料ローカルAI「Kimi K2 Thinking」、中国から登場 - すまほん!!](https://smhn.info/202511-kimi-k2-thinking)  


皆さん、AIの最新ニュースです！中国のMoonshot AIという会社が、なんと「GPT-5を超えるかも？」と話題の無料オープンソースAIモデル「Kimi K2 Thinking」を公開しました。これは、AI開発に携わる私たちエンジニアにとって、非常に注目すべき存在です。

この「Kimi K2 Thinking」は、特に「推論」や「エージェント」と呼ばれる、AIが複雑な問題を考えたり、外部ツールを使って自律的に作業を進めたりする能力で、OpenAIのGPT-5やAnthropicのClaude 4.5 Sonnet (Thinking)といった最先端モデルと同等か、それ以上の高い性能を発揮していると評価されています。

技術的な特徴は以下の通りです。
*   **効率的なAIの仕組み（MoEアーキテクチャ）**: 全体で1兆個という多くのパラメータを持つ「Mixture-of-Experts（MoE）」構造を採用し、推論時には最適な320億個のパラメータを使うことで効率を高めています。これは、多数の専門家から必要な人だけを呼ぶようなイメージです。
*   **超長文を理解する力（256Kコンテキストウィンドウ）**: AIが一度に扱える情報量を示す「コンテキストウィンドウ」が256K（約25万6千トークン）と非常に長いです。これにより、非常に長い文書を読んだり、複雑な会話の流れを記憶したりしながら、一貫した処理を行うことが可能になります。
*   **高速・省メモリな処理（INT4量子化）**: AIの計算を効率化する「INT4量子化」技術により、推論速度が速く、使用メモリも抑えられています。
*   **自律的な問題解決能力**: 最大の特徴は、人間の介入なしに、外部ツールを200〜300回も連続して呼び出し、複雑なタスクを一貫して解決できる点です。AIが自分で考えて、必要な道具を使いこなしながら、根気強く作業を進められる能力に長けていると言えます。

このモデルはオープンソースとして公開されており、AIモデルの共有プラットフォームであるHugging Faceからダウンロード可能です。また、ローカル環境でAIを動かすための「Ollama」を使えば、手元のPCなどで動かすこともできます。これにより、研究者や開発者が自由にモデルを検証したり、自分たちの用途に合わせて改良したりするチャンスが広がります。

ただし、256Kという長大なコンテキストを扱うため、導入には高性能なPCなどのハードルがあることも理解しておきましょう。

「Kimi K2 Thinking」は、高い推論能力と長大なコンテキスト処理能力を兼ね備えた、無料で利用できる画期的なAIモデルです。今後のAI開発の可能性を大きく広げる存在として、ぜひ注目していきましょう。

引用元: https://smhn.info/202511-kimi-k2-thinking


- [Apple MLXを利用したiPhone/iPad用ローカルAIクライアント「Locally AI」がMacに対応。  AAPL Ch.](https://applech2.com/archives/20251110-locally-ai-support-apple-silicon-mac.html)  


新人エンジニアの皆さん、今日のニュースは、Apple製品ユーザーにとって特に嬉しいAI関連の話題です。これまでiPhoneやiPadで使えた「Locally AI」という便利なアプリが、ついにMacでも利用できるようになりました！これは、皆さんの手元のMacで、インターネットに繋がなくてもAI（人工知能）を動かせるようになる、ということなので、ぜひ注目してください。

「Locally AI」は、人気のマッチングアプリTinderなどを手掛ける米Match Group Inc.でiOSアプリを開発するAdrien Grondinさんによって開発されました。このアプリの最大の魅力は、**プライバシーとセキュリティ**です。通常のAIサービスは、皆さんの入力したデータをクラウド上のサーバーに送信して処理しますが、「Locally AI」は、その名の通り「ローカル」、つまり**皆さんのデバイス内部だけでAIを動かします**。そのため、外部にデータが漏れる心配がなく、安心してAIを利用できるのです。ログインも不要で、データ収集も一切行われません。

この技術的な背景には、Appleが開発した機械学習フレームワーク「Apple MLX」があります。このフレームワークは、Apple Siliconチップ（M1、M2、M3チップなど）に最適化されているため、「Locally AI」はMacの高性能な処理能力を最大限に活用し、AIモデルを驚くほど高速かつ効率的に動かすことができます。これにより、まるでインターネットに接続されたAIサービスを使っているかのような快適な体験が、オフライン環境でも実現されます。

具体的にどのようなAIが使えるかというと、Lama、Gemma、Qwen、SmolLM、DeepSeekといった様々な**大規模言語モデル（LLM）**をサポートしています。これらのモデルは、Hugging Faceという有名なAIモデル配布プラットフォームからワンクリックで簡単にダウンロードでき、すぐに皆さんのMacで動かすことができます。さらに、最近iOS/iPadOS 26で導入されたApple独自の「Apple Foundation Model」にも対応している点は、最新技術に触れる良い機会となるでしょう。

利用するための条件は、macOS 26.0 Tahoe以降、またはiOS/iPadOS 18.0以降のAppleデバイスが必要です。このアプリはApp Storeで無料で公開されているため、気軽に試すことができます。ただし、現在のMac版には日本語入力に関する不具合が一部報告されているため、その点だけは注意してください。

ローカルでAIを動かす技術は、機密情報を扱う業務での活用や、インターネットが利用できない環境での作業など、様々な場面でその価値を発揮します。新人エンジニアの皆さんにとって、この「Locally AI」は、AIモデルの動作原理を理解し、実際に様々なAIを動かしてみるための素晴らしい学習ツールとなるはずです。ぜひダウンロードして、ローカルAIの世界を体験してみてください。

引用元: https://applech2.com/archives/20251110-locally-ai-support-apple-silicon-mac.html


- [How to Achieve 4x Faster Inference for Math Problem Solving](https://developer.nvidia.com/blog/how-to-achieve-4x-faster-inference-for-math-problem-solving/)  


このNVIDIAのブログ記事では、大規模言語モデル（LLM）が数学の問題を解く際の推論速度を、最大で4倍高速化する技術が紹介されています。AI数学オリンピック2024で優勝したソリューションの基盤となった技術であり、日本の新人エンジニアの方々にも分かりやすいように要点をまとめました。

LLMは複雑な数学問題を解決できますが、効率的に大規模運用するには、適切な「サービングスタック」、量子化戦略、デコーディング手法を組み合わせる必要があります。これらがバラバラのツールで提供され、連携が難しいという課題がありました。

この課題に対し、NVIDIAは「NVIDIA NeMo-Skills」ライブラリと「TensorRT-LLM」を組み合わせる方法を提案しています。これにより、高速で再現性の高い推論パイプラインを構築できます。具体的には、以下の主要な技術が活用されています。

1.  **FP8量子化**: LLMのモデルデータ量を8ビット浮動小数点（FP8）に軽量化する技術です。これにより、モデルのメモリ使用量が減り、対応するGPU（NVIDIA Hopperなど）で大幅な推論速度向上が期待できます。
2.  **ReDrafterによる投機的デコーディング**: 小さな「ドラフトモデル」があらかじめ次のトークン（単語や記号）の候補を予測し、それをメインの大きなLLMでまとめて検証・承認することで、LLMが一つずつトークンを生成するよりも高速な応答を可能にする技術です。

これらの技術を組み合わせることで、NVIDIA H100 GPUを2枚使用した場合、バッチ推論で従来のBF16設定に比べ4倍の速度向上が達成されました。記事では、以下のステップを通してこの高速化を実現する方法が説明されています。
*   既存のOpenMathモデルをFP8対応のTensorRT-LLMエンジンに変換。
*   ReDrafterドラフトモデルを学習し、メインモデルと統合。
*   これらの最適化されたモデルを使って、高速な推論サーバーを構築。
*   異なる設定でのパフォーマンス（レイテンシやスループット）を測定・比較。

さらに、OpenMath LLMは「ツールコーリング」という機能も持ち合わせています。これは、LLMが生成したPythonコードを安全なサンドボックス内で実行し、その結果を問題解決に利用するものです。LLMが自らプログラミングを行い、計算結果に基づいて思考を深めるような働きをします。

これらの技術は、LLMの推論コスト削減と応答速度向上に直結し、AIアプリケーション開発の可能性を広げるものです。新人エンジニアの皆さんが、LLMの効率的な活用を考える上で、きっと役立つでしょう。

引用元: https://developer.nvidia.com/blog/how-to-achieve-4x-faster-inference-for-math-problem-solving/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
