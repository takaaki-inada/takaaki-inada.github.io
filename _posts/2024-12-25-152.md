---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20241225.mp3
audio_file_size: 0
date: 2024-12-25 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
GitHub - browser-use/browser-use: Make websites accessible for AI agents、完全にオープンな約1,720億パラメータ（GPT-3級）の大規模言語モデル 「llm-jp-3-172b-instruct3」を一般公開～GPT-3.5を超える性能を達成～ - 国立情報学研究所 / National Institute of Informatics、vLLMを利用したLLM推論高速化テクニック、AI decodes the calls of the wild'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20241225
---

## 関連リンク


- [GitHub - browser-use/browser-use: Make websites accessible for AI agents](https://github.com/browser-use/browser-use)  


このリポジトリは、AIエージェントがウェブサイトにアクセスしやすくするためのツール「browser-use」を提供します。主な機能は、ウェブサイトのコンテンツ抽出、複数タブの自動管理、クリックした要素のXPath抽出、カスタムアクションの追加、自己修正機能などです。LangChainをサポートする様々なLLM（例：gpt4o, claude 3.5 sonnet）に対応し、複数のエージェントを並列実行できます。カスタムアクションは、同期・非同期関数で定義可能で、Pydanticモデルによるパラメータ定義も可能です。ブラウザ設定はBrowserConfigとBrowserContextConfigクラスでカスタマイズでき、headlessモードの切り替え、ブラウザのセキュリティ設定、Cookieファイルの指定などが可能です。


引用元: https://github.com/browser-use/browser-use


- [完全にオープンな約1,720億パラメータ（GPT-3級）の大規模言語モデル 「llm-jp-3-172b-instruct3」を一般公開～GPT-3.5を超える性能を達成～ - 国立情報学研究所 / National Institute of Informatics](https://www.nii.ac.jp/news/release/2024/1224.html)  


国立情報学研究所が、GPT-3と同規模の約1,720億パラメータを持つ大規模言語モデル「llm-jp-3-172b-instruct3」を公開しました。このモデルは、2.1兆トークンという大量の学習データで訓練され、日本語の理解能力を測るベンチマークでGPT-3.5を超える性能を達成しています。特筆すべきは、学習データを含めて全てオープンにされている点で、これは世界最大規模です。開発には、経済産業省・NEDOのプロジェクトや文部科学省の補助金が活用されました。モデルのアーキテクチャはLlama 2ベースで、日本語と英語のインストラクションデータでチューニングされています。今後の展開として、モデルの透明性と信頼性確保に向けた研究開発を進め、他のチェックポイントデータも公開予定です。このモデルは、LLMの研究開発を促進し、社会での利活用に貢献することが期待されています。


引用元: https://www.nii.ac.jp/news/release/2024/1224.html


- [vLLMを利用したLLM推論高速化テクニック](https://acro-engineer.hatenablog.com/entry/2024/12/24/120000)  


この記事では、LLM（大規模言語モデル）の推論を高速化するためのライブラリvLLMについて解説しています。vLLMは、Paged Attentionという技術でAttention計算を効率化し、推論を高速化します。また、Hugging Faceの主要モデルをサポートしており、カスタム実装なしで利用可能です。さらに、GPUリソース管理やCPUオフロード機能も備えています。

記事では、vLLMを使わない場合と使用した場合の推論速度を比較しています。Hugging Faceを使った場合、Qwen2.5-7Bモデルでの推論に92時間かかるところ、vLLMを使用すると281秒に短縮されました。また、vLLMに加えてAWQ（量子化技術）を利用すると、GPUメモリを削減できますが、推論時間は360秒と若干遅くなります。

さらに、Auto Prefix Cachingという機能を使うことで、プロンプトの共通部分の計算を使い回し、推論を高速化できることも紹介しています。One-Shot Sampleを先頭に加えた場合、この機能により推論時間が296秒から189秒に短縮されました。

最後に、GPUメモリが不足する場合に、CPUオフロード機能を使うことで、大規模モデルの推論も可能になることを説明しています。ただし、CPUオフロードを利用すると、推論時間はGPUのみの場合と比較して大幅に増加します。

vLLMは、LLMの推論を高速化するための様々な機能を提供しており、LLMを効率的に利用するために役立つツールです。


引用元: https://acro-engineer.hatenablog.com/entry/2024/12/24/120000


- [AI decodes the calls of the wild](https://www.nature.com/immersive/d41586-024-04050-5/index.html)  


AI技術を用いて動物のコミュニケーションを解読する研究が進んでいます。具体的には、クジラ、ゾウ、サルなどの鳴き声や音のパターンをAIで解析し、彼らが互いに何を伝え合っているのかを理解しようとしています。

例えば、マッコウクジラはクリック音の連続（コーダ）でコミュニケーションを取り、地域によって異なる方言を持つことがわかっています。AIは、これらのコーダのテンポやリズムの微妙な変化（ルバートや装飾音）を検出し、クジラが複雑な情報を共有するための「音素アルファベット」のようなものを持っている可能性を示唆しています。

また、アフリカゾウは個体ごとに異なる鳴き声を使ってお互いを「名前」で呼び合っている可能性があり、AIを使ってその特定の鳴き声を識別することに成功しています。さらに、マーモセットも家族内で特定の音を使い分けていることが明らかになっています。

これらの研究では、ディシジョンツリーやランダムフォレストといったAIアルゴリズムが用いられ、動物の鳴き声のパターンを解析しています。さらに、アース・スピーシーズ・プロジェクトでは、ニューラルネットワークを用いて、異なる動物種や人間の音声データを学習させ、動物のコミュニケーションの基本構造を理解しようとしています。

ただし、AIはあくまでツールであり、動物の行動観察や人間による解釈も依然として重要です。AIによって動物のコミュニケーションを解読することは、彼らの認知能力や社会構造を理解し、ひいては保全活動にも役立つと考えられます。


引用元: https://www.nature.com/immersive/d41586-024-04050-5/index.html



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
