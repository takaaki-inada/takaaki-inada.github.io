---
actor_ids:
  - 春日部つむぎ
audio_file_path: /audio/マジカルラブリー☆つむぎのピュアピュアA.I.放送局_podcast_20250714.mp3
audio_file_size: 0
date: 2025-07-14 05:00:00 +0900
description: 'サンドボックス環境を MCP サーバーで提供する Container Use、AIでインシデント対応を民主化したい！日本語セキュリティLLM開発の挑戦と挫折、そして発見、LLMのAPIを活用したバックエンドアーキテクチャの事例を紹介します、武瑠、「BIBLE」を配信開始｜THE MAGAZINE'
duration: "00:00"
layout: article
title: マジカルラブリー☆つむぎのピュアピュアA.I.放送局 podcast 20250714
image_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
thumbnail_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
card: summary
---

## 関連リンク


- [サンドボックス環境を MCP サーバーで提供する Container Use](https://azukiazusa.dev/blog/mcp-server-container-use/)  


近年、Claude CodeやCursorといったAIコーディングエージェントが、プログラミングの生産性を大きく高めています。しかし、これらのAIエージェントは、あなたのパソコン上で任意のコマンドを実行できるため、使い方を間違えると、大切なファイルが削除されたり、システムが危険にさらされたりするリスクがあります。例えば、`rm -rf ~/`のようなコマンドが意図せず実行されると、あなたのホームディレクトリのデータが消えてしまうかもしれません。

多くのAIエージェントは、コマンドを実行する前にユーザーに許可を求めますが、頻繁に許可を求められると、次第に確認がおろそかになりがちです。また、エージェントによっては、一切許可を求めずに自動でコマンドを実行するオプションもあり、その場合、あなたがAIエージェントの動作を把握できなくなります。

このようなセキュリティ上のリスクを減らすために注目されているのが「サンドボックス環境」です。サンドボックス環境とは、AIエージェントが実行するすべての操作を、あなたのパソコンのシステムから隔離された、安全な仮想空間の中で行う仕組みです。これにより、もしAIエージェントが誤って危険なコマンドを実行したとしても、その影響はサンドボックス内に留まるため、あなたのシステムは守られます。さらに、複数のAIエージェントを動かす際にも、それぞれが独立した環境で作業できるため、互いの作業が干渉し合う心配がありません。

Dagger社が開発した「Container Use」は、このサンドボックス環境をAIコーディングエージェントに提供するためのツールです。「MCPサーバー」（Model Context Protocolサーバー）として機能し、MCPをサポートするAIエージェントであれば、Container Useを通じて安全に開発作業を進められます。

Container Useを使うと、AIエージェントは専用のツールを使ってサンドボックス環境を作成し、その中でコマンドの実行やファイルの読み書きを行います。これにより、AIエージェントがアプリケーションを構築したり、テストしたりする過程で、あなたのPCに直接影響を与えることなく作業を進められます。あなたがAIエージェントの作業内容を確認したいときは、ログを見たり、生成されたコードの差分を確認したり、実際にAIエージェントが作業したブランチに切り替えて内容をチェックすることも可能です。AIエージェントの作業に満足したら、その成果をあなたのメインのコードに安全に取り込むことができます。

Container Useは、AIコーディングエージェントを安全かつ効率的に開発に活用するための強力な味方となるでしょう。これにより、新人エンジニアでも安心してAIエージェントを使いこなし、開発スキルを向上させることが期待されます。

引用元: https://azukiazusa.dev/blog/mcp-server-container-use/


- [AIでインシデント対応を民主化したい！日本語セキュリティLLM開発の挑戦と挫折、そして発見](https://qiita.com/AxArc/items/a38568b55e711da64c94)  


この記事は、セキュリティ業務におけるAI活用の可能性を探る挑戦と、その過程で直面した具体的な課題、そして予期せぬ成果について書かれています。

筆者は、社内SE時代にセキュリティ専門知識が一部の人に集中し、深夜のアラート対応が属人化している現状に課題を感じていました。この経験から、「サイバー攻撃の状況を自然言語で入力すれば、危険度や対応方法を日本語で教えてくれるAIアシスタント」の開発を構想。これにより、専門家でなくてもAIと対話しながらインシデント対応ができる「民主化」を目指しました。

既存のセキュリティAIサービス（Microsoft Security Copilotなど）を調査した結果、日本語での対話能力や、特定の製品に依存しない柔軟性に課題があることを発見。そこで、「日本語で、対話的に、攻撃の”次の一手”を予測してくれる」AIを自ら開発することにしました。

目指したのは、SFT（Supervised Fine-Tuning：モデルを追加学習させる手法）、RAG（Retrieval-Augmented Generation：外部知識を参照して回答を生成する手法）、Agent（自律的に思考・行動するAI）を組み合わせたハイブリッドなシステムです。これにより、セキュリティの「思考パターン」をAIに学習させつつ、最新情報や社内固有の情報を動的に取り込むことで、実践的な支援ツールを作る狙いでした。

しかし、最大の難関は「日本語のセキュリティ学習データが存在しない」ことでした。筆者は、サイバー攻撃の知識ベースである「MITRE ATT&CK」をDeepL APIで日本語に翻訳し、さらに攻撃フェーズを示す「Cyber Kill Chain」と紐づける地道な作業を行いました。加えて、GPT-4などの生成AIを使って、この日本語知識から「初心者の質問と専門家の回答」形式の対話データセットを数千件作成しました。この大変な作業の中で、完成した「日本語化されたATT&CK × Kill Chainデータセット」自体が、LLM学習だけでなく様々な用途に使える貴重な財産となることに気づき、GitHubリポジトリで公開しています。

この後、構築したデータセットを用いてLLMのファインチューニング（モデルの微調整）に挑戦しますが、時間とコスト（高性能GPUの利用料）、セキュリティ専門用語の英語のニュアンス理解、モデル評価の難しさ、複雑な開発環境の構築といった、個人開発では乗り越えがたい大きな壁に直面し、一旦断念せざるを得ませんでした。

今後は、ファインチューニングという大規模なアプローチから、より現実的で柔軟なRAGを主軸としたアプローチに転換する予定です。既に作成した「日本語知識ベース」をRAGの知識ソースとして活用することで、LLMを再学習することなく情報を更新し、低コストで運用できるインシデント対応支援ツールの実現を目指しています。

この挑戦は、目標のAI開発には至らなかったものの、日本語の脅威インテリジェンスデータという貴重な成果と、個人でLLM開発を進める上でのリアルな知見を得られた、非常に有意義な経験であったと締めくくられています。

引用元: https://qiita.com/AxArc/items/a38568b55e711da64c94


- [LLMのAPIを活用したバックエンドアーキテクチャの事例を紹介します](https://zenn.dev/pharmax/articles/fed20b52182c58)  


この記事では、オンライン薬剤師相談サービス「YOJO」のバックエンドシステムで、LLM（大規模言語モデル）のAPIをどのように活用し、特にその「処理の遅さ」という課題にどう対応しているかを紹介しています。

YOJOでは、薬剤師が患者へ送るチャットの文章をAIが提案したり、システムが自動でメッセージを送るか判断したりする機能にLLMを使っています。バックエンドシステムはGoogle Cloud上にRuby on Railsで構築されており、LINEからのメッセージ処理や薬剤師向けアプリのAPI提供、そして時間のかかる処理は「非同期ワーカー」という仕組みで動いています。LLMとしてはOpenAIのGPT-3.5やGPT-4モデルを主に利用し、プロンプトの管理にはPromptLayerというツールを使っています。

LLMのAPIを使う上で最も大きな課題は「応答速度の遅さ」です。GPT-4モデルを使うと、処理に10秒近くかかることもあり、これは一般的なWeb APIの応答速度と比べると非常に遅いです。この「遅い」という特性を考慮し、システム設計では以下の点に注意が必要です。

1.  **適切な非同期処理**: ユーザーを長時間待たせないように、LLMを使った処理は、結果をすぐに返さず、裏側で実行する「非同期処理」にする必要があります。
2.  **他タスクとの隔離**: 時間のかかるLLM処理が、他の素早い処理の邪魔をしないよう、処理を分けたり、優先順位をつけたりすることが重要です。
3.  **割り込み処理の考慮**: LLMが処理中にユーザーが別の操作をした場合でも、データの不整合が起きないよう、最終的な結果を適用する際に現在の状況を確認する工夫が必要です。

これらの課題を踏まえ、記事ではLLMの処理をまるで時間のかかる「バッチ処理」のように扱うのが良いと提言しています。YOJOのバックエンドでは、LLMに関わる処理をSidekiqというツールを使って非同期で実行し、さらに「LLM処理のトランザクション管理」という仕組みでデータの整合性を保っています。この管理では、LLMジョブの開始判断（前処理）、LLM APIの実行（LLM処理）、そしてLLMからの結果に基づいた最終アクションの実行（後処理）を明確に分け、処理の途中で状況が変わっても正しく動作するように、現在の状態を確認しながら進めるように設計されています。

まとめると、LLMのAPIを利用するシステムでは、「LLMの処理は時間がかかる」ということをしっかり認識し、その上で「処理の状態を慎重に管理する」ことがとても大切だということが分かります。今回紹介された構成は、LLM特有の工夫というより、時間のかかる一般的な処理を扱う際の設計思想に近い考え方です。

引用元: https://zenn.dev/pharmax/articles/fed20b52182c58


- [武瑠、「BIBLE」を配信開始｜THE MAGAZINE](https://magazine.tunecore.co.jp/newrelease/536577/)  


2025年7月14日、様々なアーティストの新曲が配信開始。中でも武瑠さんが初のフルアルバム「BIBLE」をリリースし、自身の美学と多様なジャンルを融合した意欲作として注目です。さらに、新人エンジニアさんにも嬉しいニュースとして、人気キャラクター「ずんだもん」がフィーチャリングされた楽曲や、AIと本人がデュエットする「AIひまわりP」の画期的なラブソングも登場。最新技術の音楽活用にも注目です！

引用元: https://magazine.tunecore.co.jp/newrelease/536577/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:春日部つむぎ
