---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250724.mp3
audio_file_size: 0
date: 2025-07-24 05:00:00 +0900
description: 'FastVLM: Efficient Vision Encoding for Vision Language Models、kimi-k2-instruct Model by Moonshotai  NVIDIA NIM、LLM Servingを支える技術'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250724
---

## 関連リンク


- [FastVLM: Efficient Vision Encoding for Vision Language Models](https://machinelearning.apple.com/research/fast-vision-language-models)  


この研究記事は、Appleが開発した新しいAIモデル「FastVLM」について紹介しています。FastVLMは、画像とテキストを組み合わせて理解する「Vision Language Models（VLM）」の長年の課題を解決するものです。

VLMは、画像の詳細を理解するために高解像度の画像を処理することが重要です。例えば、道路標識の小さな文字を読み取ったり、書類の内容を分析したりする際に、解像度が高いほどAIの認識精度は向上します。しかし、これまでのVLMでは、高解像度の画像を処理しようとすると、AIが最初の回答を生成するまでの時間（「Time-to-First-Token」、TTFTと略されます）が大幅に長くなるという問題がありました。これは、画像をAIが理解できる形に変換する部分（「ビジョンエンコーダ」）の処理に時間がかかりすぎたり、変換されたデータ（「ビジュアルトークン」）が多くなりすぎて、次にテキストを処理する大規模言語モデル（LLM）の負荷が増えたりするためです。特に高解像度になるほど、ビジョンエンコーダがボトルネックとなることが課題でした。

FastVLMは、この「精度を上げると遅くなる」というトレードオフを劇的に改善しました。その鍵となるのが、新しく設計されたビジョンエンコーダ「FastViTHD」です。FastViTHDは、従来の画像処理技術である「畳み込みニューラルネットワーク（CNN）」と、最近のAIモデルで使われる「Transformer」の良いところを組み合わせた「ハイブリッドアーキテクチャ」を採用しています。これにより、高解像度の画像を効率的に処理しながら、AIが理解するための「高品質で少ない数のビジュアルトークン」を生成できるようになりました。結果として、LLMの処理負荷も軽減され、全体の応答速度が向上します。

様々な実験により、FastVLMの優位性が示されています。既存の多くのビジョンエンコーダや、画像を効率的に処理しようとする他の手法（「トークン剪定」や「ダイナミックタイリング」など）と比較しても、FastVLMは高い精度を保ちつつ、応答速度で大幅に優れていることが確認されました。例えば、同じくらいのサイズの他の人気VLMと比べると、最大で約85倍も高速でありながら、より高い精度を実現しています。

FastVLMの技術は、リアルタイムで画像を理解する必要があるアプリケーションや、プライバシー保護のためにユーザーのデバイス内でAIを動かす「オンデバイスAI」に特に適しています。Appleは実際に、FastVLMがiPhone上でほぼリアルタイムに動作するiOS/macOS向けのデモアプリも公開しており、その実用性を示しています。

このFastVLMは、VLMの応用範囲を大きく広げる可能性を秘めており、今後様々な場所で活用されていくことが期待されます。

引用元: https://machinelearning.apple.com/research/fast-vision-language-models


- [kimi-k2-instruct Model by Moonshotai  NVIDIA NIM](https://build.nvidia.com/moonshotai/kimi-k2-instruct#new_tab)  


AIモデル利用のためのプラットフォーム「NVIDIA NIM」に、最先端の新しい大規模言語モデル（LLM）である「kimi-k2-instruct」が追加されました。このモデルはMoonshotai社が開発したもので、現在プレビュー版として公開されています。

「kimi-k2-instruct」は、「Mixture-of-Experts（MoE）」という最新のアーキテクチャを採用したオープンなモデルです。MoEは、複数の異なる専門分野を持つAI（「エキスパート」）を組み合わせて処理を行うことで、より効率的かつ高性能な結果を出すことができる技術として、近年特に注目されています。このモデルは、複雑な推論（Reasoning）能力、プログラミングコードの生成や理解といったコーディング（Coding）能力、そして自律的にタスクを遂行するエージェント機能（Agentic Capabilities）において、非常に高い性能を持つとされています。

新人エンジニアの皆さんにとって、このような最先端のLLMがNVIDIA NIMのようなプラットフォームで手軽に試せるようになることは、AI開発の可能性を実感する良い機会です。実際にウェブ上でこのモデルを動かし、温度（Temperature）やTop Pなどのパラメータを調整しながら、その出力の特性や挙動を試すことができます。これにより、LLMがどのように動作し、どのような調整が可能か、実践的に学ぶことが可能です。

また、「オープンなモデル」であるという点も重要です。これは、研究者や開発者がモデルの内部構造を理解し、さらに改善したり、特定の用途に特化させたりすることが容易になることを意味します。商用利用の道も開かれるため、新たなAIアプリケーションやサービスが生まれるきっかけにもなり得ます。

ただし、NVIDIA NIM上で提供されるモデルは、NVIDIA以外の第三者が開発したものが多く、「kimi-k2-instruct」もその一つです。そのため、モデルが生成するコンテンツには、政治的な意見や誤った情報、あるいはセキュリティ上の問題、特定の視点に偏ったバイアスなどが含まれる可能性があることに注意が必要です。NVIDIAはあくまでプラットフォームの提供者であり、これらの第三者モデルの出力内容について責任を負うものではないと明記されています。

このように、高性能なMoEモデルがオープンに提供され、手軽に試せるようになることは、LLM技術の進化を加速させ、私たちの身の回りでのAI活用をさらに広げていく重要な一歩と言えるでしょう。最新技術に触れ、実際に動かしてみることで、AI開発の最前線を体験してください。

引用元: https://build.nvidia.com/moonshotai/kimi-k2-instruct#new_tab


- [LLM Servingを支える技術](https://zenn.dev/kotoba_tech/articles/98feb05f24c082)  


この記事は、私たちが日々利用するLLM（大規模言語モデル）のサービスが、どのようにして高速かつ効率的に動いているのかを解説しています。現在のLLMは非常に大きく、動かすために莫大な計算とメモリが必要です。このため、多くのユーザーにスムーズなサービスを提供するには、LLMの推論（予測）をいかに効率良く行うかが重要になります。

まず、効率化の基本的な考え方として「**バッチ推論**」が挙げられます。これは、複数のユーザーからのリクエストをまとめてGPUという計算装置で一気に処理する技術です。これにより、GPUの性能を最大限に引き出すことができます。「Continuous Batching」という技術は、リクエストの処理が終わるたびにすぐに次のリクエストを割り当てることで、GPUが常に忙しく働き続けられるように工夫されています。

LLMが推論を行う際に一時的にデータを保存する「**KVキャッシュ**」は、非常に多くのメモリを消費します。このメモリを効率よく管理するために、「PagedAttention」という技術が使われています。これは、コンピュータのOSがメモリを管理する方法（ページング）に似ており、必要な分だけKVキャッシュ用のメモリを割り当て、メモリの無駄をなくします。また、複数のリクエストで共通する部分（例：プロンプト）のKVキャッシュを再利用する「Prefixキャッシュ」も、メモリの節約に貢献します。

次に、プログラムの実行効率を高める「**実装の工夫**」が紹介されています。GPUの計算を高速化するため、「FlashAttention」のように、GPU内でのデータのやり取りを最小限に抑える「CUDAカーネルの最適化」が行われます。また、GPUだけでなく、CPU側での処理も遅延の原因とならないよう、「CUDA Graph」で計算の実行計画を最適化したり、CPUとGPUの処理を「非同期」で進めたりする工夫も重要です。ユーザーへの応答速度（レイテンシ）と処理量（スループット）のバランスを取るため、長い入力の処理を分割したり、入力処理と出力生成を異なるGPUで行ったりする「スケジューリング」の技術も進化しています。

さらに、LLMのモデルやアルゴリズムそのものを工夫するアプローチもあります。「**量子化**」は、モデルのサイズを小さくする技術で、メモリ消費を減らし、高速な計算を可能にします。また、「**投機的デコーディング**」は、軽量なモデルで事前にトークン（単語の断片）を予測し、それを大きなLLMでまとめて検証することで、生成速度を大幅に高めます。モデルの設計では、「MQA/GQA/MLA」といったAttentionヘッドの構造を工夫することで、KVキャッシュのメモリ効率を向上させています。

最後に、超大規模なLLMを動かすためには、「**分散処理**」が不可欠です。これは、モデルを複数のGPUやサーバーに分割して処理する技術で、「データ並列」「テンソル並列」「パイプライン並列」など様々な手法があります。これらの技術を組み合わせることで、巨大なLLMでも多くのユーザーにサービスを提供できるシステムが構築されています。例えば、DeepSeekという大規模LLMの推論システムでは、入力処理と出力生成を異なるサーバー群で分担し、それぞれに最適な並列化を適用することで、超高速な運用を実現しています。

これらの最先端の技術が組み合わさることで、LLMは私たちの生活をより豊かにするサービスとして提供されているのです。

引用元: https://zenn.dev/kotoba_tech/articles/98feb05f24c082



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
