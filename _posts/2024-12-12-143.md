---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/株式会社ずんだもん技術室AI放送局_podcast_20241212_gemini_2_flash.mp3
audio_file_size: 0
date: 2024-12-12 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
Command: A new tool for building multi-agent architectures in LangGraph、Introducing Gemini 2.0: our new AI model for the agentic era、ScreenAI: A visual language model for UI and visually-situated language understanding、娘「パパ！私Vtuberになりたいの！」父「…前使ってた良いPCがある。使いなさい」娘「パパ…！」父「前使ってたWEBカメラがある。使いなさい」娘「パパ…！！」'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20241212 (gemini 2.0 flash exp生成version 特別放送)
---

## 関連リンク


- [Command: A new tool for building multi-agent architectures in LangGraph](https://blog.langchain.dev/command-a-new-tool-for-multi-agent-architectures-in-langgraph/)  


LangGraphに、マルチエージェント間のコミュニケーションを円滑にする新しいツール「Command」が追加されました。LangGraphはイベント駆動型のシステムで、グラフベースの開発者体験を提供します。従来のLangGraphでは、ノードとエッジでグラフを表現していましたが、より動的なロジックを表現するために、ノードが次に実行するノードを動的に指定できる「Command」が導入されました。これにより、ノードは状態の更新だけでなく、次に実行するノードを直接制御できます。

「Command」は、マルチエージェントアーキテクチャにおける「ハンドオフ」を容易にします。あるエージェントから別のエージェントへ制御を移す際、どのノードにジャンプするかを簡単に指定できます。また、親グラフのノードへのジャンプも可能で、階層的なエージェントアーキテクチャでのコミュニケーションが改善されます。

この新機能により、LangGraphはエージェント間のコミュニケーションをより柔軟に制御できるようになり、マルチエージェントシステムの構築が容易になります。


引用元: https://blog.langchain.dev/command-a-new-tool-for-multi-agent-architectures-in-langgraph/


- [Introducing Gemini 2.0: our new AI model for the agentic era](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)  


Google DeepMindが、エージェント時代に向けた新しいAIモデル「Gemini 2.0」を発表しました。Gemini 2.0は、従来のモデルよりも高性能で、画像や音声のネイティブ出力、ツール利用機能が強化されています。開発者や信頼できるテスター向けに「Gemini 2.0 Flash」が提供されており、来年初めには一般公開が予定されています。Googleは、Gemini 2.0を活用したエージェント体験として、Project Astra、Project Mariner、Julesといったプロジェクトを推進しています。これらのプロジェクトでは、AIがユーザーの代わりにタスクを実行したり、より複雑な問題を解決したりする能力を探求しています。特に、Project Astraは、マルチモーダルな理解に基づいた汎用AIアシスタント、Project Marinerはブラウザ上でのエージェント操作、Julesは開発者向けのコードエージェントとして期待されています。Gemini 2.0は、GoogleのAI製品に順次搭載される予定で、より便利で使いやすいAI体験を提供することを目指しています。また、GoogleはAIの責任ある開発にコミットしており、安全性とセキュリティを最優先事項としています。


引用元: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/


- [ScreenAI: A visual language model for UI and visually-situated language understanding](https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/)  


ScreenAIは、UI（ユーザーインターフェース）やインフォグラフィックを理解するための視覚言語モデルです。PaLIアーキテクチャをベースに、pix2structの柔軟なパッチ戦略を取り入れ、多様なデータセットで学習されています。特に、UI要素（種類、位置、説明）を特定するScreen Annotationタスクが特徴です。このモデルは、質問応答、UIナビゲーション、要約などのタスクにおいて、同規模のモデルと比較して高い性能を発揮します。また、Screen Annotation、ScreenQA Short、Complex ScreenQAという3つの新しいデータセットも公開されています。

ScreenAIのアーキテクチャは、画像とテキストの埋め込みを組み合わせたマルチモーダルエンコーダと、テキストを生成する自己回帰デコーダで構成されています。学習は2段階で行われ、自己教師あり学習でデータラベルを生成し、その後、人間がラベル付けしたデータでファインチューニングされます。データ生成には、様々なデバイスのスクリーンショットを使用し、UI要素の特定にはDETRモデル、アイコン分類には専用の分類器が用いられています。さらに、PaLM 2のようなLLMを活用して、質問応答、画面ナビゲーション、要約タスクのための合成データを生成しています。

実験結果では、ScreenAIはWebSRCやMoTIFなどのUIベースのタスクで最先端の性能を示し、ChartQA、DocVQA、InfographicVQAなどのタスクでも同規模のモデルの中で最高の性能を達成しています。モデルサイズを大きくすることで性能が向上することも確認されており、今後の更なる発展が期待されます。


引用元: https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/


- [娘「パパ！私Vtuberになりたいの！」父「…前使ってた良いPCがある。使いなさい」娘「パパ…！」父「前使ってたWEBカメラがある。使いなさい」娘「パパ…！！」](https://togetter.com/li/2479369)  


娘がVtuberになりたいと言い出したところ、父親が以前使っていたPCやWebカメラ、ボイスチェンジャーなどを提供するという、微笑ましいやり取りがSNSで話題になっています。この投稿に対し、他のユーザーからは「父親は過去にVtuberをしていたのでは？」という推測や、父親が娘の活動を様々な形でサポートするであろうというユーモラスなコメントが多数寄せられています。中には、父親が以前使っていたアバターやチャンネル、衣装まで提供するのではないかというジョークも飛び出し、ネット上ではこの親子の今後の展開に期待が高まっています。


引用元: https://togetter.com/li/2479369



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
