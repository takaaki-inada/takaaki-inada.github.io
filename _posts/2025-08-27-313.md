---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250827.mp3
audio_file_size: 0
date: 2025-08-27 05:00:00 +0900
description: '&quot;あの頃&quot;の強かったClaude Codeを少しでも取り戻す方法、Block unsafe prompts targeting your LLM endpoints with Firewall for AI、ローカル LLM 環境を構築する、もともと思考力が高い人は生成AIを使い倒すことができるが、批判的思考に慣れていない人は、AIを使えば使うほど、思考力が弱くなるという研究結果が出てしまう→残酷過ぎて笑う'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250827
---

## 関連リンク


- ["あの頃"の強かったClaude Codeを少しでも取り戻す方法](https://zenn.dev/discus0434/scraps/e0b1a0aa5406eb)  


この記事では、AIエンジニアの間で「性能が落ちた」と話題のClaude Codeの力を取り戻すための具体的な方法が紹介されています。新人エンジニアの方も、高性能なAIアシスタントを使いこなすためのヒントとしてぜひ読んでみてください。

**なぜClaude Codeの性能が落ちたように感じるのか？**
大きな理由として、Claude Codeが長い会話の履歴（コンテキスト）を効率的に処理しようとして、過去に参照したファイルの内容やツールの使用ログを自動的に「要約（Micro Compact）」するようになったことが挙げられます。これは、AIの処理コストを減らすためですが、結果としてAIが以前の情報を忘れ、「健忘症」のような振る舞いを引き起こす可能性があります。また、AIが「何を考えているか」を示す思考プロセスが簡略化され、意図しない挙動に気づきにくくなったことも一因と考えられています。

**性能を取り戻すための対策**

1.  **Micro Compactの無効化**:
    前述のコンテキスト要約機能（Micro Compact）を`DISABLE_MICROCOMPACT=1`という環境変数設定で停止し、AIが情報を忘れにくくします。
2.  **思考の表示をレガシーに戻す**:
    AIの思考過程が見えやすくなるように、`DISABLE_INTERLEAVED_THINKING=1`で表示方法を元に戻します。これにより、AIがなぜそのように動いたのかを把握しやすくなります。
3.  **IDE（開発環境）との統合を避ける**:
    IDEとClaude Codeの連携によって、不必要な情報がコンテキストに送られるのを防ぎます。具体的には、関連する拡張機能をアンインストールし、`CLAUDE_CODE_AUTO_CONNECT_IDE=0`などの環境変数設定で連携を停止します。これは利便性とのトレードオフになる可能性があります。
4.  **オートアップデート・テレメトリーの停止**:
    `DISABLE_AUTOUPDATER=1`などで自動更新や利用状況の送信（テレメトリー）を止めます。これにより、予期せぬ機能変更や、不要な通信を防ぐことができます。
    これらの設定は、`~/.claude/settings.json`ファイルにまとめて記述できます。
5.  **Claude Codeのバージョンダウン**:
    最新の機能は使えなくなりますが、最も確実に以前の性能を取り戻す方法として、`npm uninstall`と`npm install`コマンドで過去の安定バージョン（例: 1.0.24）に戻すことが提案されています。

**さらに性能を引き出すテクニック**

*   `serena MCP`の導入：特定のモデル（MCP）を追加することで、AIの振る舞いを改善できる場合があります。
*   `MAX_THINKING_TOKENS`の増加：AIがより深く考えるためのトークン数を増やし、推論能力を高めます。
*   関連ファイルを強制的にコンテキストに含める：`@relative/path/to/file`のように指定し、AIに必ず読ませたいファイルを明示的に与えます。これにより、コンテキストの消費は早まりますが、正確性が向上します。
*   大きなファイルを一度に読ませる：Claudeが長いファイルを少しずつ読む癖があるため、`CLAUDE.md`などで一度に全て読ませるよう指示し、全体像を見失わないようにします。

これらの対策を試すことで、Claude Codeをより効果的に活用し、開発作業の効率アップに繋がるかもしれません。

引用元: https://zenn.dev/discus0434/scraps/e0b1a0aa5406eb


- [Block unsafe prompts targeting your LLM endpoints with Firewall for AI](https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/)  


大規模言語モデル（LLM）を使ったアプリケーションが広がる中で、セキュリティの課題も増えています。悪意あるユーザーがLLMに不正な指示（プロンプトインジェクションなど）をすることで、企業の機密情報が漏れたり、AIが誤った情報を学習したり、有害なコンテンツを生成してしまうリスクがあるのです。これは、大事なAIが意図せず悪い役割を演じてしまうようなものです。

この問題に対応するため、Cloudflare（クラウドフレア）は、AIアプリケーション向けのセキュリティサービス「Firewall for AI」に、新たに「不適切なコンテンツモデレーション」機能を追加しました。この機能は、メタ社が開発した「Llama Guard」というAIモデルを利用し、ユーザーがLLMに送るプロンプトの内容をリアルタイムでチェックします。

主な機能とメリットは以下の通りです。
- **有害プロンプトのブロック**: ヘイトスピーチ、暴力的な表現、性的コンテンツなど、様々な不適切なプロンプトをLLMに到達する前に検出・ブロックします。これにより、AIが不適切な応答をするのを防ぎ、ブランドイメージやユーザーの信頼を守ります。
- **データ漏洩防止と悪用対策**: 個人情報（PII）の開示を防ぎ、プロンプトインジェクションによるLLMの予期せぬ動作を抑制します。
- **簡単な導入と柔軟な対応**: アプリケーションのコード変更やインフラ設定は不要で、Cloudflareのエッジネットワークで設定するだけで利用可能です。OpenAIやGoogle Gemini、自社開発モデルなど、特定のLLMに依存せず、あらゆるLLMに対して一貫したセキュリティポリシーを適用できます。
- **一元管理**: 複数のLLMを運用している場合でも、単一の場所でセキュリティルールを定義し、適用できるため、管理が効率化されます。

かつてマイクロソフトのAIチャットボット「Tay」が不適切な入力によって問題発言を繰り返した事例からも、LLMへの入力段階でのフィルタリングの重要性がわかります。

Cloudflareは、高性能なAIインフラ「Workers AI」上でLlama Guard 3モデルを稼働させ、ユーザー体験を損なわない低遅延でのプロンプト分析を実現しています。管理画面では、検出された不適切なプロンプトに対するログ記録やブロックといったルールを柔軟に設定できます。

将来的には、プロンプトインジェクションやAIの脱獄（ジェイルブレイク）といった高度な攻撃の検出機能、さらにはLLMの応答内容をチェックする機能も追加される予定です。LLMを安全かつ信頼性高く利用していく上で、このようなエッジでのセキュリティ対策は、AIアプリケーションの健全な発展に不可欠な基盤となるでしょう。

引用元: https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/


- [ローカル LLM 環境を構築する](https://qiita.com/kkawaharanet/items/b80d7cde49364aa94963)  


この記事では、自分のパソコン（ローカル環境）で大規模言語モデル（LLM）を動かすための環境構築方法が、具体的な手順と共に紹介されています。クラウドサービスに頼らずにLLMを動かすことで、費用を抑えたり、プライバシーを守りながら気軽にLLMを試したりできるのが大きなメリットです。最新のAI技術であるLLMを自分の手で動かし、その可能性を体験したい新人エンジニアの方にとって、実践的なガイドとなるでしょう。

環境構築には主に三つのツールを使います。まず「**Docker**」は、アプリケーションを「コンテナ」という、他の部分から隔離された小さな箱に入れて動かす技術です。これにより、複雑な環境設定の手間を減らし、どんなパソコンでも同じように安定して動かせるようになります。次に「**Ollama**」は、ローカルでLLMを実行するための強力なツールで、たくさんのLLMモデルの中から好きなものを選んでダウンロードし、手元のPCで実行できます。そして「**Open WebUI**」は、Ollamaで動かしているLLMとブラウザを通じてチャット形式でやり取りするための、使いやすい画面（GUI）を提供します。

具体的な手順としては、まずDockerをインストールし、もしNVIDIA製の高性能なグラフィックボード（GPU）を使っているなら、「NVIDIA Container Toolkit」を導入することで、LLMの処理速度を大幅に向上させることができます。次に、`docker-compose.yml`という設定ファイルを作成し、Dockerを使ってOllamaとOpen WebUIを同時に起動します。このファイルに書かれた内容を実行するだけで、必要なサービスが一気に立ち上がり、環境が整います。

サービスが起動したら、Ollama経由で「gpt-oss」のような好きなLLMモデルをダウンロードしてインストールします。モデルによってはデータサイズが大きいため、ダウンロードには時間がかかる場合があります。モデルの準備ができたら、Webブラウザで指定されたアドレス（通常は`http://localhost:3000/`）にアクセスし、Open WebUIの画面でアカウントを作成します。これで、まるでChatGPTを使うように、自分のローカル環境で動くLLMに質問を投げかけ、その応答を楽しむことができるようになります。ぜひこの手順を参考に、ローカルLLM環境の構築にチャレンジしてみてください。

引用元: https://qiita.com/kkawaharanet/items/b80d7cde49364aa94963


- [もともと思考力が高い人は生成AIを使い倒すことができるが、批判的思考に慣れていない人は、AIを使えば使うほど、思考力が弱くなるという研究結果が出てしまう→残酷過ぎて笑う](https://posfie.com/@kalofollow/p/o7zk54Y)  


最新の研究によると、生成AIの利用には注意が必要です。もともと批判的思考力が高い人はAIを効果的に活用し、作業効率を高められます。しかし、批判的思考に慣れていない人がAIに頼りすぎると、自分で考える機会が減り、思考力が弱まってしまう可能性があると指摘されています。新人エンジニアの皆さんは、AIの答えを鵜呑みにせず、常に「本当にこれで正しいか？」と問いかけ、自分で検証する力を意識して磨きましょう。AIは便利なツールですが、使いこなすにはあなたの『考える力』が鍵となります。

引用元: https://posfie.com/@kalofollow/p/o7zk54Y



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
