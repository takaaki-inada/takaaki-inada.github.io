---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251113.mp3
audio_file_size: 0
date: 2025-11-13 05:00:00 +0900
description: 'AIエージェントメモリの話、RAGの検索結果を並び替えるだけで高速化する手法、Ollamaと行くローカルLLMの道'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251113
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/25Ke6xkPOC8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [AIエージェントメモリの話](https://www.docswell.com/s/harinezumi/KJQPRX-2025-11-12-083604)  


AIエージェントがまるで人間のように「記憶」を持つにはどうすれば良いか、その技術的な仕組みについて解説した記事ですね。新人エンジニアの方も、この要約を読めばAIエージェントの頭の中が少し理解できるようになるはずです。

まず、AIエージェントは「記憶」そのものを持っているわけではありません。実は、大規模言語モデル（LLM）が会話の流れを理解するために、必要な情報を一時的に「コンテキストウィンドウ」と呼ばれる場所に詰め込んでいるだけなんです。しかし、この窓の大きさには限りがあるため、過去の会話全てを記憶することはできません。

そこで、AIエージェントのメモリは大きく3つの層で管理されています。
1.  **短期メモリ（会話履歴）**: 直近の会話を覚えておく部分です。これはLLMのコンテキストウィンドウに直接入力されます。
2.  **長期メモリ（セマンティックリコール）**: 過去の膨大な会話の中から、現在の会話と「意味的に関連性の高い情報」を検索して取り出す仕組みです。このために、会話内容を数値のベクトル（埋め込み）に変換し、似たようなベクトルを探す「ベクトル検索」という技術が使われます。
3.  **ワーキングメモリ**: 特定のユーザーに関する情報など、会話全体を通じて永続的に保持・更新したい情報を管理する部分です。これはMastraというサービスで特に注目されています。

これらのメモリ管理機能は、AWSのAmazon Bedrock Agentsや、国内のAI開発プラットフォームであるMastraといったサービスで提供されています。特に、Amazon Bedrock AgentCore Memoryは2025年10月に正式リリースされたマネージドサービスで、短期・長期記憶を統合的に管理し、豊富なAPIで様々なユースケースに対応できるようになっています。Mastraでは、このAgentCore MemoryのAPIを呼び出す「ツール」をAIエージェントに組み込むことで、より賢く振る舞うAIエージェントを開発できる事例が紹介されています。実際のコードもGitHubで公開されており、皆さんの開発の参考になるでしょう。

将来的には、現在のメモリ技術には限界があり、時間軸を考慮した「Temporal Knowledge Graph」のような、より高度な記憶管理方法が研究されています。AIエージェントが本当に賢くなるためには、この「記憶」の進化がカギとなるでしょう。

引用元: https://www.docswell.com/s/harinezumi/KJQPRX-2025-11-12-083604


- [RAGの検索結果を並び替えるだけで高速化する手法](https://zenn.dev/knowledgesense/articles/687dce3199d11f)  


### RAGと高速化の必要性
RAG（Retrieval Augmented Generation）は、大規模言語モデル（LLM）が質問に答える際、外部の知識ベースから関連情報を「検索」し、それに基づいて回答を「生成」する技術です。これにより、LLMが学習していない最新情報や特定のデータにも対応できるようになり、LLMの弱点を補強します。しかし、RAGには課題があります。LLMに渡す情報（これを「コンテキスト」と呼びます）が長くなると、LLMがその情報を処理するのに時間がかかり、回答が遅くなりがちです。また、LLMへの入力が増えると、利用コストも高くなります。特に、複数のRAG処理を組み合わせて複雑なタスクをこなす「Agent」システムでは、RAGを何度も使うため、この速度やコストの問題が顕著になります。

### 「RAGBoost」：2つの工夫でRAGを速くする
今回紹介する「RAGBoost」は、このRAGの処理速度とコストを改善するための新しい手法です。主に二つの「キャッシュ」（一度使った情報を一時的に保存しておき、次から再利用する仕組み）を賢く活用することで、RAGを効率的に高速化します。

#### 1. 過去の検索結果を効率的に再利用する
AgentのようにRAGを繰り返し使う場合、実は同じような情報源（ドキュメント）を何度も検索結果として取得することがよくあります。RAGBoostでは、一度LLMに渡したドキュメントは、次からはその内容全体ではなく「ID」で、「これは前に見たDoc ID XXX番と同じ情報だよ」と伝えます。
これは、初めて読む本はすべて読みますが、前に読んだことがある本なら「あの青い表紙の本と同じ内容だよ」と伝えるだけで済ませるイメージです。これにより、LLMが毎回同じドキュメントの全文を再処理する手間が省け、処理が速くなり、LLMへの入力トークン数が減るためコストダウンにもつながります。

#### 2. LLMのキャッシュ機能を最大限に活かす
LLMには、入力された文章の「冒頭部分が全く同じ」であれば、以前の計算結果を再利用できる「キャッシュ」機能があります。つまり、もし全く同じ文章を二度入力したら、二度目の処理が速くなるということです。
RAGBoostでは、このLLMの特性を巧妙に利用します。具体的には、RAGが検索してきた複数のドキュメントをLLMに渡す際、以前の入力と「できるだけ冒頭部分が一致する」ようにドキュメントの並び順を工夫して入れ替えます。
例えば、前回「ドキュメントA、ドキュメントB、ドキュメントC」の順でLLMに情報を渡したとします。今回「ドキュメントD、ドキュメントA、ドキュメントB」という結果が検索されたら、RAGBoostはこれを「ドキュメントA、ドキュメントB、ドキュメントD」のように並び替えてLLMに渡します。こうすることで、「ドキュメントA、ドキュメントB」の部分についてはLLMのキャッシュが効き、再計算の無駄がなくなるため、高速化とコスト削減が見込めます。

### 確かな改善効果
これらのRAGBoostの工夫により、RAGの回答精度を維持しながら、情報の処理速度（Prefill Throughput）や、LLMが最初の言葉を出力するまでの時間（First Token Latency）を大幅に改善できることが実験で示されています。

### まとめ
RAGBoostは、RAGシステム運用における処理速度やコストの課題を、過去の情報の効率的な再利用とLLMのキャッシュ機能を活用することで解決する、画期的な手法です。RAGを開発・運用するエンジニアにとって、非常に役立つ技術であり、RAGだけでなく他のLLMを用いたAgentシステムにも応用できる可能性を秘めているため、今後のAI開発において注目すべきアプローチと言えるでしょう。

引用元: https://zenn.dev/knowledgesense/articles/687dce3199d11f


- [Ollamaと行くローカルLLMの道](https://zenn.dev/sileader/articles/7a8201c8098ae7)  


この記事は、ローカル環境で手軽に大規模言語モデル（LLM）を動かすためのオープンソースツール「Ollama」について、新人エンジニアの方でも分かりやすく解説しています。

### Ollamaとは？
Ollamaは、自分のパソコン上でLLMを動かすための実行環境です。まるでDockerのように、LLMのモデルを簡単にダウンロードしたり、使ったり、管理したりできます。Windows、macOS、Linuxに対応しており、公式サイトから手軽にインストールできます。

### LLMを動かしてみよう
Ollamaを使えば、人気のあるLLMモデルをすぐに試せます。例えば、Googleの「gemma3:4b」モデルを動かすには、以下の2つのコマンドを実行するだけです。
1. `ollama pull gemma3:4b`：モデルをダウンロード
2. `ollama run gemma3:4b`：モデルを使ってチャットを開始
これだけで、手軽にLLMとの会話が楽しめます。

### もっと活用するならAPIも
OllamaはWeb APIも提供しており、独自のAPIとOpenAI API互換のAPIの2種類があります。これにより、Ollamaで動かしているLLMを自分のアプリケーションに組み込んだり、PythonやJavaScriptなどのプログラミング言語から簡単に操作したりできます。

### Ollamaの裏側とカスタムモデル作成
Ollamaは内部で「llama.cpp」という軽量なLLM実行エンジンを使用しています。このエンジンは「GGUF」という形式のモデルを扱い、モデルを圧縮する「量子化」も行えるため、より少ないリソースでもLLMを動かせます。

さらに、Ollamaでは自分だけのカスタムLLMモデルを作ることも可能です。
1. **llama.cppの準備**: まずllama.cppをダウンロードし、モデル変換に必要なツールを準備します。
2. **GGUF変換**: Hugging Faceなどで公開されているモデルを、llama.cppのスクリプトを使ってGGUF形式に変換します。
3. **量子化**: 変換したモデルをさらに量子化することで、ファイルサイズを小さくし、より効率的に動かせるようにします。
4. **Modelfileの作成**: DockerのDockerfileに似た「Modelfile」という設定ファイルを作成します。このファイルで、どのGGUFモデルを使うか、チャットの挙動（プロンプトの形式など）をどうするかなどを定義します。
5. **モデルの作成と実行**: 作成したModelfileを使って`ollama create`コマンドで新しいモデルを作り、`ollama run`で実行できます。

### 重要な注意点
Ollamaで利用できるLLMモデルには、それぞれライセンスが設定されています。個人的な利用では問題なくても、商用利用には制限があるモデルも存在します。モデルを使用する際は、必ずライセンス情報を確認するようにしましょう。

引用元: https://zenn.dev/sileader/articles/7a8201c8098ae7



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
