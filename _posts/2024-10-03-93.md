---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/株式会社ずんだもん技術室AI放送局_podcast_20241003.mp3
audio_file_size: 0
date: 2024-10-03 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
妻がVtuberになった話、GitHub - slashml/amd_inference、PyTorch Conference 2024 Recap: On Fire 🔥'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20241003
information: 
---

## 関連リンク


- [妻がVtuberになった話](https://anond.hatelabo.jp/20241002024542)  



結婚2年目の専業主婦である妻が、突如としてVtuberになることを宣言しました。最初は戸惑った夫でしたが、妻の熱意と、現在では比較的簡単にVtuberデビューできる状況を理解し、応援することにしました。

妻は事前にアバター作成、マイク、配信ソフトの準備など、綿密な計画を立てており、夫は妻の本気度を改めて認識しました。初配信はゲーム実況で、予想をはるかに超える再生数を記録。特に、ある配信では1万再生を達成するなど、大きな成功を収めました。

配信中のコメントには「可愛い」「声が癒される」といった賞賛が多数寄せられ、夫は妻が既婚者であることを視聴者が知らないという状況に、一種の優越感と自己肯定感の高まりを感じました。さらに、スーパーチャット（投げ銭）も増加し、妻の活動が経済的にも成功していることを実感しました。

夫は、妻のVtuber活動の成功と、その秘密を知っている自分自身の特殊な立場を、心地よく感じているようです。  妻はVtuberとしての成功を、夫は妻の成功と秘密を知る立場をそれぞれ楽しんでいる、という内容です。


引用元: https://anond.hatelabo.jp/20241002024542


- [GitHub - slashml/amd_inference](https://github.com/slashml/amd_inference)  



このGitHubリポジトリ`slashml/amd_inference`は、AMD GPU上で大規模言語モデル(LLM)を実行するためのDockerベースの推論エンジンを提供しています。Hugging Faceから入手可能なモデル、特にLLaMAモデルファミリーとの連携を重視した設計です。

**主な機能:**

* AMD GPUを用いたLLM推論の実行を可能にします。
* Hugging Faceのモデルを容易に利用できます。
* Dockerコンテナ内で動作するため、環境構築が容易です。

**前提条件:**

* AMD ROCm対応GPU
* Docker
* ホストシステムにROCmドライバー(バージョン5.4.2以上推奨)のインストール

**リポジトリ構成:**

ソースコード、Dockerfile、必要パッケージリスト(requirements.txt)、実行スクリプト等が含まれています。

**ライセンス:**

Apache-2.0ライセンス

**制約:**

このプロジェクトは、現在開発中である可能性があります(READMEに明記されていない場合でも、コミットログから判断できます)。  そのため、機能や安定性においては、本番環境での利用には十分な注意が必要です。 また、具体的な使用方法や高度なカスタマイズ方法は、GitHubリポジトリのREADME.md等を参照する必要があります。この要約では、詳細な使用方法やトラブルシューティングについては触れていません。


新人エンジニアの方へ: このプロジェクトは、AMD GPUを活用してLLMを高速に実行したい場合に役立ちます。Dockerを使用することで、環境構築の手間を省くことができます。ただし、開発中のプロジェクトである可能性があるため、利用にあたっては注意深くREADMEなどを確認し、必要に応じて修正や改良を行う必要があるかもしれません。  不明な点があれば、GitHubのIssue機能を利用して質問することをお勧めします。


引用元: https://github.com/slashml/amd_inference


- [PyTorch Conference 2024 Recap: On Fire 🔥](https://pytorch.org/blog/pytorch-conference-2024-recap/)  



2024年サンフランシスコで開催されたPyTorch Conferenceには、約1500人のAI研究者、開発者、愛好家が参加しました。2日間に渡り、人工知能(AI)と主要なオープンソース機械学習フレームワークであるPyTorchの進歩に焦点を当てた、活気のある議論、洞察に富んだ基調講演、実践的なセッションが行われました。参加者は、生成AI、大規模言語モデル(LLM)、そしてオープンソース技術がAIイノベーションを推進する上で果たす重要な役割について深く探求しました。

会議の主要テーマは3つありました。

1. **生成AIとLLM:**  多くのセッションでは、PyTorchが大規模言語モデルと生成AIアプリケーションの主要なフレームワークとしてどのように進化し続けているかに焦点を当てていました。これらのモデルのスケーリングから、様々なハードウェアプラットフォームでのパフォーマンスの最適化まで、LLMアーキテクチャにおける継続的な進歩と課題が示されました。

2. **オープンソースを通じたAIの民主化:** オープンソースツールとコミュニティがAIの未来を形作る上で重要であるというテーマが繰り返し取り上げられました。PyTorchは、あらゆるレベルの開発者にとっての包括性、使いやすさ、アクセシビリティに尽力しており、より多くのグローバルなオーディエンスにAIをもたらすことに重点を置いています。

3. **分散型コンピューティングとエッジコンピューティング:** 分散型コンピューティングとエッジ展開に関する議論が多く行われ、PyTorchがAIをエッジに推進するためにどのように使用されているかが強調されました。エッジアクセラレータ、スケーラブルなトレーニング、推論への焦点は、PyTorchがクラウドからデバイス上のアプリケーションまで、多様な環境で強力なモデルの展開を可能にする方法を示しています。

その他、新設されたスタートアップショーケースでは、有望なAIスタートアップが紹介され、CTGT社が優勝しました。CTGT社は、大規模言語モデルにおけるデータ系統を強化し、幻覚を削減することで、深層学習の透明性とバイアスの問題に取り組んでいます。

また、深層学習コンパイラミニサミットとファインチューニングミニサミットも開催されました。

会議の主な収穫は、LLMの重要性、オープンソースによるイノベーションの促進、倫理と持続可能性への配慮、そしてPyTorchがクラウドを超えてエッジデバイスや様々なコンピューティング環境へと拡大していることです。


PyTorch Conference 2025は、2025年10月22～23日にサンフランシスコで開催予定です。


引用元: https://pytorch.org/blog/pytorch-conference-2024-recap/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
