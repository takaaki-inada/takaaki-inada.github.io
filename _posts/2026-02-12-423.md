---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20260212.mp3
audio_file_size: 0
date: 2026-02-12 05:00:00 +0900
description: 'The two patterns by which agents connect sandboxes、AIエージェントのUXを進化させる「A2UI」でアプリを構築、GLM-5: From Vibe Coding to Agentic Engineering'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20260212
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/TB2oI3Uh_ZA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [The two patterns by which agents connect sandboxes](https://blog.langchain.com/the-two-patterns-by-which-agents-connect-sandboxes/)  


AIエージェントがプログラムを実行したりファイルを操作したりする際、セキュリティの観点から「サンドボックス（隔離された実行環境）」の利用が不可欠です。本記事では、エージェントとサンドボックスを接続するための2つの主要なアーキテクチャパターンについて、その特徴と使い分けを解説しています。

### 1. なぜサンドボックスが必要なのか
エージェントにコード実行を許可すると、悪意のあるコードや予期せぬ動作によって、ホストシステムの認証情報や重要ファイルが漏洩・破壊されるリスクがあります。サンドボックス（DockerコンテナやVM）を利用することで、エージェントの活動範囲を完全に分離し、安全な「作業場」を提供できます。

### 2. パターン1：エージェントがサンドボックス内で動く (Agent IN Sandbox)
エージェント本体をサンドボックスの中に配置し、外部からネットワーク経由でメッセージを送る形式です。
- **メリット**: ローカル開発環境と構成が近く、エージェントがファイルシステムやライブラリに直接アクセスできます。エージェントと実行環境を密結合させたい場合に適しています。
- **デメリット**: LLMを呼び出すためのAPIキーをサンドボックス内に置く必要があり、侵害時のリスクがあります。また、コードを修正するたびにコンテナイメージを再ビルドする必要があるため、開発サイクルが遅くなりがちです。

### 3. パターン2：サンドボックスを「ツール」として使う (Sandbox as Tool)
エージェント本体は自身のサーバー等で動かし、コード実行が必要な時だけリモートのサンドボックスAPI（E2BやModalなど）を呼び出す形式です。
- **メリット**: エージェントのロジックを即座に更新でき、開発効率が高いです。APIキーをサンドボックス外（安全なサーバー側）で管理できるためセキュリティ面でも優れています。また、実行時のみリソースを消費するためコスト効率も良く、並列実行も容易です。
- **デメリット**: コード実行のたびにネットワーク通信が発生するため、頻繁に小さな処理を行う場合はレイテンシ（遅延）が課題となります。

### 新人エンジニアへのアドバイス：どちらを選ぶべき？
基本的には**「パターン2（Sandbox as Tool）」**から検討することをおすすめします。エージェントの「思考（ロジック）」と「実行（サンドボックス）」を切り離すことで、デバッグがしやすくなり、セキュリティ設計もシンプルになるからです。一方で、特定のOS設定や複雑な環境状態を常に維持する必要がある高度なエージェントを構築する段階になったら、パターン1を検討してみると良いでしょう。

LangChainの「deepagents」などのフレームワークはこの両パターンに対応しており、設定次第で柔軟に使い分けることが可能です。エージェント開発において、安全性と開発スピードのバランスを考える上で非常に重要な視点となります。

引用元: https://blog.langchain.com/the-two-patterns-by-which-agents-connect-sandboxes/


- [AIエージェントのUXを進化させる「A2UI」でアプリを構築](https://acro-engineer.hatenablog.com/entry/2026/02/10/120000)  


生成AIアプリのインターフェースは現在「チャット形式」が主流ですが、テキストのみのやり取りでは何度も聞き返しが発生するなど、効率や表現力に限界があります。この課題を解決するため、Googleが2025年12月に発表したAIエージェント用UIプロトコル「A2UI（Agent to UI）」について解説します。

### 1. A2UIとは何か
A2UIは、AIエージェントが会話の文脈に応じたUIを動的に生成し、フロントエンド側で安全にレンダリングするためのオープンプロトコルです。
- **宣言的なデータ処理**: UIをJSON形式のデータとして扱います。
- **高いセキュリティ**: HTMLやJavaScriptといった実行コードを直接送らず、構造データのみをやり取りするため、XSSなどのセキュリティリスクを低減できます。
- **効率的なUX**: 例えばレストラン予約やクイズ回答において、チャットで何度も往復する代わりに、入力フォームやカード形式のUIを提示して直感的な操作を促せます。

### 2. A2UIの仕組み
UIの更新は、主に以下の4つのメッセージタイプを組み合わせた「JSONL形式」のシーケンスで行われます。
- **beginRendering**: レンダリング開始の合図。
- **surfaceUpdate**: コンポーネントの追加・更新などのUI構造の定義。
- **dataModelUpdate**: UIが参照するデータ（状態）の更新。
- **deleteSurface**: UIの削除。

基本的には「コンポーネント定義 → データ更新」というフローで動作し、UIとデータのバインディングはA2UIが提供するレンダラーライブラリ（Angular用など）が担います。

### 3. 実装のポイント
エンジニアがA2UIを組み込む際は、LLM（大規模言語モデル）に対して「どのようにJSONを生成させるか」というプロンプト設計が重要になります。
- **プロンプトの構成**: 会話パートとUIパート（JSON）を特定のデリミタで区切ること、JSONスキーマを明示すること、そしてFew-Shot（具体的な出力例）を含めることが成功の鍵です。
- **便利なツール**: CopilotKitが提供する「A2UI Widget Builder」を使えば、自然言語からA2UI用のJSONを自動生成できるため、開発のハードルを下げられます。

### まとめ
A2UIは現在パブリックプレビュー版ですが、フロントエンドで個別の画面を事前に作り込むことなく、エージェントが必要な時に必要なUIを生成できる点は非常に強力です。テキストのみの対話から一歩進んだ、次世代のAIエージェント開発に欠かせない技術となるでしょう。新人エンジニアの方も、まずはJSONを送って画面が変わる面白さを体験してみてください。

引用元: https://acro-engineer.hatenablog.com/entry/2026/02/10/120000


- [GLM-5: From Vibe Coding to Agentic Engineering](https://simonwillison.net/2026/Feb/11/glm-5/)  


AI開発の最前線で大きな注目を集める「GLM-5」のリリースと、それに伴うエンジニアの役割の変化について、Simon Willison氏のブログ記事を基に解説します。新人エンジニアの皆さんにとって、これからのキャリアを考える上で非常に重要なキーワードが含まれています。

### 1. 超大規模なオープンソースモデル「GLM-5」の登場
Z.ai社からリリースされた最新モデル「GLM-5」は、その圧倒的なスケールが最大の特徴です。
- **パラメータ数:** 7540億（754B）という驚異的な規模を誇ります。前モデルのGLM-4.7（368B）から約2倍に拡大しました。
- **データサイズ:** Hugging Face上で公開されているファイルサイズは1.51TBに達します。
- **ライセンス:** MITライセンスで提供されており、この規模のモデルが商用利用も可能な形で公開されたことは、開発者コミュニティにとって非常に大きな意味を持ちます。

### 2. 「Vibe Coding」から「Agentic Engineering」へ
この記事で最も注目すべきは、ソフトウェア開発のパラダイムシフトに関する議論です。

- **Vibe Coding（バイブ・コーディング）:** 
LLM（大規模言語モデル）を使い、なんとなく「いい感じ（Vibe）」にコードを生成させ、動くものを作るスタイルを指します。プロンプトを試行錯誤して、直感的に開発を進める初期のAI活用法です。
- **Agentic Engineering（エージェンティック・エンジニアリング）:** 
これに対し、AIエージェントを自律的なツールとして使いこなし、プロフェッショナルなソフトウェアエンジニアリングを行うスタイルを指します。Andrej Karpathy氏などの著名なエンジニアも提唱し始めており、単に「AIにコードを書いてもらう」段階から、「AIエージェントを設計・管理して、より複雑なシステムを構築する」段階への移行を示しています。

### 3. GLM-5の実力
Simon Willison氏が恒例のテスト（「自転車に乗るペリカンのSVGを生成せよ」という指示）をGLM-5で行ったところ、非常に精度の高い結果が得られました。ペリカンの描写は非常に詳細で、AIが複雑な幾何学的構造や論理的な指示を理解し、実行できる能力（推論能力）が極めて高いレベルにあることを示唆しています。

### 新人エンジニアへのメッセージ
これからのエンジニアには、単にコードを書くスキルだけでなく、GLM-5のような強力なモデルやAIエージェントをどのようにワークフローに組み込み、制御するかという「Agentic Engineering」の視点が求められます。大規模なモデルがオープンに利用できるようになった今、技術の仕組みを理解し、それをどう「エンジニアリング」として昇華させるかが、今後の成長の鍵となるでしょう。

引用元: https://simonwillison.net/2026/Feb/11/glm-5/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
