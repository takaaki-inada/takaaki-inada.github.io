---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251218.mp3
audio_file_size: 0
date: 2025-12-18 05:00:00 +0900
description: 'Gemini 3 Flash: frontier intelligence built for speed、RAGの「リランキング」を10倍速くする「MixLM」、Accelerating Long-Context Inference with Skip Softmax in NVIDIA TensorRT-LLM'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251218
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/ZJWBofrLVPA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Gemini 3 Flash: frontier intelligence built for speed](https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/)  


Google DeepMindが、高速かつ低コストなAIモデル「Gemini 3 Flash」を発表しました。Gemini 3の性能を維持しつつ、推論速度と効率性を向上させたモデルで、API、Geminiアプリ、Google検索のAIモード、Vertex AI、Gemini Enterpriseなどで利用可能です。

主な特徴は以下の通りです。

*   **高速性:** 2.5 Proモデルと比較して3倍高速。
*   **コスト効率:** 推論コストを削減。
*   **高い性能:** PhDレベルの推論能力を持ち、ベンチマークテストでGemini 3 Proと同等の性能を示す部分も。
*   **多様な利用:** コーディング、複雑な分析、インタラクティブなアプリケーションなど、幅広い用途に対応。

開発者向けには、Google AI Studio、Antigravity、Gemini CLI、Android Studioなどを通じて利用でき、企業向けにはVertex AIとGemini Enterpriseで提供されます。一般ユーザーはGeminiアプリとGoogle検索のAIモードで利用可能です。


引用元: https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/


- [RAGの「リランキング」を10倍速くする「MixLM」](https://zenn.dev/knowledgesense/articles/4eb785fd0e9a2b)  


RAGにおける「リランキング」の速度問題を解決する「MixLM」という手法を紹介する記事です。リランキングは精度は高いものの処理速度が遅く、実用上のボトルネックとなることがあります。MixLMは、文書ソースを事前に「リランキング用」にベクトル化し、質問時にそのベクトルと質問文をリランキングモデルに入力することで、従来の10倍以上の高速化を実現します。

具体的には、通常RAGで使用するベクトルとは別に、Encoder LLMを用いて各チャンクのベクトルを作成・保存しておきます。質問時には、ベクトル検索で絞り込んだチャンクと、事前に作成したリランキング用ベクトルを組み合わせ、リランキングモデルに入力します。これにより、LLMが文章全体を処理する代わりに、圧縮されたベクトルを用いることで高速化を実現しています。

LinkedInでの求人検索への実装では、Daily Active Usersが0.47%向上する成果が出ています。RAGシステムのパフォーマンス改善に関心のあるエンジニアにとって、MixLMは有効な選択肢となり得るでしょう。


引用元: https://zenn.dev/knowledgesense/articles/4eb785fd0e9a2b


- [Accelerating Long-Context Inference with Skip Softmax in NVIDIA TensorRT-LLM](https://developer.nvidia.com/blog/accelerating-long-context-inference-with-skip-softmax-in-nvidia-tensorrt-llm/)  


NVIDIA TensorRT-LLMにおけるSkip Softmaxは、LLMの長文コンテキスト処理における計算コスト増大という課題を解決する、新しい疎なアテンション手法です。従来のモデルの再学習は不要で、既存のモデルに組み込むことが可能です。

Skip Softmaxは、Softmax関数の特性を利用し、重要度の低いアテンションブロックを動的に削減します。具体的には、計算されたlogit値が事前に設定された閾値を超えないブロックの処理を省略することで、メモリ帯域幅と計算量の両方を削減します。

性能評価では、Llama 3.3 70Bモデルにおいて、デコード時に最大1.36倍、プレフィル時に最大1.4倍の高速化が確認されています。特に、長いコンテキスト長において効果を発揮します。

精度への影響は、50%程度の疎性化であればほとんど損失がないことが確認されています。TensorRT-LLMに統合されており、Hopper/Blackwell GPUで利用可能です。設定はAPIまたはYAMLファイルを通じて行えます。

より詳細な情報や今後のアップデートについては、関連ドキュメントを参照ください。


引用元: https://developer.nvidia.com/blog/accelerating-long-context-inference-with-skip-softmax-in-nvidia-tensorrt-llm/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
