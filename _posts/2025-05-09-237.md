---
actor_ids:
  - お嬢様ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on/audio/私立ずんだもん女学園放送部_podcast_20250509.mp3
audio_file_size: 0
date: 2025-05-09 05:00:00 +0900
description: 'AI エージェントを仕組みから理解する、ローカルRAGを手軽に構築できるMCPサーバーを作りました、Building Nemotron-CC, A High-Quality Trillion Token Dataset for LLM Pretraining from Common Crawl'
duration: "00:00"
layout: article
title: 私立ずんだもん女学園放送部 podcast 20250509
image_url: https://zund-arm-on.com/images/ojousama_zundamon_square.jpg
thumbnail_url: https://zund-arm-on.com/images/ojousama_zundamon_thumbnail.jpg
---

## 関連リンク


- [AI エージェントを仕組みから理解する](https://zenn.dev/dinii/articles/ai-agent-demystified)  


この記事は、AIエージェントやその仕組みに興味を持つ新人エンジニア向けに、エージェントがどのように動いているかを解説しています。

AIエージェントは、ファイル操作やWeb閲覧など、様々なツールを自律的に使って作業できます。しかし、その基盤となる大規模言語モデル（LLM）自体ができることは、実は「テキストを入力してテキストを出力する」だけです。AIエージェントの賢い動きは、LLMの能力に「仕組み」を組み合わせることで実現されています。

LLMには「記憶がない」という特性があります。過去の会話や情報は、LLMにリクエストを送る際に「コンテキスト」として毎回一緒に送ることで、文脈を維持しています。ただし、コンテキストには長さの制限があるため、必要な情報だけを選んで渡すことが重要です。

また、LLMは単独では外部のファイルやインターネットにアクセスできません。そこで、AIエージェントは「実行したいコマンド」をLLMにテキストで出力させ、その出力をプログラムが受け取って実際のツール（ファイル操作など）を実行する仕組みを使います。これは「Tool use」や「Function calling」と呼ばれ、AIエージェントの基本技術です。

さらに、様々なツールと効率的に連携するために、「MCP」という通信プロトコルが使われることがあります。これは、ツールの定義や実行を共通の形式で行うためのものです。

実際のAIエージェント（Roo Codeなど）は、LLMへの指示（システムプロンプト）と、これまでのやり取り（会話履歴）をコンテキストとして送ることで動作します。会話履歴には、ユーザーの入力だけでなく、エディタから取得した情報（例えば、ツール実行の結果や、コードのエラー情報）も含まれます。特にエラー情報をLLMにフィードバックすることで、エージェントがエラーを認識し、自動で修正できるようになります。

AIエージェントは、このようにLLMの基本能力を様々な仕組みで補い、拡張することで実現されています。過去わずか2年でLLMの性能（コスト、扱える情報量、速度）は劇的に向上しており、これが現在のAIエージェントの実用化を可能にしました。AIエージェントの仕組みを理解することは、現在の技術動向を追い、今後どのように進化していくかを考える上で役立ちます。

引用元: https://zenn.dev/dinii/articles/ai-agent-demystified


- [ローカルRAGを手軽に構築できるMCPサーバーを作りました](https://zenn.dev/mkj/articles/30eeb69bf84b3f)  


この記事は、最近注目されている「Model Context Protocol（MCP）」という技術と、LLM（大規模言語モデル）の応用技術である「Retrieval-Augmented Generation（RAG）」を組み合わせて、自分のローカル環境で使えるRAGシステムを簡単に構築するためのサーバを作った、という開発の紹介です。

RAGとは、あらかじめ用意したドキュメント（文章や資料など）の中から、AIが回答を作るのに役立つ情報を見つけ出し、その情報をAIに渡して回答を生成させる技術のことです。これにより、AIは学習データにはない、例えば社内資料のような特定の情報に基づいて、より正確で専門的な回答ができるようになります。

今回作成された「MCP RAG Server」は、このRAGを実現するための機能の一部である「ドキュメント検索」を担当するサーバです。AI（MCPホスト）からの問い合わせに対して、登録されたドキュメントの中から関連する情報を探し出し、その結果をAIに返します。このサーバはインターネット上のAIサービス（OpenAIなど）を使わないため、情報が外部に漏れる心配がなく、完全に自分のパソコンやネットワークの中で安全に利用できます。

このサーバを作るきっかけは、AIを使ったプログラミングツールなどで、自分のプロジェクトのコードやドキュメントをAIに賢く扱わせたい時に、手軽にドキュメント検索機能を使えるようにしたかったからです。

「MCP RAG Server」の主な特徴は以下の通りです。
- いろいろな種類のドキュメント（文章、プレゼン資料、PDFなど）を扱えます。
- PostgreSQLというデータベースの機能を使って、ドキュメントの意味内容で検索できます。
- 日本語を含む多くの言語に対応しています。
- 新しく追加したり変更したりしたドキュメントだけを効率的に登録できます。
- 使うための設定や登録作業と、AIが使う検索機能が分かれていて使いやすいです。
- 特定の難しいライブラリにあまり頼らず作られているため、比較的安定して使えます。

このサーバを、自分のローカル環境で動くAI（LLM）と組み合わせれば、自分の持っているドキュメントだけを使った、完全にプライベートなRAGシステムを手軽に作ることができます。例えば、会社のナレッジをAIが回答できるようにしたり、個人的なメモをAIに整理してもらったりといった応用が考えられます。

この記事では、実際にこのサーバを使ってドキュメント検索を行い、AIがその結果を基に回答を生成する例も紹介されており、システムがどのように動作するかがイメージしやすくなっています。

まとめると、この「MCP RAG Server」は、MCPという新しい連携の仕組みを活用し、ローカル環境でのRAGシステム構築を簡単にするための便利なツールです。様々なドキュメントを活用したAI連携の可能性を広げることが期待されます。このサーバは、誰でも使えるようにGitHubで公開されています。

引用元: https://zenn.dev/mkj/articles/30eeb69bf84b3f


- [Building Nemotron-CC, A High-Quality Trillion Token Dataset for LLM Pretraining from Common Crawl](https://developer.nvidia.com/blog/building-nemotron-cc-a-high-quality-trillion-token-dataset-for-llm-pretraining-from-common-crawl-using-nvidia-nemo-curator/)  


大規模言語モデル（LLM）の性能向上には、質の高い学習データセットが欠かせません。しかし、Web上の大量のデータ（Common Crawlなど）から高品質なデータを選び出すのは難しく、多くのデータが捨てられてしまうため、データ量と品質の両立が課題でした。特に、LLMに高度な推論能力を習得させるには、より洗練されたデータが必要です。

NVIDIAは、この課題を解決するために、高品質な学習データセット「Nemotron-CC」を構築する新しいパイプラインを開発しました。そして今回、このパイプラインがNVIDIAのデータキュレーションツール「NeMo Curator」に統合され、GitHubで利用可能になったことが発表されました。

このNemotron-CCパイプラインの最大の特長は、単に不要なデータを取り除くだけでなく、捨てられがちな低品質データからも有用な情報を取り出したり、既存の高品質データから新しいテキストを生成したりする「合成データ生成」技術を組み合わせている点です。これにより、データセットの量と質を同時に高めることができます。記事によると、元データから最大2兆トークンもの高品質な合成データを生成できるそうです。

パイプラインは大きく3つのステップで構成されます。まず、Webページからテキストを抽出して整理し、重複した内容を削除します。この処理にはNVIDIAのGPUを活用した高速化技術（RAPIDSライブラリなど）が使われており、非常に効率的です。次に、複数のAIモデルを使ってテキストの品質を評価し、データの質によって分類します。そして最後に、この品質評価結果をもとに、AI（LLM）を使って新しい合成データを生成します。低品質データからは有用な情報を抜き出してリライトし、高品質データからは要約や質問応答ペアなど、多様な形式のデータを生み出します。

このパイプラインで構築されたデータセットを使って最新のLLM（Llama 3.1）を学習させた結果、従来のデータセットで学習した場合と比べて、LLMの推論能力を示すMMLUという評価指標でスコアが大きく向上したことが報告されています。これは、高品質なデータセットがLLMの性能に直接貢献することを示しています。

NeMo Curatorにこのパイプラインが統合されたことで、エンジニアは複雑なデータキュレーション処理を効率的に行えるようになります。これは、これからLLMの開発や活用に取り組む新人エンジニアにとって、高性能なモデルを作るための基盤となるデータセットの重要性を理解し、実際に構築するための強力なツールとなるでしょう。この技術は、LLMの事前学習だけでなく、特定のタスクに特化させるためのファインチューニング用のデータセット作成にも応用できます。

引用元: https://developer.nvidia.com/blog/building-nemotron-cc-a-high-quality-trillion-token-dataset-for-llm-pretraining-from-common-crawl-using-nvidia-nemo-curator/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:ずんだもん
