---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251127.mp3
audio_file_size: 0
date: 2025-11-27 05:00:00 +0900
description: '「Goで作る自作コーディングエージェント nebula 開発入門」が良かった、人工知能は拡散言語モデルの夢を見るか？  PredNext ブログ、Continuous batching from first principles、「父親の遺品を整理してたらフロッピーディスクが見つかったんですけど中身わかる人いますか？」→「あっ…」どうやら18禁らしいが名作の格ゲーと判明'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251127
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/H21QNhAd8ac" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [「Goで作る自作コーディングエージェント nebula 開発入門」が良かった](https://blog.shibayu36.org/entry/2025/11/25/170000)  


この記事では、著者が「Goで作る自作コーディングエージェント nebula 開発入門」という書籍を通して、Go言語でAIコーディングエージェント「nebula」を実際に開発した体験談が語られています。著者はこの実践的な学習が非常に有益で、深い学びがあったと評価しています。

新人エンジニアの皆さんにとって、「AIエージェント」という言葉はまだ聞き慣れないかもしれません。これは簡単に言うと、まるで人間のように目標を設定し、外部の情報を使ったり判断したりしながら、自律的にタスクをこなすAIプログラムのことです。特に「コーディングエージェント」は、指示に基づいてコードを書いたり修正したりするAIですね。この本は、そんなAIエージェントの「仕組み」をGo言語で作りながら学べる、実践的な入門書です。

この実践で得られる主な学びは、AIエージェントの「賢さ」や「振る舞い」を司る重要な設計パターンです。例えば、
1.  **ツールコール**: AIが外部のプログラムやAPI（例えば、ファイルを読み書きする機能やWeb検索機能など）を適切に呼び出し、利用する仕組み。これにより、AIは自身が持たない能力を拡張できます。
2.  **システムプロンプト設計**: AIに対して「あなたはどのような役割で、どのように振る舞うべきか」を指示する、AIの「憲法」のようなもの。これをしっかり設計することで、AIは意図した通りに動きます。
3.  **メモリ機能設計**: AIが過去の会話履歴や学習した情報を覚えておき、それを次の行動に活かす仕組み。人間が経験から学ぶように、AIも「記憶」を持つことでより高度な判断ができるようになります。
これらはコーディングエージェントだけでなく、あらゆるAIエージェント開発に応用できる基礎中の基礎であり、実際に手を動かして理解できたことが大きな収穫だったと著者は述べています。

著者はこの書籍の内容を参考に、自身のGitHubリポジトリ（https://github.com/shibayu36/nebula）で「nebula」を公開しています。単に写経するだけでなく、一部の設計を改善したり、セッション一覧表示やコードの差分表示といった実用的な機能を追加したりと、自分なりに工夫を凝らしている点も注目です。これにより、読者は単なる写経を超えた、より深い理解と応用力を養うヒントを得られるでしょう。

Go言語での開発経験があり、AIエージェントという新しい技術分野に挑戦してみたい新人エンジニアにとって、この書籍は最適な学習リソースとなるはずです。著者の経験から、全ての学習を終えるのにかかる時間は約10時間程度とのこと。短期間で実践的なAI開発の基礎を身につけ、今後のキャリアの選択肢を広げるきっかけにもなりそうです。AIの進化が著しい今、Go言語とAIエージェントの組み合わせは、皆さんのスキルアップに大いに役立つでしょう。

引用元: https://blog.shibayu36.org/entry/2025/11/25/170000


- [人工知能は拡散言語モデルの夢を見るか？  PredNext ブログ](https://prednext.com/blog/diffusion-language-model/)  


2025年現在、「拡散言語モデル」という新しい技術が注目を集めています。これは、現在の大規模言語モデル（LLM）の主流である「自己回帰モデル型Transformer」が抱える性能上の課題を解決する可能性を秘めているためです。

現在のLLMは、文章を構成する単語（トークン）を一つずつ順番に予測して生成する「自己回帰モデル」という仕組みで動いています。この方式では、前の単語の生成が終わらないと次の単語の生成に進めないため、計算を並列に処理するのが難しいという特徴があります。さらに、モデルの規模が大きくなるにつれて、1トークンを生成するたびに非常に巨大なモデルデータ（例えば700億パラメータのモデルで70GB）を、メモリ（DRAM）からプロセッサ（GPUなど）へ何度も読み込む必要が生じます。このデータ転送の頻度と量が多すぎると、プロセッサがフル稼働できず、メモリとの間のデータ帯域が「ボトルネック」（処理速度の限界）となってしまうのです。

この非効率さを「B/F」（Bytes per FLOP：1回の浮動小数点演算に必要なデータ転送バイト数）という指標で表現します。プロセッサとメモリは物理的に離れた場所にあり、データのやり取りには時間がかかります。そのため、コンピュータは昔からB/F値が高い計算が苦手で、自己回帰モデル型Transformerは推論時にこのB/F値が高く、効率的な実行が難しいとされています。

そこで期待されるのが拡散言語モデルです。これは、画像生成で成功を収めている「拡散モデル」の考え方をLLMに応用したものです。拡散モデルは、元の画像にノイズを加えていき、そのノイズから元の画像を復元する過程を利用します。拡散言語モデルでは、文章の一部を`[MASK]`という特殊なトークンで隠し、その`[MASK]`部分に適切な単語を予測して埋めていく作業を繰り返すことで文章を生成します。

この方式の大きなメリットは、一度に複数の`[MASK]`トークンを並列で処理し、予測できる点にあります。これにより、モデルデータをメモリから読み込む回数を大幅に減らすことができ、B/F値を低く抑えられます。結果として、GPUなどのプロセッサをより効率的に活用できるようになる可能性があります。

しかし、拡散言語モデルの実用性についてはまだ議論の途中です。品質面では、同規模の自己回帰モデルと同程度の性能を発揮する研究報告もあります。一方、速度面では、完全な文章になるまで`[MASK]`の予測と除去を繰り返す必要があるため、繰り返し回数が増えると自己回帰モデル（特に「投機的デコーディング」のような高速化技術を導入したもの）よりも遅くなってしまう可能性も指摘されています。

まとめると、拡散言語モデルは自己回帰モデルの根本的な効率問題を解決する可能性を秘めていますが、品質と速度の両面で自己回帰モデルを「常に大きく上回る」とまでは言えないのが現状です。まだ多くの課題が残されており、今後の技術進化が期待される新しいトレンドと言えるでしょう。

引用元: https://prednext.com/blog/diffusion-language-model/


- [Continuous batching from first principles](https://huggingface.co/blog/continuous_batching)  


LLM（大規模言語モデル）を使ったAIチャットボットは、最初の応答まで少し時間がかかり、その後は1単語ずつ（トークンと呼びます）高速に生成されるのを見たことがあるかもしれません。これは、LLMがまずプロンプト全体を処理し（Prefill）、その後、前の生成結果を考慮しながら1トークンずつ予測していく（Decoding）ためです。この生成プロセスは計算コストが非常に高く、特に多くのユーザーが同時に利用するサービスでは、効率的な推論技術が求められます。その中でも特に重要なのが「Continuous Batching」という技術です。

この技術を理解するために、基礎から順に見ていきましょう。

**1. Attentionメカニズム**
LLMが文章中の単語（トークン）間の関係性を理解する中心的な仕組みがAttentionです。Attention層だけが、異なるトークンが相互に影響し合う場所です。LLMはプロンプトの各トークンに対し、クエリ（Q）、キー（K）、バリュー（V）と呼ばれる情報を計算し、これらを組み合わせて次のトークンを予測します。このとき、「因果マスク（Causal Mask）」という仕組みによって、未来のトークンが過去のトークンの計算に影響を与えないように制御されています。最初のプロンプト全体を処理する段階をPrefillと呼びます。

**2. KVキャッシュ**
LLMが一度Prefillで計算したKey（K）とValue（V）の情報を「KVキャッシュ」として保存しておくと、その後のDecoding段階でこれらの情報を再計算する必要がなくなります。これにより、Decodingの計算コストを大幅に削減し、次のトークン生成を高速化できます。まるで過去の計算結果をメモしておいて使い回すようなイメージです。

**3. Chunked Prefill（チャンクプリフィル）**
プロンプトが非常に長い場合、GPUメモリに一度に収まらないことがあります。Chunked Prefillは、KVキャッシュを使いながら長いプロンプトを小さな塊（チャンク）に分割して順番に処理する技術です。これにより、メモリの制約をクリアしつつ、長いプロンプトも効率的に扱えるようになります。

**4. Continuous Batching（コンティニュアスバッチング）**
複数のユーザーからのリクエストを同時に処理して、LLMの「スループット」（単位時間あたりの処理量）を最大化するのがバッチ処理です。しかし、複数のプロンプトを素朴にバッチ処理しようとすると、プロンプトの長さがそれぞれ異なるため、短いプロンプトに合わせて「パディング」（埋め草）が必要になり、無駄な計算が発生してしまいます。

この問題を解決するのがContinuous Batchingです。
*   **ラギッドバッチング（Ragged Batching）**: パディングの代わりに、複数のプロンプトのトークンを物理的に連結して一つにします。そして、Attentionメカニズムで使う「アテンションマスク」を賢く調整することで、各プロンプトのトークンが他のプロンプトのトークンと誤って相互作用しないように制御します。これにより、パディングによる無駄が一切なくなります。
*   **動的スケジューリング**: リクエストが完了したらすぐにバッチから外し、待機している新しいリクエスト（Chunked Prefillで処理中のものも含む）を迅速にバッチに投入します。

これらの技術を組み合わせることで、Continuous BatchingはPrefill中のプロンプトとDecoding中のプロンプトを一つのバッチ内で柔軟に混在させ、GPUを常に最大限に活用できます。この「パディングをなくし、GPUをフル稼働させる」仕組みこそが、ChatGPTのようなサービスが何千ものユーザーに同時に効率的な応答を提供できる秘訣なのです。

引用元: https://huggingface.co/blog/continuous_batching


- [「父親の遺品を整理してたらフロッピーディスクが見つかったんですけど中身わかる人いますか？」→「あっ…」どうやら18禁らしいが名作の格ゲーと判明](https://togetter.com/li/2632241)  


父親の遺品からフロッピーディスクが見つかり、X（旧Twitter）で中身を尋ねたところ、R18格闘ゲーム「ヴァリアブル・ジオ」と判明しました。これは、PCで本格的な格闘ゲームが珍しかった時代に登場したタイトルで、脱衣要素がありつつも高いゲーム性が評価され、全年齢版がスーファミ等に多数移植された名作です。フロッピーディスクという懐かしいメディアを通じて、当時のゲーム文化や技術の変遷を感じられる、新人エンジニアも楽しめる心温まるエピソードです。

引用元: https://togetter.com/li/2632241



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
