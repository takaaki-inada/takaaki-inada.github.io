---
actor_ids:
  - 春日部つむぎ
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/マジカルラブリー☆つむぎのピュアピュアA.I.放送局_podcast_20241223.mp3
audio_file_size: 0
date: 2024-12-23 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
Cappy: Outperforming and boosting large multi-task language models with a small scorer、ChatGPT o1 pro modeに東大理系数学解かせてみた、即席RAGを使用してLLMのコンテキストの限界を超える'
duration: "00:00"
layout: article
title: マジカルラブリー☆つむぎのピュアピュアA.I.放送局 podcast 20241223
information: 今週から月曜日はあーし、春日部つむぎがMCを担当するよ！よろしくね！ずんだもん先輩に負けないように頑張ってわかりやすくトレンドの記事を紹介していくよ！
image_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
thumbnail_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
card: summary
---

## 関連リンク


- [Cappy: Outperforming and boosting large multi-task language models with a small scorer](https://research.google/blog/cappy-outperforming-and-boosting-large-multi-task-language-models-with-a-small-scorer/)  


Cappyは、大規模言語モデル（LLM）の性能と効率を向上させる新しい手法です。この手法では、RoBERTaをベースにした3億6千万パラメータの軽量な事前学習済みスコアラー「Cappy」を使用します。Cappyは、命令と候補の応答を入力として受け取り、応答の正確さを0から1のスコアで評価します。Cappyは、分類タスクでは独立して機能し、生成タスクではLLMの補助コンポーネントとして機能し、LLMの性能を向上させます。

Cappyの主な利点は、ダウンストリームタスクに適応させる際に、LLMのパラメータを更新する必要がないことです。これにより、メモリ消費を抑えつつ、クローズドソースのLLMにも適用できます。また、Cappyは、LLMの入力長制限に影響されず、多くのダウンストリームデータを利用できます。

実験では、Cappyが既存のLLMと同等以上の性能を発揮することを示しました。特に、複雑なタスクにおいて、CappyはLLMの性能を大幅に向上させることが確認されました。Cappyは、LLMの性能を向上させつつ、計算コストとメモリ消費を削減できるため、実用的な応用が期待されます。


引用元: https://research.google/blog/cappy-outperforming-and-boosting-large-multi-task-language-models-with-a-small-scorer/


- [ChatGPT o1 pro modeに東大理系数学解かせてみた](https://zenn.dev/gmomedia/articles/d110a6d23077c9)  


OpenAIが発表したChatGPT Proのo1 pro modeは、数理的推論能力が非常に高いと話題になっています。そこで、東大理系数学の入試問題を解かせてみたところ、見事に正解しました。特に、途中で計算ミスに気をつけたり、慎重に計算したりする点は、これまでの生成AIにはない特徴です。
1問目は、図形問題で、与えられた条件を満たす点Pの範囲を求める問題でした。2問目は、積分を含む関数の最大値と最小値を求める問題でした。どちらも、問題を理解し、正しい手順で解き、正確な答えを導き出しました。
この結果から、AIが東大理系に合格できるレベルに達した可能性が示唆されます。かつて東大合格を目指したAIプロジェクト「東ロボくん」がブレイクスルーがないと凍結されましたが、今、AIは大きな進歩を遂げていると言えます。


引用元: https://zenn.dev/gmomedia/articles/d110a6d23077c9


- [即席RAGを使用してLLMのコンテキストの限界を超える](https://zenn.dev/knowledgesense/articles/7f93fad4a8c0d2)  


LLM（大規模言語モデル）は、長大なコンテキストに適切に対応できないケースが多く、RAG（Retrieval-Augmented Generation）が提案されていますが、準備時間やQAタスク以外での性能に疑問が残ります。この問題に対して、MixPRという手法が提案されています。MixPRは、PageRankと質問をベースとした処理の最適化を組み合わせ、高速に重要なテキストを抜き出すことで、高速化と精度の向上に成功しています。

MixPRは事前の準備を必要とせず、質問とソースとなる文章を入力として、質問に対する回答を高速に出力します。最終的な回答は、質問と関連文章をLLMに渡すことで生成し、MixPRは質問から関連する文章を取得する部分に特徴があります。この手法は、Embedding処理を追加しないことで速度の問題を回避し、その他の工夫によって精度の改善に成功しています。

MixPRは、CPU環境で動作し、一般的なRAGを構築するよりも早く、Embeddingモデルを使用しないので料金も抑えられます。精度面でも、直接的または間接的に関連した情報を含むソースから重要な情報を抜き出す精度は既存のRAGを凌駕しており、要約タスクについてはほとんど差が見られなかったとしています。

MixPRは、精度の面でより優れたモデルがありつつも、既存のモデルを拡張できるという点で今後も利用し続けられる可能性のある手法です。また、構築の速度が速いことから、その場ですぐに大量のファイルから関連する情報を取得する、といった方法でも利用可能です。

引用元: https://zenn.dev/knowledgesense/articles/7f93fad4a8c0d2



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:春日部つむぎ
