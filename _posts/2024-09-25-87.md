---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/株式会社ずんだもん技術室AI放送局_podcast_20240925.mp3
audio_file_size: 0
date: 2024-09-25 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more、RAGを超えた新技術登場！その名も「Self-Route」、話題のGraphRAGにAWSで挑戦しよう！（LlamaIndexとNeptuneに入門）'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20240925
information: 
---

## 関連リンク


- [Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more](https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/)  


Googleは、Gemini 1.5シリーズの最新モデルである「Gemini-1.5-Pro-002」と「Gemini-1.5-Flash-002」をリリースしました。これらのモデルは、Google I/Oで発表されたGemini 1.5モデルをベースに、性能が大幅に向上しています。

**主な改善点**は以下の通りです。

* **性能向上**: 数学、長文処理、画像認識などのタスクで、特にGemini-1.5-Proは性能が約7%向上しました。
* **価格改定**: Gemini-1.5-Proの入力と出力のトークン価格が50%以上削減されました。
* **速度向上**: 出力速度が2倍、レイテンシが3倍削減されました。
* **利用制限緩和**: Gemini-1.5-FlashとGemini-1.5-Proの利用制限がそれぞれ2倍と3倍に緩和されました。
* **安全対策**: デフォルトの安全フィルター設定が変更され、開発者が用途に合わせて設定できるようになりました。


これらのモデルは、Google AI Studio、Gemini API、Vertex AIを通じて利用可能です。特に、Gemini-1.5-Proは最大200万トークンの長いコンテキストを処理できるため、複雑な文書や動画の処理に適しています。


今回のアップデートにより、Gemini 1.5シリーズはより高速で、コスト効率が高く、使いやすくなりました。これらのモデルを活用することで、より高度なAIアプリケーションを開発できるようになるでしょう。


**制約事項**としては、Gemini-1.5-Proの価格改定は128Kトークン未満のプロンプトに適用される点、安全フィルターはデフォルトで適用されない点などに注意が必要です。詳細な情報は、Gemini APIのドキュメントを参照ください。 


引用元: https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/


- [RAGを超えた新技術登場！その名も「Self-Route」](https://qiita.com/ryosuke_ohori/items/a94c648df3243b5af323)  


近年、大規模言語モデル（LLM）は複雑な質問に答えられる一方、計算コストが課題となっています。一方、検索拡張生成（RAG）は低コストで迅速な回答を提供できますが、複雑な処理には不向きです。

「Self-Route」は、LLMとRAGの長所を組み合わせた新しい技術です。クエリの内容に応じて、LLMとRAGを使い分けることで、コストと精度のバランスを最適化します。

**Self-Routeの仕組み**
1. **RAG-and-Routeステップ**: まず、RAGでクエリに関連する情報を検索します。
2. **長文コンテキスト予測ステップ**: RAGだけでは処理できない複雑なクエリの場合、LLMが全文脈を解析し、回答を生成します。
3. **動的ルーティング**: クエリに応じてLLMとRAGを自動的に使い分けることで、無駄なくリソースを活用します。

**Self-Routeのメリット**
- コスト効率の向上：RAGで処理できるクエリはRAGで処理し、LLMの使用を最小限に抑えることで、コストを削減できます。
- 高精度な回答：複雑なクエリにはLLMが対応するため、精度の高い回答を得られます。
- 柔軟な適応性：クエリに応じて適切な技術を選択することで、幅広い種類のクエリに対応できます。


**Self-Routeの制約**
- 多段階推論が必要な質問、曖昧な質問、長くて複雑な質問、暗黙的な理解を要する質問など、RAGだけでは処理できないクエリには限界があります。


**今後の課題**
- RAGの多段階推論能力の向上
- 実際のデータセットを用いた評価


Self-Routeは、LLMとRAGの利点を融合した画期的な技術であり、今後、様々な分野で活用されることが期待されています。新人エンジニアの皆さんも、この新しい技術に注目してみてはいかがでしょうか。 


引用元: https://qiita.com/ryosuke_ohori/items/a94c648df3243b5af323


- [話題のGraphRAGにAWSで挑戦しよう！（LlamaIndexとNeptuneに入門）](https://qiita.com/minorun365/items/a62dec701bef17cf7562)  


近年、LLM（大規模言語モデル）において、RAG（Retrieval Augmented Generation）という検索結果を組み合わせる技術が注目されています。従来のRAGは、ドキュメントをベクトルに変換して検索していましたが、Microsoftが公開したGraphRAGは、ドキュメントを「グラフ」として保存することで検索精度向上を目指しています。

グラフとは、ノードとエッジで関係性を表すもので、GraphRAGでは知識グラフを扱うことが多いです。知識グラフを保存するにはグラフDBが必要で、AWSではAmazon Neptuneが利用できます。Neptuneは、データベース機能に加え、大規模グラフを高速に分析できるNeptune Analyticsも提供しています。

一方、RAGの実装にはLangChainなどのフレームワークが用いられますが、LlamaIndexは特にRAGに特化したフレームワークです。今回は、LlamaIndexを使ってAmazon Neptune上でGraphRAGを試す方法を紹介します。

**GraphRAGを試すための制約**

AWSのサービスを利用するため、AWSアカウントが必要です。また、Amazon Bedrockのモデルを有効化し、Neptune Databaseを構築する必要があります。構築には無料利用枠のインスタンスを利用可能ですが、検証後は不要なリソースを削除する必要があります。

**GraphRAGの試しかた**

1. AWSアカウントとAmazon Bedrockのモデルを有効化します。
2. Amazon Neptune Databaseを作成します。
3. JupyterLab上で、LlamaIndexのサンプルコードを実行します。
4. 対象のドキュメントをPDF形式でダウンロードし、Neptune Databaseに登録します。
5. Neptune Graph Explorerで、グラフを可視化して確認します。
6. サンプルコードでクエリを実行し、GraphRAGの動作を確認します。


今回の記事では、AWSのサービスを使ってGraphRAGを体験できる手順を紹介しました。GraphRAGは、従来のRAGでは難しかったユースケースで効果を発揮する可能性を秘めています。生成AIアプリケーションの開発に携わるエンジニアは、ぜひGraphRAGやグラフDBについて学習し、今後の開発に役立ててください。 


引用元: https://qiita.com/minorun365/items/a62dec701bef17cf7562



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
