---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20240904.mp3
audio_file_size: 0
date: 2024-09-04 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
Build reliable agents in JavaScript with LangGraph.js v0.2: Now supporting Cloud and Studio、オープンソースのRAG UI「kotaemon」を試す、「ELYZA-japanese-Llama-2-70b」開発における、大規模モデル学習ノウハウの解説、非エンジニアの営業担当が生成AIと協力してWEBアプリを開発した話'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20240904
information: 
---

## 関連リンク


- [Build reliable agents in JavaScript with LangGraph.js v0.2: Now supporting Cloud and Studio](https://blog.langchain.dev/javascript-langgraph-v02-cloud-studio/)  


LangChain.js v0.2.0は、JavaScript/TypeScriptでLLM(大規模言語モデル)を活用したエージェントを構築するためのフレームワークです。今回のバージョンアップでは、以下の機能強化がされました。

* **ストリーミング機能の強化**: 中間ステップやチャットモデルのメッセージを柔軟にストリーミングできるようになりました。
* **チェックポイントシステム**: 過去の状態に戻ったり、モデルの応答エラーなどをデバッグできるようになりました。
* **ヒューマン・イン・ザ・ループのサポート**: グラフの任意の時点で実行を中断、内部状態の更新、再開が可能になりました。
* **並列ノードサポート**: 複数のノードを同時に実行し、結果を組み合わせることが可能になりました。

さらに、デスクトップ向けのAgent IDEであるLangGraph Studioと、エージェントをデプロイするためのスケーラブルなインフラストラクチャであるLangGraph Cloudのベータ版が利用可能になりました。

LangGraph.jsは、LLM特有の長時間実行や非決定的な性質による課題を解決し、エージェント開発を容易にします。具体的には、以下のようなメリットがあります。

* **応答性の向上**: 結果をトークン単位でストリーミングすることで、リアルタイムなインタラクティブな体験を提供できます。
* **回復力の向上**: ノードレベルの再試行ポリシーとチェックポイントにより、サービス停止やステップ失敗時でも、以前の状態から実行を再開できます。
* **アクセス制御**: 特定のツールへのアクセスを人間の承認に制限することで、アプリケーションのセキュリティを強化できます。

LangGraph Studioは、TypeScript/JavaScript環境におけるエージェントのデバッグを支援するツールです。グラフの入力/出力の可視化や、状態の巻き戻し、ステップ実行などが可能です。

LangGraph Cloudは、LangGraph.jsで構築したエージェントをWeb規模でデプロイするためのサービスです。タスクキューやサーバーの管理、LangSmithとの統合による詳細なトレース、状態の巻き戻しによるトラブルシューティングなどが可能です。

LangGraph.jsは、Node.js、Deno、Cloudflare Workersなど、多くのJavaScriptランタイムで動作します。今後もコミュニティからのフィードバックを参考に、機能強化を続けていく予定です。 


引用元: https://blog.langchain.dev/javascript-langgraph-v02-cloud-studio/


- [オープンソースのRAG UI「kotaemon」を試す](https://zenn.dev/kun432/scraps/c0642e79169635)  



kotaemonは、LLMとベクトルデータベースを組み合わせ、ドキュメントから質問に答えるRAG（Retrieval Augmented Generation）のUIを提供するオープンソースツールです。DockerイメージまたはPython仮想環境から起動できます。

**概要**

kotaemonは、LLM（大規模言語モデル）とベクトルデータベースを連携させ、ドキュメントから質問への回答を生成するRAGシステムのUIを提供するツールです。OpenAIやOllamaなどのLLM、および様々なベクトルデータベースと連携可能です。ローカル環境でLLMを活用したRAGシステムを構築し、手軽に試したいエンジニアに適しています。

**制約**

- GraphRAG機能は、現時点ではOllamaでは動作せず、OpenAIを用いる必要があります。
- GraphRAG機能は、安定性に課題があり、クエリによってはエラーが発生する可能性があります。


kotaemonを利用するには、まずDockerイメージから起動するか、Python仮想環境でレポジトリをクローンして起動します。その後、LLMとEmbeddingモデル、インデックス作成時のEmbeddingモデル、検索・推論時のLLMを、使用するOllamaモデルに設定を変更します。  さらに、RAGで使用するドキュメントをアップロードし、インデックスを作成することで、チャット画面から質問し、ドキュメントからの回答を得ることができます。

GraphRAG機能を利用するには、Python仮想環境で必要なパッケージをインストールし、環境変数を設定してからkotaemonを起動します。その後、ドキュメントをアップロードし、GraphRAGでインデックスを作成すると、質問に対する回答と同時に、グラフ、エンティティの説明、テキストチャンク、レポート、リレーションなどが表示されます。これにより、回答の根拠をより詳細に理解することができます。


本記事では、Ollamaと連携したkotaemonのインストール手順と、GraphRAG機能を含む設定方法について解説しています。新人エンジニアの方でも、手順に沿って進めることで、簡単にRAGシステムを体験できます。ただし、GraphRAG機能は開発中の機能であり、安定性に課題がある点にご注意ください。 


引用元: https://zenn.dev/kun432/scraps/c0642e79169635


- [「ELYZA-japanese-Llama-2-70b」開発における、大規模モデル学習ノウハウの解説](https://zenn.dev/elyza/articles/a8ea394675b06d)  


ELYZA社は、日本語に特化した大規模言語モデル（LLM）の開発に力を入れており、Meta社の「Llama-2」シリーズをベースに、日本語データで追加学習を行うことで、高性能な日本語LLMを構築してきました。本記事では、700億パラメータを持つ「ELYZA-japanese-Llama-2-70b」の開発における知見を共有し、国内における大規模言語モデル開発コミュニティへの貢献を目指しています。

**開発の背景と目的**

ELYZA社は、オープンソースモデルをベースに日本語LLMを開発することで、日本におけるAI需要への迅速な対応を目指しています。これまで、「Llama-2-7b」や「Llama-2-13b」をベースとした日本語LLMを開発し、日本語LLM評価ベンチマークで高い性能を達成してきました。しかし、グローバルモデルとの性能差を埋めるには、さらに大規模なモデルが必要だと考え、「Llama-2-70b-chat」をベースとした「ELYZA-japanese-Llama-2-70b」の開発に着手しました。

**開発における課題と解決策**

700億パラメータのモデルを扱うには、学習基盤の整備、日本語データの増加、質の高いフィードバック学習など、多くの課題がありました。ELYZA社は、産総研の大規模言語モデル構築支援プログラムに採択され、AI用スパコン「ABCI」を活用することで、これらの課題を克服しました。

**事前学習の詳細**

事前学習では、1,000億トークンの日本語データを用いて「Llama-2-70b-chat」を追加学習しました。学習効率を最大化するため、PyTorchのFSDPを用いた分散学習を採用しました。ABCIのハードウェア構成、特にInfiniband HDRによる高性能なノード間通信が、高い学習効率に貢献しています。また、チェックポイントの保存方法や監視システムの構築など、学習の安定性と迅速な復旧にも工夫を凝らしました。

**学習期間中の障害と対応**

大規模な学習では、ノードの不調やネットワークエラーなど、様々な障害が発生する可能性があります。本開発でも、ノード不調による学習停止やNCCLエラーなどの問題が発生しましたが、予備ノードの確保や障害対応フローの整備によって、迅速な復旧を実現しました。

**成果と今後の展望**

「ELYZA-japanese-Llama-2-70b」の開発を通して得られた知見は、後続の「Llama-3-ELYZA-JP」の開発にも活かされています。「Llama-3-ELYZA-JP」は、日本語の生成能力において「GPT-4」を上回る性能を達成しました。ELYZA社は今後も、日本語LLMの研究開発と実用化に注力し、AI技術の発展に貢献していきます。


本要約が、新人エンジニアの方々にとって、大規模言語モデル開発の取り組みを理解する上で役立てば幸いです。 


引用元: https://zenn.dev/elyza/articles/a8ea394675b06d


- [非エンジニアの営業担当が生成AIと協力してWEBアプリを開発した話](https://techblog.insightedge.jp/entry/creating-web-app-with-ai)  


Insight Edgeの営業担当である塩見さんが、エンジニア経験なしで生成AIを活用し、約2ヶ月でWEBアプリを開発した事例を紹介する記事です。

**開発の目的**は、フロントエンド・バックエンドのシステム開発に関する理解を深めることでした。生成AIの力を借りることで、塩見さんは短期間でチャット機能や掲示板機能を持つWEBアプリをゼロから開発することに成功しました。

**開発で利用した主なツール・技術**は、生成AI（ChatGPT、GitHub Copilot）、React、Next.jsなどです。特にChatGPTは、コード生成や質問への回答を通して、塩見さんの開発を強力にサポートしました。

**開発プロセス**では、まず手描きイラストからtldraw makerealというツールでWEBアプリのデザインとHTMLを自動生成しました。その後、ChatGPTと対話しながら、機能の実装を進めていきました。

**塩見さんが得た学び**として、生成AIを活用した開発は学習効率が高いこと、ノーコードツールよりも柔軟で高速に開発できること、そしてChatGPTと正確に対話する技術が重要であることが挙げられています。

**今後の展望**としては、開発中のアプリに新たな機能を追加していくとともに、生成AIを活用した開発の進化に期待しているとのことです。


この記事は、生成AIを活用したアプリ開発に興味を持つ、特にエンジニア経験の浅い方にとって、非常に参考になる内容です。生成AIの活用方法や開発における課題、そして学習効率の向上など、具体的な事例を通して理解を深めることができます。新人エンジニアの方にとっても、生成AIが開発プロセスにどのように役立つのか、そしてどのような点に注意すべきなのかを学ぶ上で有益な情報が得られるでしょう。 


引用元: https://techblog.insightedge.jp/entry/creating-web-app-with-ai



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
