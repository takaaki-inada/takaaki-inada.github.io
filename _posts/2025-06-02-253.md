---
actor_ids:
  - 春日部つむぎ
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/マジカルラブリー☆つむぎのピュアピュアA.I.放送局_podcast_20250602.mp3
audio_file_size: 0
date: 2025-06-02 05:00:00 +0900
description: 'Cerebras beats NVIDIA Blackwell: Llama 4 Maverick Inference、When Fine-Tuning Actually Makes Sense: A Developers Guide、Why DeepSeek is cheap at scale but expensive to run locally、VTuber×声優が初の生共演！原宿で朗読劇「ずんだもんとつむぎのバーチャル大冒険」6月開催'
duration: "00:00"
layout: article
title: マジカルラブリー☆つむぎのピュアピュアA.I.放送局 podcast 20250602
image_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
thumbnail_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
card: summary
---

## 関連リンク


- [Cerebras beats NVIDIA Blackwell: Llama 4 Maverick Inference](https://www.cerebras.ai/press-release/maverick)  


このニュースは、AIモデルを使ったサービスの「速さ」、特に「推論速度」に関する重要な進歩を伝えています。推論速度とは、AIに何か指示を与えたときに、どれだけ早く回答や結果を出してくれるかのスピードのことです。この速さは「トークン/秒（TPS）」という単位で測られます。トークンは文章の単語や記号のまとまりのようなもので、TPSは1秒間に生成できるトークンの数を表します。

今回、アメリカのCerebrasという会社が、Metaが開発した大規模AIモデル「Llama 4 Maverick」（400Bというパラメータ数のモデル）の推論速度で、世界記録を達成しました。独立機関であるArtificial Analysisが行ったベンチマークテストによると、Cerebrasは2,522 TPS以上を記録しました。これは、同じテストでNVIDIAの最新GPUである「Blackwell」が記録した1,038 TPSを大きく上回る速度です。他の主要ベンダーと比較しても、Cerebrasが最も速い結果となりました。

なぜ推論速度が速いと良いのでしょうか？最近の高度なAIアプリケーション、例えばAIがプログラムコードを自動で書いたり、複雑な問題を段階的に推論したりするような場合、AIは多くの情報を処理し、長い文章を生成することがあります。このとき推論速度が遅いと、ユーザーは応答を長時間待たされることになり、AIを実用的に使う上で大きな課題となります。速度が速ければ速いほど、AIはスムーズに動き、より快適に利用できるようになります。

Cerebrasの成果の注目すべき点は、この記録的な速さを特別なソフトウェアの最適化に頼らずに達成したこと、そしてそのハードウェアとAPIが既に利用可能であることです。一方、NVIDIAのベンチマーク結果はカスタム最適化によるものであり、一般的に利用できるサービスではまだその速度が出ていない状況も指摘されています。

今回のCerebrasの発表は、大規模AIモデルの性能を向上させる上で重要な「速さ」の面で新たな基準を示したものであり、これからのAI活用の可能性を広げる一歩と言えるでしょう。

引用元: https://www.cerebras.ai/press-release/maverick


- [When Fine-Tuning Actually Makes Sense: A Developers Guide](https://getkiln.ai/blog/why_fine_tune_LLM_models_and_how_to_get_started)  


この記事は、LLM（大規模言語モデル）を開発する上で、ファインチューニングがどのような場合に役立つのか、その具体的なメリットと始め方について、開発者向けに解説しています。

AIモデルの能力を引き出すためにプロンプトを工夫することは重要ですが、プロンプトだけでは限界がある場合があります。例えば、出力形式（JSONなど）が安定しない、プロンプトが長くなりすぎてコストや速度が悪化する、モデルに特定の細かいルールや振る舞いを教えたい、といった問題です。このような、プロンプトでは解決しきれない具体的な課題に対して、ファインチューニングは非常に有効な手段となります。

ファインチューニングを行うことには、主に以下のようなメリットがあります。

1.  **タスク品質の向上:** 特定のタスクに対するモデルの精度や、出力のスタイル、JSONやXMLのような特定のフォーマットを正確に生成する能力を大幅に改善できます。
2.  **コスト削減と高速化:** ファインチューニングにより、同じタスクをより短いプロンプトや、より小さなモデルで実行できるようになります。これにより、推論にかかるコストを抑え、処理速度を向上させることが可能です。小さなモデルは、個人のPCやデバイス上で動かすことも容易になり、データのプライバシー保護にもつながります。
3.  **ルールやロジックの遵守:** モデルに特定の条件に基づいた応答や複雑なルールに従わせる能力を高められます。期待しない振る舞い（バグ）を修正するためにも利用できます。
4.  **ツール利用の精度向上:** モデルが外部ツールを適切に判断し、正確なフォーマットで利用する能力を高めることができます。

ただし、**知識の追加**を目的とする場合は、ファインチューニングは最適な方法ではありません。最新情報や特定の知識をモデルに参照させたい場合は、RAG（外部情報参照）や、プロンプトに直接コンテキストとして情報を含める方法の方が適しています。

ファインチューニングを始める際は、まず「何を解決したいのか」という具体的な目標を明確にすることが大切です。その目標に合わせて、どのベースモデルを使うかを選びます。少量でも良いので学習データを用意し、実際に学習させて結果を評価し、改善を繰り返していくのが基本的な流れです。

このように、ファインチューニングは、プロンプトだけでは実現が難しいAIモデルのカスタム化や性能改善のための強力な手法です。特定の問題に直面しているエンジニアにとって、試す価値のある技術と言えるでしょう。

引用元: https://getkiln.ai/blog/why_fine_tune_LLM_models_and_how_to_get_started


- [Why DeepSeek is cheap at scale but expensive to run locally](https://www.seangoedecke.com/inference-batching-and-deepseek/)  


DeepSeekのような大規模言語モデル（LLM）が、なぜサービスとして大規模に提供されると安価で高速なのに、自分のパソコンなどローカル環境で動かすと遅く高価になるのか、という疑問について技術的な観点から解説しています。

その主な理由は、AI推論を行う際の「スループット」（単位時間あたりに処理できる量）と「レイテンシ」（応答速度、待ち時間）の間に存在するトレードオフにあります。このトレードオフは、推論処理の際に「バッチサイズ」（複数のユーザーからのリクエストをまとめて一度に処理する数）をどう設定するかによって生じます。

GPUは、多数の計算をまとめて大きなひとつの行列計算（GEMM）として行うのが非常に得意です。これに対し、小さな行列計算を何度も繰り返すのは苦手です。バッチ推論では、複数のユーザーからのリクエストをまとめて大きなバッチとしてGPUに投入します。これにより、個々のリクエストをバラバラに処理するよりも、GPUを効率的に使い、全体としての処理能力（スループット）を大幅に向上させることができます。

しかし、このバッチ処理には待ち時間が伴います。例えば、「バッチサイズ128」で処理すると決めた場合、リクエストは128件分集まるまで待機キューで待つことになります。バッチサイズが大きいほど、GPUの利用効率は上がりスループットは向上しますが、個々のユーザーの応答が返ってくるまでの時間（レイテンシ）は長くなります。逆にバッチサイズが小さいとレイテンシは短くなりますが、GPUが非効率になりスループットは低下します。

特にDeepSeek-V3のような「Mixture-of-Experts (MoE)」と呼ばれるアーキテクチャを持つモデルや、層が多く大規模なモデルは、その構造上、GPUの効率的な利用のために大きなバッチサイズがほぼ必須となります。
MoEモデルは多数の「専門家」を持っており、大きなバッチで処理しないと、各専門家が担当する計算量が少なくなり、GPUが非効率になります。
また、多層モデルはGPUを複数使って処理をパイプライン化しますが、バッチサイズが小さいとGPU間の待ち時間（パイプラインバブル）が発生しやすくなり、これも非効率の原因となります。

そのため、DeepSeekのようなモデルは、多くのユーザーからの同時リクエストがある大規模なサービス環境では、大きなバッチサイズで効率的に実行できます。しかし、ローカル環境で一人のユーザーが使う場合、バッチサイズが非常に小さくなるため、GPUが非効率になり、結果として処理が遅く感じられるのです。

OpenAIやAnthropicのモデルが比較的応答が速いのは、アーキテクチャが異なるか、推論効率を高める特別な技術を使っているか、あるいはGPUを大量に投入して低レイテンシを実現している可能性があると推測されています。

このバッチ処理によるスループットとレイテンシのトレードオフは、LLMの運用コストや性能を理解する上で重要な考え方です。

引用元: https://www.seangoedecke.com/inference-batching-and-deepseek/


- [VTuber×声優が初の生共演！原宿で朗読劇「ずんだもんとつむぎのバーチャル大冒険」6月開催](https://hotakasugi-jp.com/2025/06/01/stage-news-zundamon-kasukabetsukushi/)  


VTuberと声優が舞台で初めて生共演する朗読劇が6月に原宿で開催されます。「ずんだもん」の声優さんや、VTuberの春日部つくしさんらが出演。リアルタイムでキャラクターを動かす技術を使った「ハイブリッド演劇」として注目されています。新しいエンタメ技術に興味のある新人エンジニアさんは要チェックです。

引用元: https://hotakasugi-jp.com/2025/06/01/stage-news-zundamon-kasukabetsukushi/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:春日部つむぎ
