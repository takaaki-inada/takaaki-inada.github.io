---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250708.mp3
audio_file_size: 0
date: 2025-07-08 05:00:00 +0900
description: 'Mercury: Ultra-Fast Language Models Based on Diffusion、LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM、LLMを本番品質に育てる PromptOps：”100回の試行錯誤”を支えた仕組みと文化、自分の10年越えTwitterログが超記憶として対話可能に！Twilog専用MCPサーバーが使えるようになりました。'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250708
---

## 関連リンク


- [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)  


「Mercury」は、拡散モデルという新しい技術を取り入れた、次世代の大規模言語モデル（LLM）です。これまでのLLMは一つずつ単語やコードを生成していましたが、Mercuryは複数の単語やコードの断片を同時に予測して生成できる点が画期的です。これにより、非常に高速な動作が可能になりました。これは、LLMの基盤技術であるTransformerアーキテクチャを使いながら、同時に複数の要素を予測できるように学習させる新しいアプローチで実現されています。

特に注目されているのは、プログラミングコードの生成に特化した「Mercury Coder」というモデルです。このモデルには「Mini」と「Small」の2つのサイズがあります。独立した評価機関によるテストでは、Mercury Coder Miniが1秒あたり1109トークン、Mercury Coder Smallが1秒あたり737トークンという驚異的な処理速度を記録しました。これは、現在速度に特化した最先端のLLMと比較しても、平均で最大10倍も高速でありながら、生成されるコードの品質は同等レベルを保っていることを示しています。

さらに、実際の開発者が利用する評価プラットフォーム「Copilot Arena」では、Mercury Coderは品質面で全モデル中2位にランクインし、速度においては全モデルの中で最速を記録しました。これは、論文上の数値だけでなく、実際の開発現場でもその優れた性能が認められていることを意味します。

開発元のInception Labsは、この「Mercury Coder」を外部サービスから利用できるパブリックAPIと、無料で試せるプレイグラウンドも公開しています。新人エンジニアの皆さんにとって、日々のコーディング作業を劇的に効率化し、AIを活用した開発のスピードを飛躍的に向上させる可能性を秘めた、まさに画期的な技術の登場と言えるでしょう。この超高速LLMの登場は、AI開発の未来を大きく変える「ブレイクスルー」となるでしょう。

引用元: https://arxiv.org/abs/2506.17298


- [LLM Inference Benchmarking: Performance Tuning with TensorRT-LLM](https://developer.nvidia.com/blog/llm-inference-benchmarking-performance-tuning-with-tensorrt-llm/)  


大規模言語モデル（LLM）を実際に使う際、その「推論性能」はとても重要です。どれだけ速く、たくさんのユーザーのリクエストを処理できるかが、ユーザー体験やサービスの効率を大きく左右します。この記事では、NVIDIAが提供するオープンソースのAI推論エンジン「TensorRT-LLM」を使って、LLMの性能を最大限に引き出すためのベンチマークとチューニング方法を、新人エンジニアにも分かりやすく解説しています。

まず、LLMの性能を測るためのツール「`trtllm-bench`」の使い方が紹介されています。このツールを使うことで、実際にLLMを動かすことなく、モデルの性能を簡単に測定・分析できます。ベンチマークを行うには、GPU環境の準備と、テスト用のデータセットを用意します。データセットは、質問とそれに対する期待される回答の長さなどを指定して作成します。

ベンチマークを実行すると、様々な性能指標が得られます。特に注目すべきは、「Request Throughput（1秒あたりのリクエスト処理数）」、「Total Output Throughput（1秒あたりの出力トークン数）」、そしてユーザー体験に直結する「Average time-to-first-token [TTFT]（最初のトークンが出るまでの時間）」や「Average time-per-output-token [TPOT]（トークンごとの生成時間）」です。これらの指標を分析し、アプリケーションの目的に合わせて最適なバランスを見つけることが、性能チューニングの鍵となります。例えば、ユーザーへの応答速度を重視するなら「Per User Output Speed」という指標を最大化するように調整します。

記事では、データの精度を少し落とす代わりに処理を高速化する「FP8量子化」されたモデルと、標準の「FP16」モデルを比較し、FP8モデルがより多くの同時ユーザーを処理できる例を示しています。このように、`trtllm-bench`を使えば、さまざまな設定を試して、どの設定が一番効率的かをグラフで視覚的に確認できます。

最適な設定が見つかったら、それを「`trtllm-serve`」というツールを使って、LLMを動かすサーバーに適用します。`trtllm-serve`はOpenAI互換のAPIを提供するため、チューニングされたLLMをアプリケーションから簡単に呼び出して利用できるようになります。

TensorRT-LLMは、LLMの性能ベンチマークから最適な設定でのデプロイまで、一貫してサポートする強力なツールです。これにより、開発者はLLMの性能を最適化し、ユーザーに最高の体験を提供することに集中できます。

引用元: https://developer.nvidia.com/blog/llm-inference-benchmarking-performance-tuning-with-tensorrt-llm/


- [LLMを本番品質に育てる PromptOps：”100回の試行錯誤”を支えた仕組みと文化](https://zenn.dev/elyza/articles/3b25b8e44fc280)  


この記事では、株式会社ELYZAと株式会社マイナビが共同開発した「マイナビAI Pencil」の開発を通して見えてきた、LLM（大規模言語モデル）を実際のビジネスで役立つレベルに育てるための「Prompt Engineering」と、それを支える仕組み「PromptOps」について解説されています。

LLMはとても賢いですが、その出力をそのまま本番環境で使うには、期待通りの品質を安定して出すための細かい調整が欠かせません。この「なんだか微妙」という漠然とした課題を、具体的な指示に変えてプロンプト（LLMへの指示文）を磨き上げていく作業が「Prompt Engineering」です。例えば、ユーザーの自己PR文章を生成するタスクでは、より魅力的な文章にするために100パターンものプロンプトを試すような、地道な試行錯誤が必要になります。

しかし、このようなプロンプト調整のスキルが特定の人だけの「職人技」になってしまうと、改善が滞ったり、ノウハウが共有されなかったりといった問題が起こります。そこで重要になるのが、「PromptOps」という仕組みです。これは、Prompt Engineeringを組織全体の活動として体系化し、継続的に改善していくための運用基盤を指します。

ELYZAでは、PromptOpsを実現するために以下の取り組みを行っています。
1.  **エンジニアに閉じない「開かれたプロンプト改善」**: プロンプトの質は技術的な正しさだけでなく、お客様の業務や業界知識も重要です。そのため、MLエンジニアだけでなく、プロジェクトマネージャーやビジネスサイドのメンバーもNotionなどを活用してプロンプトのレビューや改善に加わります。
2.  **バージョン管理**: ソフトウェア開発で使われる「SemVer（セマンティックバージョニング）」というルールを参考に、プロンプトにもバージョン番号を付け、「いつ、誰が、なぜ」変更したかを記録します。これにより、変更の意図が明確になり、過去の優れたプロンプトもチームの「資産」として活用できます。
3.  **客観的データに基づく実験管理と高速な改善サイクル**: 勘ではなくデータに基づいてプロンプトの性能を評価します。少量のデータで素早く試す「クイック改善」、より大規模なデータで客観的に評価する「定性・定量評価」、そしてお客様に実際の出力を確認してもらう「顧客レビュー」というサイクルを高速で回し、最適なプロンプトを見つけ出します。

これらの仕組みにより、プロンプト管理の混乱が解消され、職種を超えたチーム開発が実現しました。今後は、手作業の自動化や最先端の外部ツールの活用を通じて、さらなる開発効率の向上を目指していくとのことです。

新人エンジニアの皆さんも、LLMを使った開発をする際には、プロンプトをソフトウェアのようにバージョン管理し、変更履歴をしっかり残すこと、そして長文のプロンプト管理には「TOML」という形式が読み書きしやすくて便利、という点をぜひ参考にしてみてください。

引用元: https://zenn.dev/elyza/articles/3b25b8e44fc280


- [自分の10年越えTwitterログが超記憶として対話可能に！Twilog専用MCPサーバーが使えるようになりました。](https://note.com/togetter/n/n084819ff6502)  


Twilogが初のAI機能「Twilog専用MCPサーバー」をリリースしました。MCPはAIが外部サービスデータ（今回の場合はTwilogのXログ）を自律的に参照・活用できる仕組みです。これにより、あなたの過去のXの投稿やいいね、ブックマークといった膨大なログをAIが読み込み、自然言語で質問できるようになります。「去年のラーメンの投稿をまとめて」のように聞けば、AIが自動でログを検索・整理してくれます。Twilogがあなたの「超記憶装置」となり、ログから必要な情報を簡単に引き出す新しい体験が提供されます。

引用元: https://note.com/togetter/n/n084819ff6502



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
