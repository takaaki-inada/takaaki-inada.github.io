---
actor_ids:
  - 春日部つむぎ
audio_file_path: /audio/マジカルラブリー☆つむぎのピュアピュアA.I.放送局_podcast_20251208.mp3
audio_file_size: 0
date: 2025-12-08 05:00:00 +0900
description: 'うちのAIがやらかしまして ─ Claude Codeの暴走を防ぐCLAUDE.md運用術、Titans + MIRAS: Helping AI have long-term memory、TypeScript 向けの AI フレームワーク TanStack AI を試してみた、ChatGPTとGeminiに「おにぎり」の話でトークさせたら一生止まらなかった、最後はAI同士で機械語で語り合うかもしれない'
duration: "00:00"
layout: article
title: マジカルラブリー☆つむぎのピュアピュアA.I.放送局 podcast 20251208
image_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
thumbnail_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
card: summary
---

## 関連リンク


- [うちのAIがやらかしまして ─ Claude Codeの暴走を防ぐCLAUDE.md運用術](https://tech.findy.co.jp/entry/2025/12/06/070000)  


この記事では、AIエージェント「Claude Code」との協働で起こりがちな「うっかりミス」や「意図しない挙動」を防ぎ、より効果的にAIを使いこなすための具体的な運用術が紹介されています。特に、新人エンジニアの皆さんがAIツールを実務で使う際のヒントが満載です。

まず、実際にあったAIの「やらかし」事例が二つ挙げられています。
1.  **コードレビューでの誤ったコメント**: 「コードレビューを確認して」という指示に対し、AIが「確認して**対応する**」と解釈し、レビュアーに意図しないコメントを投稿してしまったケースです。原因は、人間側の指示が曖昧だったことでした。
2.  **Pull Requestに不要なファイルが混入**: 作業指示とは関係ない一時的なバックアップファイルが、AIが`git add -A`（全ての変更ファイルをステージングに追加するコマンド）を実行した際に、そのままPull Requestにコミットされてしまったケースです。これもAIに任せきりで、最終確認を怠ったことが原因でした。

これらの経験から、筆者は同じミスを繰り返さないための対策をまとめました。その中心となるのが、AIエージェントの振る舞いを定義する「CLAUDE.md」というファイルを使った運用術です。

**「CLAUDE.md」を活用した対策のポイント:**
*   **曖昧な指示の確認を促す**: 指示が不明確な場合、AIが勝手に判断して進めるのではなく、「この指示は具体的にどうすれば良いですか？」と質問を返すようにCLAUDE.mdに設定を追加しました。これにより、人間とAIの間の認識のズレを防ぎます。
*   **意図しない挙動の振り返り**: AIが期待と異なる動きをした場合、その原因をAI自身に分析させ、再発防止策を提案させます。そして、その対話で得られた教訓をCLAUDE.mdに追記し、AIが次に同じ状況に遭遇した際に適切に対応できるように学習させます。
*   **コミット前の最終確認の徹底**: 上記の振り返りから、Pull Requestに不要なファイルが混入しないよう、コミット前に以下のコマンドでステージングされているファイルを確認する手順をCLAUDE.mdに明記しました。
    *   `git status`: 現在の変更状況を確認
    *   `git diff --cached --name-only`: ステージングされているファイルの名前だけを確認し、意図しないファイルが含まれていないかをチェック

AIエージェントは非常に強力なツールですが、最初から完璧に動くわけではありません。私たち人間がAIに適切な指示を出し、その挙動を注意深く確認し、時には対話を通じて「成長」を促すことが大切です。AIを単なる道具ではなく、共に成長するパートナーとして捉え、積極的に関わっていくことで、開発効率を大きく向上させることができるでしょう。新人エンジニアの皆さんも、ぜひAIとの協働を楽しんでみてください。

引用元: https://tech.findy.co.jp/entry/2025/12/06/070000


- [Titans + MIRAS: Helping AI have long-term memory](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)  


現在のAIモデル、特に大規模言語モデル（LLM）の基盤である「Transformer」は、入力の中から重要な部分に注目する「アテンション（注意機構）」で革新をもたらしました。しかし、処理する情報の長さが長くなると計算コストが急増し、文書全体を理解したり、ゲノム解析のような非常に長い文脈を扱うのが苦手という課題がありました。

この課題に対し、従来の「リカレントニューラルネットワーク（RNN）」や「状態空間モデル（SSM）」である「Mamba-2」などは、情報を固定サイズに圧縮して効率化を図りました。しかし、これでは超長文の豊かな情報を十分に捉えきれない限界がありました。

Google Researchが発表した「Titans」と「MIRAS」は、この長期記憶の課題を解決する新しいアプローチです。「Titans」はRNNの速度とTransformerの精度を組み合わせた具体的なAIアーキテクチャ（ツール）で、「MIRAS」はこれらのアプローチを一般化するための理論的枠組み（設計図）です。両者は、AIモデルが実行中に新しい情報を学習し、長期的に記憶する「実行時記憶」の能力を大きく向上させます。

特にTitansは、人間の脳が短期記憶と長期記憶を分けているように、情報を要約しつつ重要な文脈を失わない「深層ニューラルネットワーク」を長期記憶モジュールとして活用します。この記憶モジュールは、従来の固定サイズメモリよりもはるかに高い表現力を持つため、膨大な情報を理解・統合できます。

Titansの重要な仕組みは、「サプライズ指標」です。これは、モデルが記憶している内容と新しい入力の間に大きな差がある場合に「重要」と判断するメカニズムです。例えば、モデルが金融レポートを読んでいる途中で、全く関係のない「バナナの皮の画像」といった予期せぬ情報が来た場合、それを「高サプライズ」として捉え、優先的に長期記憶に保存します。これにより、モデルは本当に新しい、あるいは文脈を大きく変える情報だけを選択的に記憶し、全体の処理を効率的に保ちます。さらに、直前の文脈の流れも考慮する「モメンタム」と、不要になった情報を適切に捨てる「忘却（重み減衰）」の仕組みを組み合わせることで、記憶の容量を効果的に管理します。

一方、MIRASは、全てのシーケンスモデルを「連想記憶」の一種と捉える統一的な理論的視点を提供します。これにより、従来のモデルが「平均二乗誤差（MSE）」や「ドット積類似度」といった標準的な評価基準に強く依存してきた限界を超え、より柔軟で頑健なモデル設計を可能にします。この枠組みから、データ中の例外に強い「YAAD」や、記憶安定性を重視した「MEMORA」といった新しいモデルも生まれました。

実験では、TitansとMIRASのモデルが、既存のTransformerやMamba-2といった最先端モデルを多くのベンチマークで上回る性能を示しました。特に、200万トークンを超えるような極めて長い文脈での理解能力において、GPT-4のような大規模モデルよりも少ないパラメータで、優れた成果を出しています。言語モデルだけでなく、ゲノムモデリングや時系列予測でもその汎用性が確認されました。

この「Titans」と「MIRAS」の登場は、AIがこれまで苦手としてきた「長文の深い理解」と「長期的な記憶」を可能にし、これからの長文AI時代の幕開けを告げる、非常に重要な技術進展と言えるでしょう。

引用元: https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/


- [TypeScript 向けの AI フレームワーク TanStack AI を試してみた](https://azukiazusa.dev/blog/try-tanstack-ai/)  


この記事は、TypeScript/JavaScript向けの新しい軽量AIフレームワーク「TanStack AI」について、新人エンジニアの方にも分かりやすく解説します。AIエージェント開発の第一歩を踏み出す上で非常に役立つツールです。

AIエージェントを開発する際、OpenAIやAnthropicといった複数のLLM（大規模言語モデル）のAPIを利用することがよくあります。しかし、これらのプロバイダーはそれぞれ異なる方法でAPIを提供しているため、開発中にモデルを切り替えたり、異なるモデルを組み合わせたりする際に、コードが複雑になりがちです。「TanStack AI」は、このようなLLMのAPI呼び出しの違いを吸収し、統一されたシンプルなインターフェースで扱えるようにするSDK（Software Development Kit）です。既存のAI SDKやPythonのLangChainと同様に、LLM活用のための基本的な抽象化を提供し、開発効率を大幅に向上させます。

TanStack AIの最も基本的な機能は、LLMとのチャット対話です。`chat`関数を使うことで、ユーザーのメッセージをLLMに送信し、その応答をストリーミング形式（テキストがリアルタイムで少しずつ表示される）で受け取ることができます。これにより、応答を待つ間のユーザー体験が向上します。また、アダプターパッケージを切り替えるだけで、使用するLLMプロバイダー（例: AnthropicのClaude、OpenAIのGPTなど）を簡単に変更できるため、将来的なモデルの選択肢が広がるのも大きなメリットです。

さらに、AIエージェントが外部システムと連携するための「ツール」機能もTanStack AIの重要な特徴です。例えば、「現在の天気情報を教えて」というユーザーのリクエストに対して、AIが外部の天気予報APIを呼び出して結果を返す、といったことが可能になります。TanStack AIでは、ツールの「定義」（どんな機能か）と「実装」（実際にどう動くか）が分離されています。この分離により、例えば同じ天気ツールでも、サーバーサイドではデータベースから情報を取得し、クライアントサイドではブラウザのローカルストレージを使う、といった柔軟な実装が可能になります。また、`Zod`というライブラリを活用することで、ツールの入出力データに厳密な型を定義でき、型安全な開発を進められます。セキュリティ面も考慮されており、AIがコード実行など危険な操作を伴うツールを呼び出す前に、ユーザーに承認を求める機能も簡単に組み込めます。

記事では、Next.jsを使った実践的なAIチャットボットの構築例も紹介されています。Next.jsのAPIルート（Route Handlers）でサーバーサイドのエンドポイントを作成し、そこでTanStack AIを使ってLLMと通信します。クライアントサイドでは、`@tanstack/ai-react`パッケージの`useChat`フックを利用することで、チャットの状態管理、メッセージの送信、ストリーミングされた応答の表示などを少ないコード量で効率的に実装できます。

まとめると、TanStack AIは、TypeScript環境でAIエージェントやAIアプリケーションを開発する新人エンジニアにとって、LLMとの連携を簡素化し、外部ツールとの安全かつ効率的な統合を実現するための、非常に強力で理解しやすいフレームワークと言えるでしょう。

引用元: https://azukiazusa.dev/blog/try-tanstack-ai/


- [ChatGPTとGeminiに「おにぎり」の話でトークさせたら一生止まらなかった、最後はAI同士で機械語で語り合うかもしれない](https://togetter.com/li/2636146)  


ChatGPTとGeminiが「おにぎり」について対談する様子が、人間らしい自然な会話で話題になっています。AIの息継ぎや表現力の豊かさが注目され、その音声対話の進化に多くの驚きの声が上がっています。この技術は、AIがパーソナリティを務めるラジオ番組や、新たなエンターテイメント創造の可能性も示唆しています。AIが身近な話題で盛り上がる様子は、新人エンジニアの皆さんにとって、AIの発展とユニークな活用例を理解する良いきっかけとなるでしょう。

引用元: https://togetter.com/li/2636146



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:春日部つむぎ
