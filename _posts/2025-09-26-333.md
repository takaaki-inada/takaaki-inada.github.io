---
actor_ids:
  - お嬢様ずんだもん
audio_file_path: /audio/私立ずんだもん女学園放送部_podcast_20250926.mp3
audio_file_size: 0
date: 2025-09-26 05:00:00 +0900
description: 'Gemini Robotics 1.5 brings AI agents into the physical world、Video models are zero-shot learners and reasoners、PostgreSQL 18 Released!、バーガーキング、ほぼ“肉”だけで勝負。100％ビーフパティをそのまま味わえる「オン ザ ビーフ」3種の販売決定。バーガーとは一体…'
duration: "00:00"
layout: article
title: 私立ずんだもん女学園放送部 podcast 20250926
image_url: https://zund-arm-on.com/images/ojousama_zundamon_square.jpg
thumbnail_url: https://zund-arm-on.com/images/ojousama_zundamon_thumbnail.jpg
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/HJAZpsCRmTM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Gemini Robotics 1.5 brings AI agents into the physical world](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/)  


Google DeepMindは、物理世界で機能するAIエージェントの新たな進化として「Gemini Robotics 1.5」と「Gemini Robotics-ER 1.5」を発表しました。この技術革新により、ロボットがこれまで以上に複雑で多段階のタスクを、まるで人間のように「考えて」実行できるようになります。

主要なモデルは二つです。
「**Gemini Robotics 1.5**」は、ロボットの目（視覚）と耳（言語指示）から得た情報をもとに、具体的な動き（アクション）を指示するモデルです。このモデルの特長は、行動する前に「どう動くべきか」を自分で考え、その思考プロセスを自然な言葉で説明できる点です。これにより、ロボットの行動がより透明になります。さらに、異なる種類のロボット（例えばアーム型や人型など）の間で学習した動きを転用できるため、新しいスキル習得が非常に効率的になりました。

もう一つは「**Gemini Robotics-ER 1.5**」です。これは、物理世界について深く推論し、Google検索のようなデジタルツールを自在に使いこなし、複雑なタスクのための詳細な計画を立てる、いわばロボットの「司令塔」のような役割を担います。このモデルは空間を正確に理解する能力に優れ、与えられたミッション達成のための多段階計画を自動で作成します。

これら二つのモデルは連携して動作します。まずGemini Robotics-ER 1.5が全体の戦略と高レベルな判断を下し、その計画に基づいてGemini Robotics 1.5が具体的な行動を指示・実行します。例えば、「洗濯物を色ごとに仕分けて」という指示に対して、ERモデルがインターネットで分別ルールを調べ、全体計画を立てます。その後、1.5モデルが実際に洗濯物を識別して適切な場所へ運ぶ、といった具体的な動きを担当します。これにより、ロボットは多様な環境やより長いタスクにも柔軟に対応できるようになります。

Google DeepMindは、これらのAIエージェント技術を安全かつ責任ある形で発展させることに重点を置いています。開発の初期段階から、ロボットが行動前に安全性を考慮したり、人間との適切な対話を行ったり、衝突回避システムと連携したりすることで、人間中心の環境で安心して利用できるロボットを目指しています。

このGemini Robotics 1.5は、物理世界における汎用人工知能（AGI）の実現に向けた重要な一歩と位置づけられています。単なる指示への反応を超え、自ら推論し、計画し、ツールを使いこなし、そして学習を汎化できるロボットの未来が期待されます。

開発者の皆さんへ：Gemini Robotics-ER 1.5は、Google AI StudioのGemini APIを通じて、本日より利用可能です。ぜひ、この新しい物理AIエージェントの可能性を探ってみてください。

引用元: https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/


- [Video models are zero-shot learners and reasoners](https://video-zero-shot.github.io/)  


Google DeepMindが発表した最新の研究は、動画モデル「Veo 3」がまるで人間のように、見たことのないタスクでも対応できる「ゼロショット学習」と「推論」能力を持つことを示しています。これは、AI分野、特に「マルチモーダルAI」（複数の情報形式を扱うAI）の進化において非常に重要な一歩です。

これまで、大規模言語モデル（LLM）がインターネット上の膨大なテキストデータを学習することで、人間が指示する様々な言語タスクをこなせる「基盤モデル」となりました。今回の研究は、同じように大規模な動画データを学習した生成動画モデルも、将来的にLLMが言語理解で果たした役割を、視覚理解の分野で果たす可能性を秘めていることを示唆しています。

Veo 3は、特定のタスク向けに明示的に訓練されていないにもかかわらず、驚くほど多岐にわたる視覚タスクをゼロショットで解決できます。例えば、動画から特定のオブジェクトを自動で切り抜いたり（セグメンテーション）、画像の端っこを認識したり（エッジ検出）といった基本的なことから、さらに以下のような複雑な操作が可能です。

*   **画像・動画の編集**: 背景の除去、スタイル変換、色付け、画像の足りない部分を補完するインペインティング、画像の外部を生成するアウトペインティング、テキスト操作、落書きによる画像編集など。
*   **シーンの理解と生成**: シーンの構成、新しい視点からの生成、3Dでのポーズ変更など。
*   **物理的な理解と操作**: 物体の持つ機能（アフォーダンス）を認識し、瓶を開ける、物を投げたりキャッチする、瞑想玉を扱うといった器用な操作のシミュレーション、視覚的な指示に従ってブリトーを作るなど。

さらに、迷路を解いたり、物体の対称性を理解するといった、初期段階の「視覚推論」能力まで見せています。

これらの能力は、Veo 3が単に動画を生成するだけでなく、視覚世界を「認識」「モデル化」「操作」し、さらには「推論」までできることを意味します。この研究は、動画モデルが将来的に、言語のLLMのように、あらゆる視覚タスクの基盤となる汎用的なAIモデルへと進化する可能性を示しており、私たち日本のエンジニアが今後のAI技術動向を理解する上で、ぜひ注目しておきたい画期的な発表と言えるでしょう。

引用元: https://video-zero-shot.github.io/


- [PostgreSQL 18 Released!](https://www.postgresql.org/about/news/postgresql-18-released-3142/)  


皆さん、こんにちは！人気のオープンソースデータベース、PostgreSQLの最新版「PostgreSQL 18」が2025年9月25日にリリースされました。AIやLLM（大規模言語モデル）のバックエンドとしてもよく使われるPostgreSQLの進化は、私たちエンジニアにとって大切なニュースです。

今回のリリースでは、特にデータベースの「速さ」「安定性」、そして「使いやすさ」が大きく改善されています。新人エンジニアの皆さんも、ぜひそのポイントを押さえておきましょう！

**1. 劇的なパフォーマンス向上**
一番の目玉は、新しい「非同期I/O (AIO)」という仕組みです。これは、データベースがデータを読み込む際に、複数の要求を同時に処理できるようにする技術です。まるで、一度にたくさんのトラックを動かして荷物を運ぶように、データのやり取りを効率化します。これにより、**データ読み込み速度が最大3倍にも向上する**ケースがあるそうです。これまでOS任せだったデータの読み込みを、データベース自身が賢くコントロールすることで、全体的な処理能力が大幅に上がりました。
他にも、クエリがインデックス（本の索引のようなもの）をもっと賢く利用できるようになり、複雑な条件のSQLでも自動的に速くなる可能性が高まります。

**2. アップグレードがスムーズに、開発も快適に**
データベースのメジャーバージョンアップグレードが、これまでよりも断然スムーズになりました。アップグレード時に、データベースがクエリを効率的に実行するための「統計情報」が引き継がれるようになり、**アップグレード直後の性能低下を抑え、すぐに安定したパフォーマンスを発揮**できるようになります。

開発者向けの機能としては、「仮想生成列」が導入されました。これは、テーブルに直接データを保存せずに、必要な時にだけ値を計算する列です。例えば、「商品の合計金額」のようなものを、毎回保存する手間なく、クエリ時に自動で計算させることができます。また、タイムスタンプ順に並べやすいUUIDを生成する`uuidv7()`関数も追加され、データ管理がより効率的になります。

**3. セキュリティと運用性の強化**
セキュリティ面では、認証方法として「OAuth 2.0」がサポートされ、シングルサインオン (SSO) システムとの連携が簡単になりました。これは、普段使っているGoogleアカウントなどでデータベースにログインできるようなイメージです。また、古い`md5`パスワード認証は非推奨となり、より安全な「SCRAM認証」の使用が強く推奨されます。

運用面では、SQLの実行計画を分析する`EXPLAIN`コマンドがさらに賢くなり、クエリがなぜ遅いのか、どこにボトルネックがあるのかを、バッファやI/Oの使用状況など、より詳細に教えてくれるようになりました。これにより、データベースのチューニングがしやすくなります。

PostgreSQL 18は、私たちが日々利用するアプリケーションのバックエンドとして、より高速で、より安全に、そして開発者にとってより親しみやすいデータベースへと進化しました。ぜひ、今回の新機能を活用して、皆さんの開発や運用に役立ててください！

引用元: https://www.postgresql.org/about/news/postgresql-18-released-3142/


- [バーガーキング、ほぼ“肉”だけで勝負。100％ビーフパティをそのまま味わえる「オン ザ ビーフ」3種の販売決定。バーガーとは一体…](https://news.denfaminicogamer.jp/news/2509253v)  


バーガーキングが、直火焼き100%ビーフパティを主役にした新商品「オン ザ ビーフ」3種を9月26日より2週間限定で販売します。「オン ザ ビーフ」は塩コショウでシンプルに肉の旨みを堪能でき、「ソース オン ザ ビーフ」は3種の特製ソースから選べます。さらに、京都の老舗米屋と共同開発したライスパティと組み合わせた「ライス オン ザ ビーフ」も登場。ハンバーガーの枠を超えた、肉本来の美味しさを存分に楽しめるユニークな商品です。

引用元: https://news.denfaminicogamer.jp/news/2509253v



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:ずんだもん
