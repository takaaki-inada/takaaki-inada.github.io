---
actor_ids:
  - 春日部つむぎ
audio_file_path: /audio/マジカルラブリー☆つむぎのピュアピュアA.I.放送局_podcast_20250421.mp3
audio_file_size: 0
date: 2025-04-21 05:00:00 +0900
description: 'Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs、ついに量子コンピュータでLLMの追加学習に成功。量子コンピュータとテンソルネットワークがLLM計算を変える。、テキスト埋め込みモデルPLaMo-Embedding-1Bの開発 - Preferred Networks Research &amp; Development、間違ったこと言った時の「あ、ごめん。嘘言った」という言い回しが通じなくて、『なんで嘘つくの！』と言われて少しトラブルになった話「エンジニアの癖ですよね」「関西は結構多くの人が使う印象」'
duration: "00:00"
layout: article
title: マジカルラブリー☆つむぎのピュアピュアA.I.放送局 podcast 20250421
image_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
thumbnail_url: https://zund-arm-on.com/images/tsumugi_podcast_thumbnail.png
card: summary
---

## 関連リンク


- [Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)  


Googleは、高性能な最新オープンAIモデル「Gemma 3」を発表しました。このモデルは高い性能を持っていますが、本来はBF16という精度で動かすために、NVIDIA H100のような高性能で高価なGPUが必要でした。

もっと多くの日本のエンジニアやAI開発者がこの素晴らしいモデルを気軽に使えるようにするため、Googleは「QAT (Quantization-Aware Training)」という技術で最適化された新しいGemma 3モデルを発表しました。

QATとは、AIモデルのデータを圧縮する「量子化」という技術を、モデルを訓練する（学習させる）段階から組み込む手法です。モデルのデータを小さくすることで、実行に必要なGPUのメモリ（VRAM）を大幅に減らすことができます。単純に量子化するとモデルの性能が落ちやすいのですが、QATを使うことで、性能の低下を最小限に抑えつつ、データサイズを小さくできるのが特徴です。

このQATによる最適化の効果は絶大です。例えば、Gemma 3の最大のモデルである27B（パラメータ数が多いほど高性能だがサイズも大きい）の場合、BF16精度だと54GBのVRAMが必要でしたが、int4という精度に量子化されたQATモデルでは、必要なVRAMがわずか14.1GBに減りました。これにより、NVIDIA RTX 3090（VRAM 24GB）のような、一般的に入手しやすいデスクトップPC向けGPUでも、これまで一部の研究者や企業でしか動かせなかったような高性能なGemma 3 27Bモデルを、自分のPCで実行できるようになります。

他のサイズのモデル（12B, 4B, 1B）も同様にVRAMが大幅に削減されており、ラップトップPCのGPUや、さらにメモリが少ないデバイスでも動かしやすくなりました。

これらのQAT最適化済みモデルは、Ollama、llama.cpp、MLXといった人気のAI実行ツールに対応しており、Hugging FaceやKaggleといったプラットフォームで公開されています。これにより、既存の開発環境に簡単に組み込んで試すことができます。

この取り組みは、最新のAI技術を特定の高性能ハードウェアを持つ人だけでなく、より多くのエンジニアが手軽に利用できるようにすることを目指しています。これにより、AI開発のハードルが下がり、新しいアイデアが生まれやすくなることが期待されます。高価なGPUがなくても、手元のPCで最先端のGemma 3モデルを使ってAI開発を始めるチャンスです。

引用元: https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/


- [ついに量子コンピュータでLLMの追加学習に成功。量子コンピュータとテンソルネットワークがLLM計算を変える。](https://blueqat.com/yuichiro_minato2/40faced2-a224-4c07-849d-3651ebcb3356)  


この記事では、最先端技術である量子コンピュータと大規模言語モデル（LLM）が結びつき、特にLLMの追加学習（ファインチューニング）の方法が大きく変わり始めている状況について解説しています。

これまでのLLM学習は、たくさんのGPUを使った大規模な計算が中心でした。しかし最近では、LoRA（Low-Rank Adaptation）のように、モデル全体ではなく一部だけを効率よく学習させる「軽量な微調整手法」が注目されています。量子コンピュータは、少ない計算資源（量子ビット）でも「重ね合わせ」や「もつれ」といった量子力学の不思議な性質を使うことで、高い表現力を持つ可能性があります。ここに、量子コンピュータを使ってLLMを扱うメリットがあります。

この量子コンピュータ上でのLLMの学習を可能にしている重要な技術の一つが「テンソルネットワーク」です。これは、量子状態や複雑なデータ構造を効率的に扱える数学的な手法で、巨大なLLMの一部を量子コンピュータ向けに変換したり学習させたりするのに役立ちます。これにより、従来の数学的手法では難しかった「量子的情報構造」の調整ができるようになります。

量子コンピュータを使ったLLMのファインチューニングは、ただ単に学習を速くするだけでなく、従来とは全く異なる「新しいやり方（別の相）」として捉えられています。量子コンピュータの性質を最大限に活かすには、従来の学習アルゴリズムだけでは不十分なため、量子回路に特化した新しい学習方法の研究も進んでいます。

実際に、いくつかの研究では、量子コンピュータ上でLLMのファインチューニングの一部を実行したり、特定の分野（法律や医療など）に特化した小さなモデルを量子コンピュータで調整したりする例が出てきています。これにより、計算の高速化だけでなく、LLMがより柔軟に対応できるようになり、少ないエネルギーで動いたり、より深い表現力を獲得したりすることが期待されています。

結論として、量子コンピュータがLLMに利用されることで変わるのは、単なる計算速度ではなく、LLMをデータに合わせて調整し、進化させる「学習・適応・最適化」というプロセスそのものです。LoRAやテンソルネットワークといった技術が、この新しい時代の扉を開いています。これは、AIが今までとは違う「量子的な考え方」を取り入れ始める可能性を示唆していると言えるでしょう。

引用元: https://blueqat.com/yuichiro_minato2/40faced2-a224-4c07-849d-3651ebcb3356


- [テキスト埋め込みモデルPLaMo-Embedding-1Bの開発 - Preferred Networks Research & Development](https://tech.preferred.jp/ja/blog/plamo-embedding-1b/)  


Preferred Networks（PFN）は、日本語の文章を数値ベクトルに変換する新しいモデル「PLaMo-Embedding-1B」を開発しました。これは、同社の大規模言語モデル（LLM）「PLaMo」をベースにしています。

テキスト埋め込みモデルは、文章の意味を捉えて数値ベクトルとして表現します。意味的に似た文章は近いベクトルに、遠い文章は離れたベクトルになります。これにより、ベクトルの類似度を計算することで、文章同士がどれくらい似ているかを測ることができます。この技術は、膨大な文章の中から関連性の高いものを探す「情報検索」や、最近注目されているLLMと検索技術を組み合わせた「RAG (Retrieval Augmented Generation)」などのシステム構築において非常に重要な役割を果たします。

PFNが開発したPLaMo-Embedding-1Bは、日本語のテキスト埋め込みモデルの性能を測るベンチマーク「JMTEB」で、公開時点でOpenAIの高性能モデルなどを上回り、トップクラスの評価を得ました。特に、文章検索に関わるRetrievalタスクで優れた性能を発揮しています。

この高い性能を達成するために、いくつかの技術的な工夫が行われています。
1.  **LLM構造の応用**: 言語生成に使われるLLMの構造を、テキスト全体を考慮して埋め込みベクトルを計算する Bidirectional モデルに適した形に変換する「LLM2Vec」という手法を取り入れています。
2.  **段階的な学習プロセス**: まず、大量のデータを使って基本的な能力を高める「事前学習」を行い、その後、より質の高いデータで精度を向上させる「ファインチューニング」を行いました。
3.  **効率的な学習**: 学習では「対照学習」という手法を用いて、意味的に関連のある文章のベクトルは近づけ、そうでない文章のベクトルは遠ざけるようにモデルを調整しています。
4.  **データの工夫**: 異なる種類のデータで事前学習したモデルを組み合わせる「モデルマージ」や、リランカーという別のモデルを使って学習データの質をチェックし、適切なデータを選ぶフィルタリングを行いました。また、学習をより効果的にするために、一見似ているが意味が異なる「Hard-negative」と呼ばれる難しいサンプルも活用しています。
5.  **検索特化の調整**: 検索タスクで使われる「質問文（クエリ）」に対して、特定の短いフレーズ（prefix）を付けることで、クエリと検索対象の文章のベクトル表現に非対称性を導入し、検索精度を高める工夫も施されています。

開発されたPLaMo-Embedding-1Bモデルは、Hugging Faceで公開されており、Apache v2.0ライセンスのもと、個人利用・商用利用を問わず誰でも自由に利用できます。

これらの様々な技術と工夫により、日本語のテキスト埋め込みにおいて高い性能を持つPLaMo-Embedding-1Bが実現しました。この開発で得られた知見は、今後のPFNにおけるLLM開発にも活かされていく予定です。

引用元: https://tech.preferred.jp/ja/blog/plamo-embedding-1b/


- [間違ったこと言った時の「あ、ごめん。嘘言った」という言い回しが通じなくて、『なんで嘘つくの！』と言われて少しトラブルになった話「エンジニアの癖ですよね」「関西は結構多くの人が使う印象」](https://togetter.com/li/2540758)  


今回の記事は、Twitter（現X）で話題になった、間違ったことや勘違いを訂正する際に使う「あ、ごめん。嘘言った」という言い回しが、人によっては伝わりにくく、誤解を招くことがある、という体験談をまとめたものです。

発端となったツイートでは、このフレーズを使ってしまい、相手に「なんで嘘つくの！」と問われて少しトラブルになったことが語られています。そして、「これって理系用語なのだろうか？」と疑問が投げかけられました。

この問いに対し、多くのユーザーから様々な意見が寄せられました。「エンジニアの癖だ」「理系やエンジニア界隈でよく聞く言い方だ」という声が多く、システムのパラメータや数値の誤りをすぐに訂正する際や、通信・放送の現場で使われることがあるなど、具体的な場面を挙げる人もいました。これは、単なる間違いや勘違いであり、意図的に事実と異なることを述べた「嘘」ではない、というニュアンスで使われているようです。中には、高校の理科や工業の先生が使っていたのを聞いて覚えた、研究職でも使う、といった経験談もありました。

一方で、「関西地方ではエンジニアかどうかに関わらず、普通に使う人が多い印象だ」という地域差に関する指摘もありました。短い言葉で端的に誤りを伝えられる点が、理由として推測されていました。

しかし、記事の発端となった体験談が示すように、この言い回しは誤解を生む可能性があります。「嘘」という言葉は、通常、意図的な欺きを連想させるため、悪意なく使ったとしても相手に不信感を与えてしまうリスクがあります。実際、「事実と異なること＝嘘なので用法は合っているが、悪意の有無が伝わらないと問題になる」といった意見もありました。

新人エンジニアの皆さんにとって、このような「業界内では当たり前のように使われているけれど、一般的には少し違ったニュアンスで捉えられる言葉」があることを知っておくのは大切なことです。技術的な専門用語だけでなく、日常会話に近いフレーズでも、特定のコミュニティ内でのみ通じる独特の表現が存在することがあります。

コミュニケーションにおいては、自分の意図が正確に相手に伝わることが重要です。「嘘言った」以外にも、間違えを訂正する表現は「間違えました」「訂正します」「正確には〜です」など様々あります。状況や相手に合わせて、より分かりやすく、誤解のない言葉を選ぶことを心がけましょう。

仕事では、技術力だけでなく、チーム内外の人たちと円滑に連携するためのコミュニケーション能力も非常に重要です。言葉の選び方一つで、相手の受け取り方やその後の関係性が変わることもあります。今回の記事を参考に、普段自分が使っている言葉について少し意識を向けてみるのも良い学びになるはずです。

引用元: https://togetter.com/li/2540758



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:春日部つむぎ
