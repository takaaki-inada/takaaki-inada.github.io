---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20260205.mp3
audio_file_size: 0
date: 2026-02-05 05:00:00 +0900
description: 'Build with Kimi K2.5 Multimodal VLM Using NVIDIA GPU-Accelerated Endpoints、Apple SiliconでAIやっている人に朗報です。vllm-mlxが凄い。、Finance – Claude Plugin  Anthropic、東北復興キャラ「ずんだもん」絵本が子どもたちへ クラファンで目標の倍以上220万円が集まった理由とは…宮城・白石市'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20260205
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/4T02oZst_rs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Build with Kimi K2.5 Multimodal VLM Using NVIDIA GPU-Accelerated Endpoints](https://developer.nvidia.com/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/)  


NVIDIAは、Moonshot AIが開発した最新のオープンなマルチモーダル視覚言語モデル（VLM）である「Kimi K2.5」が、NVIDIAのGPUアクセラレーションエンドポイントで利用可能になったことを発表しました。このモデルは、テキストだけでなく画像やビデオの入力にも対応しており、高度な推論、コーディング、数学、そして自律的に動く「AIエージェント」のワークフローにおいて非常に高い性能を発揮します。

新人エンジニアが注目すべき技術的特徴は、その効率的なアーキテクチャです。Kimi K2.5は「混合エキスパート（MoE: Mixture-of-Experts）」という仕組みを採用しています。総パラメータ数は1兆（1T）という巨大な規模ですが、推論時にはそのうちの3.2%（約330億パラメータ）のみを動的に使用するため、高い処理能力と効率性を両立させています。また、262Kという非常に長いコンテキストウィンドウ（一度に読み込める情報量）を持っており、膨大な資料や長い動画の解析にも適しています。

視覚処理の面では、独自の「MoonViT3d Vision Tower」を搭載しており、画像やビデオフレームを効率的にベクトルデータに変換します。トレーニングにはNVIDIAの「Megatron-LM」フレームワークが使用されており、GPUの並列処理能力を最大限に引き出す最適化が施されています。

開発者向けの活用方法として、以下の3つのステップが紹介されています。
1. **プロトタイピング**: NVIDIA Developer Programに登録すれば、ブラウザ上のプレイグラウンド（build.nvidia.com）で無料かつ手軽にモデルの性能を試すことができます。
2. **API利用**: OpenAI互換のAPIエンドポイントが提供されているため、Pythonなどのコードから簡単にモデルを呼び出してアプリケーションに組み込めます。
3. **デプロイとカスタマイズ**: 高速な推論を実現する「vLLM」でのデプロイや、NVIDIA NeMo Frameworkを用いた独自のデータによる微調整（ファインチューニング）もサポートされています。

NVIDIAの最新GPU環境に最適化されたこの強力なオープンモデルは、これからのAIアプリケーション開発において、エンジニアにとって非常に魅力的な選択肢となるでしょう。

引用元: https://developer.nvidia.com/blog/build-with-kimi-k2-5-multimodal-vlm-using-nvidia-gpu-accelerated-endpoints/


- [Apple SiliconでAIやっている人に朗報です。vllm-mlxが凄い。](https://qiita.com/yosim/items/bbc8671d4295139c6e6d)  


Apple Silicon（Mac）でのLLM実行環境を劇的に進化させる新しいフレームワーク「vllm-mlx」についての解説記事です。これまで高性能な推論サーバーの代名詞であった「vllm」は、Mac環境ではCPU実行に限定されるなどの制約がありましたが、本プロジェクトはApple純正の計算ライブラリ「MLX」をベースにすることで、MacのGPU（Metal）性能を最大限に引き出したvllmライクなインターフェースを実現しています。

### 概要
vllm-mlxは、Apple Silicon（M1〜M4チップ）にネイティブ対応した、マルチモーダルな推論プラットフォームです。単なるモデル実行用のラッパーにとどまらず、プロダクトレベルの運用に耐えうる高度なメモリ管理機能とスループット性能を備えている点が最大の特徴です。

### 主な特長
- **マルチモーダル対応**: テキストだけでなく、画像、動画、音声の推論を一つのプラットフォームで統合的に扱えます。
- **圧倒的なパフォーマンス**: vllmと同じ「Paged KV Cache（ページングKVキャッシュ）」アーキテクチャを採用。従来のMLX関連ツールと比較して、処理スピードが**1.14倍高速化**し、メモリ消費量を**約80%に節約**することに成功しています。
- **高度なサービング機能**: 複数ユーザーの同時接続を効率よく処理する「連続バッチ処理（Continuous Batching）」に対応しています。
- **OpenAI API互換**: OpenAIクライアントをそのまま代替として利用可能なローカルサーバーを構築できます。
- **MCPツール呼び出し**: モデルコンテキストプロトコル（MCP）を介して外部ツールと連携でき、AIエージェントの開発にも適しています。

### 新人エンジニアに向けた注目ポイント
Mac一台で「爆速かつ省メモリ」なLLM環境が手に入ることは、開発効率を大きく高めます。特に、これまで個別に使い分ける必要があった「mlx-lm（言語モデル用）」や「mlx-vlm（画像モデル用）」などのツールが、このフレームワーク一つで統一的に扱えるようになる点は非常に大きなメリットです。

### 制約
- **動作環境**: Apple Silicon（M1/M2/M3/M4）搭載のMacが必須です。
- **モデル形式**: Hugging Faceの`mlx-community`リポジトリなどで公開されている、MLX形式に変換・量子化されたモデルを使用することが推奨されます。
- **連携性**: Ollamaなどの既存ツールに比べると、他のAI基盤とのエコシステム連携はまだ発展途上の段階にありますが、OpenAI API互換性により多くのAIエージェントアプリとの連携は既に可能です。

引用元: https://qiita.com/yosim/items/bbc8671d4295139c6e6d


- [Finance – Claude Plugin  Anthropic](https://claude.com/plugins/finance)  


Anthropicが公式に提供を開始した、財務・経理業務に特化したClaude用プラグイン「Finance」に関する紹介記事です。このツールは、企業の財務担当者が日々行う複雑なワークフローをAIの力で効率化することを目的に開発されました。

このプラグインの最大の特徴は、専門的な財務処理を「スラッシュコマンド」によって実行できる点にあります。具体的には、以下のような業務をサポートします。

1. **仕訳と決算処理（/journal-entry）**: 未払費用の計上や固定資産、給与などの仕訳入力を、正しい借方・貸方のルールに基づいて準備します。
2. **残高照合（/reconciliation）**: 総勘定元帳（GL）と銀行明細、サブシステム、あるいは第三者のデータと比較し、不一致項目の特定を支援します。
3. **財務諸表の作成と分析（/income-statement, /variance-analysis）**: 期間比較を含めた損益計算書の作成や、予算と実績の差異をウォーターフォール分析（要因分析）によって可視化します。
4. **コンプライアンス対応（/sox-testing）**: SOX法（内部統制）に基づいた監査用テストのワークペーパー作成を支援します。

技術的な側面では、Anthropicが提唱する「MCP（Model Context Protocol）」を活用している点が重要です。これにより、ERP（企業資源計画システム）やデータウェアハウス、スプレッドシートといった外部のデータソースとClaudeを直接連携させることが可能になります。ファイルを手動でアップロードする手間を省き、基幹システムにある生のデータに基づいた高度な分析を実現します。

新人エンジニアの方にとって注目すべき点は、汎用的なAI（LLM）が「特定の専門ドメイン（この場合は財務）」において、どのようなインターフェースとプロトコルを通じて実務に組み込まれていくのかという具体例であることです。単なるチャットボットを超え、企業の基幹データと繋がり、専門的なルールに基づいたタスクを遂行する「AIエージェント」に近い活用形を示しています。

なお、本ツールは強力な支援機能を提供しますが、財務報告や法定書類として使用する前には、必ず資格を持つ専門家によるレビューが必要であるという運用上の注意点も強調されています。AIを信頼できる「副操縦士」として、人間の監督下で活用する現代的なAI活用のベストプラクティスを体現したプラグインと言えます。

引用元: https://claude.com/plugins/finance


- [東北復興キャラ「ずんだもん」絵本が子どもたちへ クラファンで目標の倍以上220万円が集まった理由とは…宮城・白石市](https://topics.smt.docomo.ne.jp/amp/article/tbcsendai/region/tbcsendai-2446918)  


東北復興応援キャラクター「ずんだもん」の絵本を子供たちに届けるプロジェクトが話題です。仙台の飲食店店主が実施したクラウドファンディングには、目標の2倍を超える約220万円の支援が集まり、宮城県白石市へ絵本が寄贈されました。ずんだもんは東北の企業であれば無償で商用利用できるという、OSSにも似た「地域開放型」の権利形態が特徴です。ファンに支えられ、地域活性化に貢献する心温まるニュースです。

引用元: https://topics.smt.docomo.ne.jp/amp/article/tbcsendai/region/tbcsendai-2446918



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
