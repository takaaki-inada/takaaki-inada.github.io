---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20260113.mp3
audio_file_size: 0
date: 2026-01-13 05:00:00 +0900
description: 'AI エージェントのために CLI でブラウザを操作する agent-browser、NeuralGCM harnesses AI to better simulate long-range global precipitation、数GBのLLMモデルを、LambdaでLinuxシステムコールを駆使して本番水準で動かす、ずんだもんの読み上げアプリをリリースしました！'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20260113
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/uBOD0YLzdbw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [AI エージェントのために CLI でブラウザを操作する agent-browser](https://azukiazusa.dev/blog/agent-browser-for-ai-agents/)  


本記事は、Vercelが開発したAIエージェント向けのブラウザ操作ツール「agent-browser」について解説しています。AIエージェントが自律的にタスクをこなす際、特にフロントエンド開発においては「ブラウザ上での動作確認」が不可欠ですが、これまでの手法にはLLMの特性上の課題がありました。

### 1. 概要と開発の背景
「agent-browser」は、CLI（コマンドラインインターフェース）を通じてブラウザを操作するためのツールです。これまでAIにブラウザを操作させる手段としては「Playwright MCP」などが主流でしたが、これらはツールの定義や操作の中間過程がLLMの「コンテキスト（一度に処理できる情報量）」を大量に消費してしまうという弱点がありました。Vercelが開発したこのツールは、CLIコマンドを介してやり取りを完結させることで、コンテキストの消費を抑えつつ効率的なブラウザ操作を実現するように設計されています。

### 2. 主な特徴と制約
このツールの最大の特徴は、現在のページ状態を「アクセシビリティツリー」として取得できる点にあります。
- **効率的な状態把握**: スクリーンショット画像のような重いデータではなく、要素の階層構造やテキスト内容を示す軽量なテキスト情報をLLMに渡すため、AIがページの構造を素早く正確に理解できます。
- **動作環境と制約**: 内部でPlaywrightを利用しているため、実行環境にはChromiumブラウザのインストールが必要です。また、ブラウザ自体はバックグラウンド（ヘッドレス）で動作するため、リソース消費を抑えた運用が可能です。
- **操作の仕組み**: `open`（URLを開く）、`snapshot`（状態取得）、`click`（クリック）といったシンプルなコマンドをAIに実行させることで操作を行います。

### 3. AIエージェントへの組み込み
Claude Codeなどの最新ツールと組み合わせることで、開発中のWebアプリの動作確認をAIに自律的に行わせることができます。「エージェントスキル」という仕組みを利用して、AIが必要な時だけこのツールを呼び出すように設定すれば、LLMの記憶容量を節約しながら高度な自動化が可能になります。

### 4. 既存ツールとの比較
Playwright MCPと比較した場合、agent-browserは「コンテキスト消費の少なさ」で勝りますが、要素の特定やクリックの正確性において、現時点ではPlaywright MCPの方が安定している面もあります。これは、AIへのフィードバックの返し方やセレクタの指定方法の違いによるものです。

### まとめ
新人エンジニアの皆さんにとって、AIに「ブラウザを操作する能力」を与えることは、テストやデバッグの自動化を劇的に進化させる第一歩となります。agent-browserは、Vercelらしいシンプルかつ効率的なアプローチで、AIエージェントによる開発支援をより実用的なものにしてくれるツールです。

引用元: https://azukiazusa.dev/blog/agent-browser-for-ai-agents/


- [NeuralGCM harnesses AI to better simulate long-range global precipitation](https://research.google/blog/neuralgcm-harnesses-ai-to-better-simulate-long-range-global-precipitation/)  


Google Researchが発表した「NeuralGCM」は、従来の物理ベースの気象モデルとAI（ニューラルネットワーク）を組み合わせたハイブリッド型の地球規模大気シミュレーションモデルです。特に予測が困難とされる「降水量」のシミュレーションにおいて、劇的な精度向上を達成しました。

### 1. 物理学とAIの「ハイブリッド」というアプローチ
従来の気象予測には、流体力学などの物理方程式を解く「数値気象予測」が使われてきました。しかし、雨の源となる「雲」は数百メートル単位の極めて小さなスケールで発生するため、地球規模の大きな網目（グリッド）で計算するモデルでは、細かな物理現象を正確に計算できず、近似式（パラメータ化）に頼らざるを得ないという課題がありました。
NeuralGCMは、大きなスケールの動きは物理エンジンで計算し、雲の形成や降水といった複雑で小さなスケールの現象をAIが担当することで、この課題を解決しています。

### 2. NASAの衛星データによる直接学習
今回の大きな進展は、AI部分の学習に「NASAの衛星観測データ（IMERG）」を直接使用したことです。
これまでのAIモデルの多くは、物理モデルと観測値を組み合わせた「再解析データ（ERA5など）」を学習に使っていましたが、これには元の物理モデルが持つ「雨が降りすぎる、あるいは極端な豪雨を過小評価する」といった弱点が含まれていました。NeuralGCMは、衛星から得られた「生の降水データ」を学習することで、物理モデルの限界を超えた高精度な予測を可能にしました。

### 3. エンジニアが注目すべき3つの成果
*   **予測精度の圧倒的な向上**: 15日間の長期予測において、世界最高峰の物理モデル（ECMWF）を上回る精度を記録しました。また、数十年単位の気候シミュレーションでは、従来の主要モデル（IPCC報告書で使用されるもの）と比較して、誤差を40%削減しました。
*   **「極端な気象」の再現**: これまでのモデルが苦手としていた「100年に一度の嵐」のような極端な降水現象（上位0.1%の豪雨）を正確にシミュレートできるようになりました。
*   **日次サイクルの正確性**: 例えばアマゾンの熱帯雨林では「午後に雨が降る」という明確な1日のサイクルがありますが、従来のモデルは数時間ズレることが一般的でした。NeuralGCMは、この降雨のタイミングと量も正確に再現します。

### 4. オープンソースと社会実装
このプロジェクトは、すでにGitHubでコードが公開されており、ライブラリとして利用可能です。実際の活用事例として、インドの農業現場でモンスーン（季節風）の到来時期を予測するパイロットプログラムにも採用され、3,800万人以上の農家を支援するツールとして実戦投入されています。

AIと物理学を対立させるのではなく、それぞれの得意分野を組み合わせることで、気候変動という地球規模の課題に挑むエンジニアリングの最前線を示す事例と言えます。

引用元: https://research.google/blog/neuralgcm-harnesses-ai-to-better-simulate-long-range-global-precipitation/


- [数GBのLLMモデルを、LambdaでLinuxシステムコールを駆使して本番水準で動かす](https://nealle-dev.hatenablog.com/entry/2026/01/08/103135)  


本記事は、数GB規模のローカルLLM（llama.cppなど）をAWS Lambda上で動作させ、本番環境に耐えうる低レイテンシを実現するための高度な技術スタックを解説したレポートです。通常、サーバーレス環境で巨大なモデルを扱うには「パッケージサイズ制限」と「起動時間（コールドスタート）」が大きな壁となりますが、これらをLinuxのシステムコールやAWSの最新機能を組み合わせて突破しています。

主な技術的ポイントは以下の3点です。

1. **Linuxシステムコール `memfd_create` によるストレージ制限の回避**
Lambdaで高速起動を実現する機能「SnapStart」を利用する場合、一時ストレージ（`/tmp`）は512MBに制限されます。数GBのモデルはこの制限に収まりませんが、本手法ではLinuxのシステムコール `memfd_create` を使用してメモリ上に仮想的なファイル（インメモリファイル）を作成します。S3からモデルデータを直接このメモリ領域にストリーミング書き込みすることで、ディスク領域を介さず、容量制限をバイパスしてモデルをロードすることに成功しています。OSの仕組みを応用した非常にスマートなハックです。

2. **Lambda SnapStartによる起動時間の劇的な短縮**
巨大なモデルを起動のたびにS3からダウンロードすると、コールドスタートに80秒以上かかる場合があります。これに対し、SnapStartを有効化することで、モデルをメモリにロードした初期化済みの状態をスナップショットとして保存します。これにより、次回以降の起動時にはダウンロード時間をスキップでき、スナップショットからの復元（約4秒程度）のみで動作を開始できるようになります。

3. **Lambda Web Adapterを用いたレスポンスのストリーミング**
LLMのUXを向上させるには、生成されたテキストを順次送信する「ストリーミング」が不可欠です。本アーキテクチャでは「Lambda Web Adapter」と「FastAPI」を組み合わせ、Lambda Function URLs経由でストリーミングを実現しています。結果として、最初の1文字目が届くまでの時間（TTFT）を1〜3秒程度まで短縮しており、サーバーレスとは思えない応答性を確保しています。

**新人エンジニアへの学び:**
この事例は、クラウドサービスの制約を「OSレベルの機能（システムコール）」や「ツールの内部仕様」を深く理解することで解決する、エンジニアリングの醍醐味を体現しています。単に既存の機能を組み合わせるだけでなく、「なぜ制限があるのか」「どうすればOSの仕組みで回避できるか」という一歩踏み込んだ視点が、パフォーマンスの限界を突破する鍵になることを教えてくれます。LLMに限らず、巨大なデータを扱うバックエンド開発において非常に汎用性の高い知見が詰まっています。

引用元: https://nealle-dev.hatenablog.com/entry/2026/01/08/103135


- [ずんだもんの読み上げアプリをリリースしました！](https://pr-free.jp/2026/140821/)  


合同会社わらしべTechが、iOS向けテキスト読み上げアプリ「よみあげボイス」をリリースしました。本アプリは音声合成エンジン「VOICEVOX」を搭載し、ずんだもん等の人気キャラで高品質な音声を生成可能です。モバイル特化のUIにより、スマホ操作だけでイントネーション等の細かな調整ができ、PC環境に劣らない効率でショート動画のナレーション制作等が行える、クリエイターの創作を支援するツールです。

引用元: https://pr-free.jp/2026/140821/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
