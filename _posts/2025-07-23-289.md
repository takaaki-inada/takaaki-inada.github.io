---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250723.mp3
audio_file_size: 0
date: 2025-07-23 05:00:00 +0900
description: 'Gemini 2.5 Flash-Lite is now stable and generally available、Pioneering an AI clinical copilot with Penda Health、Understanding NCCL Tuning to Accelerate GPU-to-GPU Communication'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250723
---

## 関連リンク


- [Gemini 2.5 Flash-Lite is now stable and generally available](https://deepmind.google/discover/blog/gemini-25-flash-lite-is-now-ready-for-scaled-production-use/)  


Googleは、最新のAIモデル「Gemini 2.5 Flash-Lite」の安定版を一般公開しました。これは、GoogleのGemini 2.5モデルファミリーの中で、最も高速で、かつ最も低コストなAIモデルです。AI開発の現場で「費用対効果（インテリジェンス・パー・ドル）」を最大限に高めることを目指して作られ、特に返答速度が重要な翻訳や分類のようなタスクにぴったりです。

この「Gemini 2.5 Flash-Lite」の主なポイントは以下の通りです。

*   **クラス最高のスピード**: 従来の2.0 Flash-Liteや2.0 Flashモデルよりも、幅広いタスクでより低い遅延（レイテンシー）を実現しています。つまり、処理がより速いということです。
*   **非常に高い費用対効果**: 入力トークン100万個あたり0.10ドル、出力トークン100万個あたり0.40ドルという、これまでの2.5モデルの中で最も低価格です。さらに、音声入力の価格も以前より40%削減され、大量のリクエストを低コストで処理できます。
*   **賢くてコンパクト**: モデル自体はコンパクトながらも、プログラミング、数学、科学、推論、そして画像や動画などのマルチモーダルな理解能力において、幅広い分野で高い品質を示しています。
*   **充実した機能**: 100万トークンという非常に長い文章を一度に扱える（コンテキストウィンドウ）、AIの思考プロセスを制御できる「思考予算」、Google検索と連携する「Grounding with Google Search」、コード実行機能、URLの内容を理解する機能など、開発に役立つ様々な機能が備わっています。

すでに多くの企業がこのモデルを活用し、素晴らしい成果を出しています。例えば、Satlytは衛星データの処理速度を45%向上させ、消費電力を30%削減しました。HeyGenはAIを使って動画の企画を自動化し、180以上の言語への動画翻訳を可能にしています。DocsHoundは製品デモ動画から数千枚のスクリーンショットを素早く抽出し、ドキュメント作成やAIエージェントのトレーニングデータ生成を効率化しています。EvertuneはAIモデルにおけるブランド表現の分析を高速化し、クライアントにリアルタイムな洞察を提供しています。

Gemini 2.5 Flash-Liteは、Google AI StudioやVertex AIで「gemini-2.5-flash-lite」という名前を指定することで、すぐに使い始めることができます。コストと性能のバランスが取れたこのモデルは、皆さんの今後のAI開発に大きな可能性をもたらすでしょう。

引用元: https://deepmind.google/discover/blog/gemini-25-flash-lite-is-now-ready-for-scaled-production-use/


- [Pioneering an AI clinical copilot with Penda Health](https://openai.com/index/ai-clinical-copilot-penda-health)  


AI技術、特に大規模言語モデル（LLM）は、医療の質向上に大きな可能性を持っています。しかし、AIの能力と実際の現場での活用には「モデルと実装のギャップ」という課題があります。この課題解決のため、OpenAIはケニアのPenda Healthと共同で、医師向けのAIコパイロット「AI Consult」を開発し、その導入効果を研究しました。

AI Consultは、GPT-4oを基盤としたAIアシスタントです。医師が患者の診察中に記録を入力すると、AI Consultはリアルタイムで潜在的な診断や治療のエラーを検出し、医師にフィードバックします。これは、医師の最終判断を尊重しつつ、見落としを防ぐ「セーフティネット」として機能します。

約4万件の患者診察データを対象とした大規模な研究の結果、AI Consultを導入した医師グループでは、導入しなかったグループと比較して、診断エラーが16%減少、治療エラーが13%減少という顕著な効果が確認されました。このデータは、AIが医療現場で具体的な成果を上げ、患者ケアの質を向上させることを明確に示しています。

この成功には三つの重要な要因がありました。一つ目は、GPT-4oのような「高性能なAIモデル」を活用したこと。二つ目は、医療現場のワークフローにシームレスに溶け込むよう「臨床現場に合わせた設計」を行ったこと。そして三つ目は、医師がAI Consultを効果的に活用できるよう、トレーニングやコーチング、インセンティブ付与といった「積極的な導入支援」を徹底したことです。

今回の事例は、AIが医療分野で大きな価値を生み出す可能性を示唆しています。新人エンジニアの皆さんにとって、システム開発において技術力だけでなく、実際の利用者や現場のニーズに合わせた設計、そして導入後の手厚いサポートがいかに重要であるかを教えてくれる、実践的な学びとなるでしょう。AIは「仕事を奪う」のではなく、「仕事をサポートし、より良い成果を出す」ための強力なパートナーとなり得ます。

引用元: https://openai.com/index/ai-clinical-copilot-penda-health


- [Understanding NCCL Tuning to Accelerate GPU-to-GPU Communication](https://developer.nvidia.com/blog/understanding-nccl-tuning-to-accelerate-gpu-to-gpu-communication/)  


大規模なAI処理において、GPU間の高速なデータ通信は非常に重要です。この通信を担うのがNVIDIA Collective Communications Library（NCCL）です。NCCLは、GPUの性能を最大限に引き出すために多くの最適化を内部で行っていますが、様々なシステム環境では、標準設定のままだと最高の性能が出ないことがあります。この記事では、なぜNCCLの調整（チューニング）が必要なのか、そしてどのように性能を向上させるかについて解説しています。

NCCLは、実行する通信処理（例えば、全GPUでデータを集計する「Allreduce」のような操作）に対して、最適な「プロトコル」（通信方式）や「アルゴリズム」（処理手順）、さらにGPU内の計算単位である「CTA（Cooperative Thread Array）」の数などを、内部の「コストモデル」と「ダイナミックスケジューラ」を使って自動的に選びます。コストモデルは処理にかかる時間を予測し、最適なプロトコルとアルゴリズムを決定します。ダイナミックスケジューラは、データサイズに応じてCTAの数やデータの分割方法を調整し、並列処理を効率的に行います。

しかし、ネットワーク機器やCPU、PCIeの設定など、システムの環境によっては、NCCLがデフォルトで選ぶ設定が必ずしも最適ではない場合があります。このような状況では、手動でNCCLの動作を調整することがパフォーマンス向上につながります。

この調整を行うための推奨される方法が「チューナープラグイン」です。チューナープラグインは、NCCLのデフォルト設定を上書きし、特定の環境に最適な通信プロトコルやアルゴリズム、CTA数を適用できるようにする仕組みです。このプラグインはアプリケーションのコードを変更することなく透過的に動作するため、大規模なAIワークロードを運用するクラスター管理者などが、自社の環境に合わせて調整済みのプラグインをユーザーに提供するケースが多いです。

チューニングを行う際には注意も必要です。例えば、CTA数を闇雲に増やしても、GPUのリソースを過剰に消費してしまい、かえって全体のパフォーマンスが悪化することがあります。NCCLは、利用可能な帯域幅を飽和させるために「ちょうど良い」CTA数を選ぶように設計されています。そのため、もしパフォーマンスに問題があると感じたら、それがNCCLのチューニングの問題なのか、あるいは根本的なシステム設定の問題なのかを最初に確認することが大切です。

チューナープラグインの他に、環境変数を設定してNCCLの動作を調整する方法もありますが、これは設定がグローバルに適用されるため、ベンチマーク目的以外では慎重に使うべきです。

記事では、実際のパフォーマンスグラフ（S-カーブ）を例に、デフォルト設定のままでは性能が低下するケースを示し、NVIDIAが提供する「example tuner plugin」を使って、どのように最適な設定を見つけて適用し、パフォーマンスを劇的に改善したかという事例も紹介しています。

NCCLのチューニングは、AIやHPC（高性能計算）のワークロードで、GPUの通信性能を最大限に引き出すために非常に重要です。チューナープラグインを適切に活用することで、システムの特性に合わせた最適な通信を実現し、AIモデルの学習や推論をより高速に進めることができます。

引用元: https://developer.nvidia.com/blog/understanding-nccl-tuning-to-accelerate-gpu-to-gpu-communication/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
