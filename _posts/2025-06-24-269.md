---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250624.mp3
audio_file_size: 0
date: 2025-06-24 05:00:00 +0900
description: 'AI Agent Manager (AAM) として生きていく : 作業環境とワークフローの設計、LiteLLMを使ったLLMの集約 &amp; 簡易的なKey管理 + langfuse添え、The rise of &quot;context engineering&quot;、プログラマ不要論が度々話題に上がる一方、正確な要件定義とちゃんとした成果物レビューが求められているが、それってプログラミング能力そのものなのでは？'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250624
---

## 関連リンク


- [AI Agent Manager (AAM) として生きていく : 作業環境とワークフローの設計](https://qiita.com/icoxfog417/items/f15e92f05b14411fd642)  


この記事は、AI Agent（AIアシスタント）が開発現場の主役になる未来を見据え、人間のエンジニアが「AI Agent Manager (AAM)」という新しい役割を担う可能性について解説しています。これは、まるで人間の上司が部下をマネジメントするように、AI Agentを管理する仕事が中心になる、という考え方です。

AAMの仕事は、従来のエンジニアが直接コードを書くこととは少し違います。AI Agentが効率的に開発を進められるように、以下のような管理業務が主な役割になります。
*   AI Agentに仕事の目的を伝える
*   プロジェクトやタスクの背景など、必要な情報を丁寧に教えてオンボーディングする
*   AI Agentの進捗を管理する
*   計画や成果物を評価する
*   作業プロセスを改善するためのPDCAサイクルを回す

この記事では、AAMとして特に重要な「働く環境の整備」と「ワークフローの設計」という2つのノウハウが共有されています。

**1. AI Agentが働く環境の整備**
AI Agentの主な作業場所はGitリポジトリになります。人間はリポジトリ外の情報（デザインツールやチャットなど）から多くのヒントを得ますが、AI Agentはそうではありません。そのため、AI Agentに必要な情報を漏れなく与える「コンテキストの付与」が非常に大切です。これには、リポジトリ内にルールやガイドラインをまとめたファイル（例：`CLAUDE.md`）を置いたり、タスク固有のメモをリポジトリから除外されるフォルダに保存したり、さらに外部の情報源にアクセスできる「MCP Server」と呼ばれる仕組みを充実させたりする方法があります。特に、会社やチーム全体でルールを統一し、AI Agentがどこでも効率的に働けるようにMCP Serverを拡充することが重要だと述べられています。

**2. AI Agentのワークフロー設計**
AI Agentに仕事をさせるための「手順」を設計することもAAMの重要な仕事です。記事では、リポジトリの初期設定から、タスクの立ち上げ、必要な情報の読み込み、実装方針の検討、依存関係の学習、実際のコード実装、テスト、そして作業の振り返りまで、具体的な8つのステップが提案されています。それぞれのステップで、AI Agentに適切な指示（プロンプト）を与えることが求められます。特に、AI Agentに「カスタムコマンド」としてこれらの指示を登録し、決まった手順で実行させることで、効率的に作業が進むようになります。AI Agentが意図しない動きをした場合は、振り返りを通じてプロンプトやルールを改善していくことが大切です。

将来的に、AAMにはAI Agentの能力を最大限に引き出し、開発の生産性を向上させるスキルが求められます。具体的には、複数のAI Agentが同時に作業できる環境を整えたり、新しいAgentでもすぐに仕事に取りかかれるように準備したり、AI Agentが常に必要な情報を得られるようにしたり、開発コストを最適化したりする能力です。また、AI Agentが正しいコードを書き、質の高いレビューができるように、コマンド設計やワークフローの改善、さらにはAIモデル自体の性能を高める「チューニング」に関する知識も重要になると提言されています。

この記事は、AI Agentが開発の中心となる未来において、エンジニアがどのように活躍し、自身のスキルを磨いていくべきか、新しい視点を提供してくれるでしょう。

引用元: https://qiita.com/icoxfog417/items/f15e92f05b14411fd642


- [LiteLLMを使ったLLMの集約 & 簡易的なKey管理 + langfuse添え](https://zenn.dev/vlntr_telco_rd/articles/litellm-key-and-ops)  


LLM（大規模言語モデル）の利用が広がる中で、OpenAIやClaude、Azure OpenAIなど様々なプロバイダーのLLMを使うと、それぞれAPIの形式が異なったり、APIキーの管理が複雑になったりして困ることがあります。この記事では、こうした課題を解決する「LiteLLM」というツールと、その便利な機能について、新人エンジニアの方にも分かりやすく解説します。

**LiteLLMとは？**
LiteLLMは、複数のLLMプロバイダーへのアクセスを「統一された窓口」として提供するオープンソースのソフトウェアです。これを「LiteLLM Proxy Server」として動かすことで、異なるLLMのAPI形式の違いを意識することなく、OpenAIと同じような形式でリクエストを送れるようになります。単にAPIをまとめるだけでなく、誰が、どれくらいの費用で、どのモデルを使っているかといった「利用状況の管理」や、リクエストの「回数制限」なども設定できる、便利な「LLMの司令塔」のような役割を果たします。

**APIキーの管理をシンプルに**
LiteLLMの特に便利な機能の一つがAPIキーの管理です。
1.  **Master Key (マスターキー)**: これはLiteLLM全体を管理するための「何でもできる鍵」です。この鍵があれば、LiteLLMのあらゆる設定を変更したり、新しい利用者向けの鍵（Virtual Key）を発行したりできます。非常に強力なので、厳重に管理する必要があります。
2.  **Virtual Key (バーチャルキー)**: これは、実際にLLMを使うアプリケーションやユーザーに発行する「用途限定の鍵」です。この鍵には、「どのモデルだけを使えるようにするか」「どれくらいの予算（利用量）まで使えるか」「1分間に何回までリクエストを送れるか」といった、細かい利用制限を設定できます。これにより、社内やチームでLLMを使う際に、一人ひとりの利用状況を細かくコントロールし、無駄な利用を防ぎながら安全にLLMを活用できるようになります。

**Langfuse連携で利用状況を「見える化」**
LiteLLMは「Langfuse」というツールと連携できます。これにより、LiteLLMを経由したLLMへのリクエスト（誰が、どのモデルに、どんな内容を送ったかなど）が自動的に記録され、Langfuseの画面で確認できるようになります。これは、LLMの利用状況を把握したり、何か問題が起きたときに原因を調べたりするのに非常に役立ち、「実際どれくらい使っているんだろう？」「ちゃんとリクエスト通ってるかな？」といった疑問を簡単に解消できます。

LiteLLMを使うことで、たくさんのLLMを効率的に管理し、APIキーの運用も安全かつ柔軟に行えるようになります。LLMを活用した開発を進める上で、ぜひ知っておきたい便利なツールです。

引用元: https://zenn.dev/vlntr_telco_rd/articles/litellm-key-and-ops


- [The rise of "context engineering"](https://blog.langchain.com/the-rise-of-context-engineering/)  


LLM（大規模言語モデル）を使ったアプリケーション開発において、「コンテキストエンジニアリング」という考え方が非常に重要になってきています。これは、LLMが任されたタスクを正確にこなせるよう、適切な情報やツールを正しい形式で、かつ動的に提供するシステムを構築する技術を指します。

なぜこのスキルが注目されているのでしょうか？LLMが期待通りに動作しない主な原因は、実はモデル自体の性能不足よりも、モデルに与えられた情報（コンテキスト）や指示、あるいは使えるツールが不十分だったり、伝わりにくい形で提供されたりすることにあります。LLMは人間のように状況を察することはできないため、「ゴミを入れればゴミが出る（Garbage in, garbage out）」という言葉の通り、必要な情報はすべて明確に与える必要があるのです。

「プロンプトエンジニアリング」と何が違うのでしょうか？プロンプトエンジニアリングが「どう尋ねるか、どう指示を出すか」という問いかけ方に焦点を当てるのに対し、コンテキストエンジニアリングは、LLMに与える情報の「内容」や「構造」、さらに外部ツールとの連携を含めた、より広範なシステム設計の側面を指します。つまり、プロンプトエンジニアリングはコンテキストエンジニアリングの一部と考えることができます。

コンテキストエンジニアリングの具体的なアプローチとしては、以下のようなものがあります。
*   **ツール利用**: LLMが外部の情報を検索したり、特定の行動を実行したりできるよう、必要なツールを提供し、そのツールからの返答もLLMが理解しやすいように整える。
*   **記憶の利用**: これまでの会話履歴を要約して提供したり、ユーザーの過去の好みや設定を記憶して、次の対話に活用したりする。
*   **明確な指示**: LLMがどのような振る舞いをすべきか、具体的なルールや手順をプロンプトで詳細に指示する。
*   **情報検索（Retrieval）**: 外部データベースから関連情報を動的に取得し、LLMへの入力に含める。

LangChainが提供する「LangGraph」や、LLMアプリケーションの動作を可視化・デバッグするための「LangSmith」といったツールは、このようなコンテキストエンジニアリングを実践し、改善していく上で非常に役立ちます。これらのツールを使うことで、LLMにどのような情報が、どのような形で提供されているかを詳細に確認し、より効果的なシステムを構築できるようになります。

新人エンジニアの皆さんも、LLMアプリ開発では、単にプロンプトを工夫するだけでなく、「LLMに何を、どうやって伝えるか」という、より広範な「コミュニケーション設計」に目を向けることが、成功への鍵となるでしょう。

引用元: https://blog.langchain.com/the-rise-of-context-engineering/


- [プログラマ不要論が度々話題に上がる一方、正確な要件定義とちゃんとした成果物レビューが求められているが、それってプログラミング能力そのものなのでは？](https://togetter.com/li/2567083)  


「プログラマは不要になる」という声がある一方、記事では「正確な要件定義」や「成果物レビュー」こそがプログラミング能力の本質と論じます。プログラミングとは単にコードを書くことではなく、複雑な問題を論理的に整理し、システムとして具体化する思考力のこと。AIやローコードツールが発展しても、この本質的な「システムを設計し、評価する能力」はエンジニアに不可欠です。新人エンジニアの皆さんも、技術の進化を味方にしつつ、この論理的思考力を磨き続けることが大切です。

引用元: https://togetter.com/li/2567083



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
