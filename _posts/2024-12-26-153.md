---
actor_ids:
  - ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/株式会社ずんだもん技術室AI放送局_podcast_20241226.mp3
audio_file_size: 0
date: 2024-12-26 05:00:00 +0900
description: 'AIやテクノロジーに関する記事を紹介  
2024年生成AIエージェントのおすすめ論文 16選、LLMのモデルマージ手法  データアナリティクスラボ、Devin AIにテストを丸ごと書かかせてCIがパスするまで作業してもらう方法、Amazon Bedrockで「ユーザがアップロードしたドキュメントから回答を得る」方法のまとめ'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20241226
---

## 関連リンク


- [2024年生成AIエージェントのおすすめ論文 16選](https://masamasa59.hatenablog.com/entry/2024-best-papers-on-ai-agents)  


この記事は、AIエージェント研究に一年を費やした筆者が、2024年に発表されたAIエージェント関連の論文の中から、特にビジネスやエンジニア層にとって学びが多いと思われる16本を厳選して紹介しています。論文は、AIエージェントの基礎から応用、評価方法、具体的な構築のポイント、マルチエージェント、人間とのインタラクション、学習方法、そしてメタ認知能力や脱出ゲームへの挑戦といったユニークなテーマまで幅広くカバーしています。各論文の概要とともに、読者が理解を深めるための質問例も提示されており、AIエージェントに関わる技術者にとって、現状を把握し、今後の開発や研究のヒントを得るのに役立つ内容となっています。また、記事の最後に、AIエージェント関連の優れたまとめ記事へのリンクも紹介されており、より深く学びたい読者への配慮もなされています。


引用元: https://masamasa59.hatenablog.com/entry/2024-best-papers-on-ai-agents


- [LLMのモデルマージ手法  データアナリティクスラボ](https://dalab.jp/archives/journal/llm-merge-techniques/)  


この記事では、LLM（大規模言語モデル）のモデルマージ技術について解説しています。モデルマージとは、複数のモデルのパラメータを組み合わせて新しいモデルを作る技術で、計算コストを抑えつつ高性能なモデルを効率的に構築できる可能性があります。

**モデルマージの種類**
- パラメータ空間でのマージ：モデルの各層のパラメータの重みを統合する
- データフロー空間でのマージ：複数のモデルの層を組み替えて新しいモデルを作る

この記事では、パラメータ空間でのマージに焦点を当てています。

**モデルマージの効果**
- モデルスープ：複数の微調整モデルの重みを平均化することで、精度とロバスト性を向上させることができる。
- モデルの重みを平均化することで、損失関数の「平坦解」に近づき、汎化性能が向上する可能性がある。

**モデルマージの具体的な手法**
- Task Arithmetic：微調整後のモデルの重みから微調整前の重みを引いてタスクベクトルを作成し、それらを足し引きすることでタスク能力を付与する。
- TIES：デルタパラメータ（タスクベクトルと同義）の枝刈り（重要度の低いパラメータを削除）と、優位な符号に一致する重みをマージすることでパラメータ干渉を抑制する。
- DARE：デルタパラメータをランダムに枝刈りし、リスケーリングすることで精度を維持する。
- Model Breadcrumbs：デルタパラメータの絶対値の上下を閾値として枝刈りし、外れ値を除去する。
- TALL Mask：タスク間の干渉を考慮し、タスクに重要な重みを残すようにマスクする。
- DELLA：デルタパラメータの重みの大きさに応じてドロップする確率を設定し、枝刈りを行う。
- MetaGPT：モデルマージ後の損失と各モデルの損失の差を最小化するように、最適な按分を求める。
- KnOTS：LoRAで微調整したモデルのマージに特化し、SVD（特異値分解）を用いて共通の基底を抽出し、マージの精度を向上させる。

**その他**
- 進化的モデルマージ：進化アルゴリズムを用いてマージのハイパーパラメータを最適化する。
- MoE Merging：複数のモデルをエキスパートとしてMoE（Mixture of Experts）モデルを構築する。

**実装方法**
- mergekit：様々なマージ手法をサポートするライブラリ。
- 進化的モデルマージの実装例は、記事内で紹介されているリンクを参照。

モデルマージは、ドメイン特化LLMの構築において、コストを抑えつつ高性能なモデルを作るための有効な手段となる可能性があります。


引用元: https://dalab.jp/archives/journal/llm-merge-techniques/


- [Devin AIにテストを丸ごと書かかせてCIがパスするまで作業してもらう方法](https://zenn.dev/ubie_dev/articles/devin-for-test)  


Devinは、ソフトウェア開発を効率化するAIプラットフォームで、特にテストコードの自動生成に優れています。Slackでテスト作成を依頼すると、Devinは指定されたリポジトリにアクセスし、既存のテスト事例を参考にテストコードを生成、GitHubにPRを作成します。CIが失敗した場合は、自動で修正を試みます。さらに、SlackやGitHubのPRコメント、DevinのUIから追加の作業依頼も可能です。Devinは過去のフィードバックを学習し、リポジトリごとに「Knowledge」として保存するため、継続的に利用することで、より効率的な開発が期待できます。ただし、テストの最終チェックはエンジニアが行う必要があり、複雑な作業は事前に事例や指示を明確にした方がスムーズです。Devinはテスト以外にも、リファクタリング、エラーハンドリング、ドキュメント作成など多岐にわたる作業をこなします。月額$500〜の従量課金制で、チームの状況によっては非常にリーズナブルな価格設定と言えるでしょう。


引用元: https://zenn.dev/ubie_dev/articles/devin-for-test


- [Amazon Bedrockで「ユーザがアップロードしたドキュメントから回答を得る」方法のまとめ](https://qiita.com/nasuvitz/items/3a3976bf8095c26373d0)  


この記事では、Amazon Bedrockを使って、ユーザーがアップロードしたドキュメントの内容に基づいて回答を得る方法を解説します。
RAG（Retrieval-Augmented Generation）という技術を使うことで、AIが学習していない情報も扱えるようになります。
特に、ユーザーがドキュメントをアップロードして、その内容に関する質問に答えられる機能は、最近ニーズが高まっています。

Amazon Bedrockでは、以下の2つの方法でこの機能を実現できます。

1. **Anthropic Claude メッセージAPI**:
   -  BedrockのプレイグラウンドでClaudeモデルを選ぶと、ファイルの添付ボタンが現れます。
   -  このAPIを使うと、アップロードしたファイルの内容を基に回答が得られます。
   -  boto3を使う場合、ファイルのバイト列をBase64エンコードする必要はありません。
   -  ただし、ファイルサイズは4.5MB未満である必要があります。
   -  このAPIでのドキュメント読み取り精度は非常に高いです。

2. **Amazon Bedrock Knowledge BaseのRetrieveAndGenerate API**:
   - Knowledge Baseの画面からドキュメントをアップロードして回答を得られます。
   -  このAPIを使う場合は、ファイルのバイト列をBase64エンコードする必要があります。
   -  また、`contentType`にはMIMEタイプを指定する必要があります。
   -  このAPIもドキュメントの読み取り精度が非常に高く、詳細な情報を読み取って回答を生成できます。

どちらのAPIも、事前にナレッジベースを構築したり、S3バケットを使う必要がなく、シンプルに実装できる点が大きなメリットです。
これらの機能は、ChatGPTなどの他のAIチャットサービスでも利用されており、その利便性が注目されています。
これらのAPIを上手く活用することで、チャットボットに組み込みやすく、ユーザーが求める情報に迅速にアクセスできるようになります。


引用元: https://qiita.com/nasuvitz/items/3a3976bf8095c26373d0



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
