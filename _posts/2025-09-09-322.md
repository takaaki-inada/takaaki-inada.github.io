---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20250909.mp3
audio_file_size: 0
date: 2025-09-09 05:00:00 +0900
description: 'Temporal Knowledge Graphで作る！時間変化するナレッジを扱うAI Agentの世界、AIコードレビューがチームの&quot;文脈&quot;を読めるようになるまで 〜 レビューの暗黙知を継承する育成コマンド 〜、Experimenting with local LLMs on macOS、漫画やアニメにはあまり出てこないのに「褐色エジプト猫耳ロリ」のデザインイメージが共有されているのはなぜか→ソシャゲの影響？起源はもっと古い？'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20250909
---

## 関連リンク


- [Temporal Knowledge Graphで作る！時間変化するナレッジを扱うAI Agentの世界](https://tech.layerx.co.jp/entry/tkg-agent)  


AI Agent（AIエージェント）が賢く動くためには、まるで人間が経験や知識を記憶するように、「メモリ」や「ナレッジ」（知識）を持つことが非常に大切です。特に、社内ルールのように時間とともに変化する情報を扱うのは難しい課題でした。この記事では、この課題を解決する「Temporal Knowledge Graph（TKG）」（時間知識グラフ）という最新技術と、その具体的な応用事例について、新人エンジニアにも分かりやすく解説されています。

まず「Knowledge Graph（KG）」（知識グラフ）とは、情報（エンティティ）とその関係性をグラフの形で整理する技術です。これにより、データから複雑な関連性を直感的に理解しやすくなります。最近では、LLM（大規模言語モデル）の進化により、大量の文書から知識グラフを簡単に構築できるようになりました。KGは、単純な事実検索よりも、複数の概念が絡む複雑な推論や深い文脈理解が必要なタスクで特に威力を発揮します。

しかし、従来のKGは時間の変化に対応できません。そこで登場するのが「Temporal Knowledge Graph」（TKG）です。TKGは、情報に「いつ」という時間軸の概念を加えることで、時間とともに変化する情報や動的な変更を扱えるようにした知識グラフです。これにより、AI Agentは新しい情報やユーザーとのやり取りを通じて自律的に学習し、行動を改善していくサイクルを構築できるようになります。

TKGの代表的なフレームワークの一つに「Graphiti」（Zep）があります。Graphitiは、生の入力データ（エピソードサブグラフ）、そこから抽出されたエンティティと関係性（セマンティックエンティティサブグラフ）、さらに高レベルな概念の要約（コミュニティサブグラフ）という3層構造で情報を管理します。LayerXでは、このTKGを活用し、時間変化する申請ルールに対応するAI Agentの概念実証（PoC）を進めています。例えば、自然言語でルール変更を取り込んだり、人間の差し戻しコメントから学習してレビュー精度を高めたり、社内規定の曖昧な部分をAI Agentが自律的に発見・補完するといった、現実世界の課題を解決する機能が検討されています。

TKGを実際に構築する上でのノウハウも共有されています。特に日本語文書からグラフを構築する際は、長い文章を細かく分ける「チャンク化」や、日本語特有の「主語の省略」への対応が重要です。LLMへのプロンプトで、主語を明確にするよう具体的に指示を出す工夫が紹介されています。また、大量のデータを効率的に取り込むための「一括挿入」機能や、テナントごとに情報を分離する「group_id」の活用もポイントです。

構築したTKGやAI Agentの性能を評価することも非常に重要で、グラフの品質指標や、RAG（Retrieval-Augmented Generation）の評価フレームワークである「Ragas」を活用する方法が紹介されています。Temporal Knowledge Graphは、時間変化する動的なナレッジをAI Agentに与えるための強力な基盤であり、今後のサービス開発において競争優位性を生み出す重要な技術として期待されています。

引用元: https://tech.layerx.co.jp/entry/tkg-agent


- [AIコードレビューがチームの"文脈"を読めるようになるまで 〜 レビューの暗黙知を継承する育成コマンド 〜](https://www.m3tech.blog/entry/2025/09/06/110000)  


エムスリーのエンジニアチームでは、開発プロセスにAIコードレビューを導入し、効率とコード品質の向上を目指しています。AIは、変数名のタイポや文法ミス、冗長なコード、セキュリティの脆弱性などを自動で指摘してくれるため、エンジニアはより本質的なロジックや設計といった、考える必要があるレビューに集中できるようになりました。

しかし、AIコードレビューには課題もありました。一般的な指摘は得意ですが、チーム固有の「文脈」や「暗黙知」を理解できない点です。例えば、「プロジェクト全体の設計思想に合っているか」「共通関数を使っているか」「ファイルの置き場所はルールに沿っているか」といった、チームで培われた細かなルールや知見は、AIには判断できませんでした。また、AIに与える指示（プロンプト）の更新も滞りがちで、最新のチームの知見が反映されにくいという問題も抱えていました。

この課題を解決するため、チームは「AIレビュアー育成コマンド」を開発しました。これは、過去に人間が行ったレビューの内容をAIに学習させ、AIのコードレビュー用プロンプトを自動で更新する仕組みです。このコマンドはClaude Codeで利用でき、以下のステップで動作します。

1.  **マージリクエストの取得**: GitLabから、最近マージされたコード変更（マージリクエスト）の履歴を取得します。
2.  **コメントとコードの抽出**: 各マージリクエストから、レビューコメントと、そのコメントが指摘しているコードをセットで取り出します。
3.  **レビュー観点の要約**: AI自身が、取り出したコメント群から「このプロジェクトで重要なレビュー観点（チェックポイント）」を要約し、Markdownファイルとして出力します。
4.  **プロンプトの更新**: 生成されたMarkdownファイルの内容を元に、既存のAIコードレビュー用プロンプトに新しい知見を追記・更新します。

実装の工夫としては、AIが一度に多くの情報を処理しきれない問題を避けるため、特定の役割を持たせた「サブエージェント」を活用しています。また、AIが勝手に「ありそうなレビュー観点」を想像してしまうことを防ぐため、「実際に誰がどんなコメントをしたか」をAIに明示的に伝えることで、より実践的なレビュー観点を抽出できるように工夫されています。

このコマンドの導入により、プロンプトの更新作業が非常に楽になり、チームの知見を継続的にAIに反映できるようになりました。今後は、「LGTM（了解）」のような不要なコメントの抽出を改善し、最終的にはコマンドの自動実行によって、AIが自律的にチームの文脈を学び、自己成長する「AIレビュアー」を実現することを目指しています。これにより、レビューにかかるコストを削減し、チーム全体の開発体験を向上させることが目標です。

引用元: https://www.m3tech.blog/entry/2025/09/06/110000


- [Experimenting with local LLMs on macOS](https://blog.6nok.org/experimenting-with-local-llms-on-macos/)  


この記事は、Macユーザーが自分のパソコンでLLM（大規模言語モデル）を動かす方法と、その魅力や注意点について解説しています。著者はLLMに少し懐疑的ながらも、新しい技術に触れる面白さからローカル環境での利用を推奨しています。

なぜローカルでLLMを動かすのでしょうか？クラウドサービスでも使えますが、ローカル環境にはいくつかの大きなメリットがあります。
1.  **技術的な興味と実験**: 自分のPCでLLMが動くのは、まるで魔法のようです。エンジニアとして、この技術がどう動くのかを実際に試すのは、とても興味深い体験になります。
2.  **データプライバシーの確保**: ChatGPTなどのサービスでは、入力したデータが企業に利用される可能性があります。ローカルで動かせば、機密情報が外部に漏れる心配がなく、安心して使えます。
3.  **倫理的な懸念**: AI企業によっては、データ利用や環境負荷、著作権侵害など、倫理的に問題視される行為があると著者は指摘します。ローカルでオープンなモデルを使えば、そうした懸念を避けられます。

macOSでローカルLLMを動かす方法は主に二つ紹介されています。
*   **llama.cpp**: オープンソースのプロジェクトで、様々なプラットフォームに対応しており、Web UIも備えています。自分でコマンドを叩いて、細かく設定をいじりたい人向けです。
*   **LM Studio**: こちらはGUIが非常に使いやすく、モデルの検索、ダウンロード、チャット管理など、ほとんどの操作を直感的に行えます。初心者のエンジニアの方には特におすすめです。

モデルを選ぶ際には、いくつかのポイントがあります。
*   **モデルサイズとメモリ**: LLMは大量のメモリ（RAM）を消費します。お持ちのMacのRAM容量（例えば16GBなら12GB以下のモデルが目安）に合わせてモデルを選ぶことが重要です。大きすぎるとPCが動かなくなってしまうこともあります。
*   **ランタイム**: モデルの種類によって、`llama.cpp`が使うGGUF形式か、Appleが開発したMLXランタイムが使うMLX形式かを選びます。
*   **量子化（Quantization）**: モデルの精度を少し下げてファイルサイズを小さくする技術です。通常は「Q4」などのデフォルト設定で十分です。
*   **機能**: 画像を読み解く「Visionモデル」、回答を生成する前に「考える」ステップを挟む「Reasoning（推論）」機能、外部ツールと連携する「Tool use」機能など、モデルによって得意なことが異なります。Tool useは非常に強力ですが、セキュリティには注意が必要です。

ローカルLLMは、最新のクラウドLLMほどの性能や速度はないかもしれませんが、その仕組みを理解し、活用するための良い学習の場となります。会話が長くなるとLLMが前の内容を忘れてしまうことがあるので、要約を頼むなどして、大事な情報を覚えてもらう工夫も紹介されています。

自分のMacで「パーソナルなAIアシスタント」を動かす楽しさをぜひ体験してみてください。

引用元: https://blog.6nok.org/experimenting-with-local-llms-on-macos/


- [漫画やアニメにはあまり出てこないのに「褐色エジプト猫耳ロリ」のデザインイメージが共有されているのはなぜか→ソシャゲの影響？起源はもっと古い？](https://togetter.com/li/2600143)  


「褐色エジプト猫耳ロリ」という、特定の作品に由来しない共通のキャラクターイメージがなぜ広まっているのか、その起源が話題になっています。多くの人がスマホゲーム「パズドラ」の「バステト」を有力な元ネタとして挙げました。ソシャゲは日々プレイされるため、アニメや漫画よりもイメージ形成に大きな影響を与える可能性や、エジプト文化への関心など、複合的な要因で共通イメージが作られている面白さが議論されました。

引用元: https://togetter.com/li/2600143



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
