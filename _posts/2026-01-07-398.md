---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20260107.mp3
audio_file_size: 0
date: 2026-01-07 05:00:00 +0900
description: 'Claude Code、Google開発チームの1年分を1時間で実現──AI支援コーディングの転換点、Inside the NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer、Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next、親族の中で私だけがオタクで中身が子どもだなって思ってたけど、子どもの価値観が唯一わかるので親戚キッズには大人気だった「的確に褒めることができる」'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20260107
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/AvtU2NwS-Kw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Claude Code、Google開発チームの1年分を1時間で実現──AI支援コーディングの転換点](https://innovatopia.jp/ai/ai-news/76604/)  


Googleのプリンシパルエンジニアであるヤナ・ドーガン氏が、Anthropic社のエージェント型コーディングツール「Claude Code」を用い、自身のチームが1年かけて開発してきたシステムに匹敵するプロトタイプをわずか1時間で構築したと報告し、大きな注目を集めています。

**1. 衝撃の報告とその背景**
ドーガン氏が作成したのは、複数のAIエージェントを効率的に管理・調整する「分散型エージェントオーケストレーション」という複雑なシステムの概念実証（PoC）です。Googleチームが1年間、様々なアプローチを試行錯誤し議論を重ねてきた内容を、Claude Codeは提示された問題定義からわずか1時間で形にしました。
ただし、これは「本番環境用（プロダクショングレード）」ではなく、あくまで「動作モデル（トイバージョン）」であると補足されています。しかし、専門知識を持つ人間がAIを活用することで、自身の知見をこれほどの短時間で再構築・具現化できるようになった事実は、開発プロセスの劇的な変化を象徴しています。

**2. AIコーディングの急速な進化曲線**
記事では、ここ数年のAI支援プログラミングの進化が以下の通りまとめられています。
*   2022年：コードの「行」単位の補完
*   2023年：コードの「セクション（ブロック）」全体の処理
*   2024年：複数ファイルにまたがる作業、小規模アプリの構築
*   2025年：コードベース全体を理解した作成・再構築
かつては5年先と考えられていたレベルに既に到達しており、開発効率の向上は専門家の想像を遥かに超えるスピードで進んでいます。

**3. エンジニアのための実践的な活用ヒント**
Claude Codeを最大限に活用するための重要な戦略も示されています。
*   **フィードバックループの構築**：AIに自身の作業を検証する方法（テストコードなど）を与えることで、出力の品質が2〜3倍向上します。
*   **「プランモード」の活用**：いきなり実装させるのではなく、まず対話を通じて計画を十分に固めてから実行に移すことが成功の鍵です。
*   **並列実行と外部ツール連携**：複数のタスクを並列してAIに実行させたり、エラーログ監視ツール等と連携させたりすることで、開発サイクルを加速させます。

**4. これからのエンジニアの役割**
現在、Anthropic社内ではコードの約90%がAIによって書かれているといいます。エンジニアの役割は「自らコードを書く人」から、戦略的思考や複雑な問題解決、そして「AIシステムを管理・監督する人」へとシフトしています。
新人エンジニアにとっても、AIを単なる補助ツールとしてではなく、自身の能力を拡張するパートナーとして使いこなし、高次の設計や検証に注力するスキルが今後ますます重要になるでしょう。

引用元: https://innovatopia.jp/ai/ai-news/76604/


- [Inside the NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer](https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/)  


NVIDIAは、次世代AIプラットフォーム「Rubin（ルービン）」の詳細を発表しました。本プラットフォームは、AIが単なるモデルの推論を超え、常に動作し続け知能を生み出す「AIファクトリー（AI工場）」へと進化した現状に対応するために設計されました。

Rubinの最大の特徴は「エクストリーム・コーデザイン（究極の共同設計）」という思想です。これは、GPU単体の性能向上に留まらず、CPU、ネットワーク、ソフトウェア、冷却システムまでを一つの計算システムとして統合的に設計する手法です。これにより、データセンターそのものを一つの計算ユニットとして扱います。

中心となるのは、以下の6つの新型チップです。
1. **Vera CPU**: カスタム設計の「Olympusコア」を搭載。データ転送のボトルネックを解消し、GPUの稼働率を最大化する「データエンジン」として機能します。
2. **Rubin GPU**: HBM4メモリを搭載し、最新のTransformer Engineにより推論性能を飛躍させた「実行エンジン」です。
3. **NVLink 6 スイッチ**: GPU間通信を3.6TB/sに倍増させ、ラック内のGPUを一つの巨大な計算機として繋ぎます。
4. **ConnectX-9 SuperNIC**: ラック外との超高速通信を実現します。
5. **BlueField-4 DPU**: ネットワークやセキュリティなどのインフラ処理を専門に引き受け、計算リソースをAI処理に集中させます。
6. **Spectrum-6 イーサネットスイッチ**: 数万基規模のGPUを連携させる広域ネットワーク基盤を提供します。

これらを統合したラックシステム「Vera Rubin NVL72」は、前世代のBlackwellと比較して、推論スループットが最大10倍、1トークンあたりのコストが1/10という圧倒的な効率を実現します。

新人エンジニアにとって心強いのは、この進化が「CUDA」との完全な後方互換性を維持している点です。開発者は既存のコード資産を活かしつつ、ハードウェアの進化による恩恵を享受できます。また、液冷システムや電力平準化（Power Smoothing）技術の導入により、運用の安定性と環境効率も高められています。

Rubinは、10兆パラメータ規模の巨大なLLM（MoE：混合エキスパートモデル）の学習を現実的なものにし、高度な推論をリアルタイムで提供するための、次世代のAI開発における標準インフラとなるでしょう。

引用元: https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/


- [Introducing NVIDIA BlueField-4-Powered Inference Context Memory Storage Platform for the Next](https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/)  


AIエージェントの普及に伴い、数百万トークンに及ぶ「長いコンテキスト（文脈）」を扱う必要性が増しています。これに伴い、推論時の履歴情報を保持する「KVキャッシュ」の管理が、インフラエンジニアにとって大きな課題となっています。従来のメモリ階層（GPUメモリから一般的なストレージまで）では、容量不足や転送速度の遅延、過大な消費電力がボトルネックとなり、高価なGPUの性能を十分に引き出せなくなっています。

この課題を解決するためにNVIDIAが発表したのが、次世代「Rubinプラットフォーム」の一部となる「Inference Context Memory Storage (ICMS)」です。これは、推論コンテキストに特化した新しいストレージ・インフラです。

**1. 「G3.5」という新しいメモリ層の追加**
これまでメモリ階層は、高速だが小容量なGPUメモリ（G1）から、低速だが大容量な共有ストレージ（G4）までで構成されていました。ICMSは、その中間を埋める「G3.5」という新しい層を確立します。これは、BlueField-4データプロセッサ（DPU）を活用した、イーサネット接続のフラッシュストレージ層です。

**2. 圧倒的なパフォーマンスと効率性**
ICMSは、従来の汎用ストレージと比較して、秒間トークン数（TPS）を最大5倍に向上させ、電力効率も5倍に改善します。これは、KVキャッシュを「エンタープライズ向けの永続データ」ではなく、「再計算可能で一時的なAI専用データ」として最適に扱うことで、冗長なデータ保護処理などを省き、徹底的に高速化・低消費電力化した結果です。

**3. 基盤技術：BlueField-4とSpectrum-X**
ICMSの心臓部には、800Gb/sの接続性と64コアのGrace CPUを搭載した「NVIDIA BlueField-4 DPU」が採用されています。また、ネットワークには「NVIDIA Spectrum-X Ethernet」を使用し、低レイテンシで広帯域なRDMA接続を実現しています。これにより、ポッド内の複数のGPU間でKVキャッシュを高速に共有・再利用することが可能になります。

**エンジニアへのメリット：**
新人エンジニアにとっても重要なポイントは、この技術が「推論コストの削減」と「スケーラビリティ」に直結することです。KVキャッシュの再計算（履歴の読み直し）を減らし、コンテキストを効率的にステージングすることで、同じ電力・コストでより多くのエージェントを動かすことが可能になります。これは、大規模なAIシステムを構築・運用する上で、今後のスタンダードとなるインフラ進化と言えるでしょう。

引用元: https://developer.nvidia.com/blog/introducing-nvidia-bluefield-4-powered-inference-context-memory-storage-platform-for-the-next-frontier-of-ai/


- [親族の中で私だけがオタクで中身が子どもだなって思ってたけど、子どもの価値観が唯一わかるので親戚キッズには大人気だった「的確に褒めることができる」](https://togetter.com/li/2648385)  


親族の中で「自分はオタクで中身が子供のまま」と感じていた投稿者が、お正月に子供たちから絶大な人気を博したエピソードです。シールのレアリティを正確に理解して「的確に褒める」姿勢が、子供たちの自尊心を刺激し深い信頼を築いています。エンジニアの探究心や専門知識が、意外な場面で世代を超えたコミュニケーションの武器になることを教えてくれる、ジュニアエンジニアにも親しみやすい心温まる読み物です。

引用元: https://togetter.com/li/2648385



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
