---
actor_ids:
  - ずんだもん
audio_file_path: /audio/株式会社ずんだもん技術室AI放送局_podcast_20251224.mp3
audio_file_size: 0
date: 2025-12-24 05:00:00 +0900
description: 'Googles year in review: 8 areas with research breakthroughs in 2025、GPT-1 から GPT-5.2 まで: LLM の特殊トークン徹底解説【2025年12月最新】、NVFP4: 4ビットの浮動小数点でLLMを学習する仕組み'
duration: "00:00"
layout: article
title: 株式会社ずんだもん技術室AI放送局 podcast 20251224
---

## youtube版(スライド付き)

<div class="article-video"><iframe src="https://www.youtube.com/embed/LWtxoNUOlcU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>


## 関連リンク


- [Googles year in review: 8 areas with research breakthroughs in 2025](https://deepmind.google/blog/googles-year-in-review-8-areas-with-research-breakthroughs-in-2025/)  


2025年は、AIが単なる「便利な道具」から、自ら考え、行動し、人間と共に探求する「実用的なエージェント（Utility）」へと進化した、エンジニアにとって非常に刺激的な1年となりました。Googleが発表した8つの主要分野におけるブレイクスルーを、若手エンジニア向けに要約して紹介します。

**1. 基盤モデルの飛躍：Gemini 3の登場**
2025年の最大の成果は、推論能力と効率性が劇的に向上した「Gemini 3」シリーズのリリースです。最上位の「Gemini 3 Pro」は、人間のような深い思考をテストする複雑なベンチマークで最高スコアを記録しました。特筆すべきは「Gemini 3 Flash」で、前世代のProモデルを凌駕する性能を持ちながら、圧倒的な低遅延と低コストを実現しています。「次世代のFlashは前世代のProを超える」という進化のサイクルが確立されました。

**2. 開発体験の変革：エージェント型AIの普及**
開発者向けのツールは、コード補完を助ける段階から、開発者と協力してシステムを構築する「エージェント型」へと進化しました。新たな開発支援システム「Google Antigravity」の登場は、AIが自律的にタスクを遂行する新しいソフトウェア開発時代の幕開けを象徴しています。

**3. 科学と数学におけるパートナーとしてのAI**
AIが科学者の「共同研究者」として定着しました。タンパク質構造予測の「AlphaFold」は5周年を迎え、300万人以上の研究者に活用されています。また、Geminiに搭載された「Deep Think」機能は、国際数学オリンピックや競技プログラミング（ICPC）でゴールドメダル級の成績を収め、高度な抽象的推論が可能であることを証明しました。

**4. コンピューティングとハードウェアの進化**
AIモデルを支えるインフラも進化しています。AIチップの設計自体をAI（AlphaChip）が行う手法により、推論に特化した新型TPU「Ironwood」が開発されました。また、量子コンピューティング分野ではGoogleの研究者がノーベル物理学賞を受賞するなど、実用化に向けた理論と技術の両面で大きな前進がありました。

**5. オープンモデル「Gemma 3」と責任ある開発**
Googleは、単一のGPUやTPUでも動作する軽量なオープンモデル「Gemma 3」を公開し、誰もが最先端技術に触れられる環境を整えました。同時に、AIが生成した動画や画像の検証機能の強化など、安全性（AI Safety）への取り組みも最優先で行われています。

2025年の動向は、AIを「いかに使いこなすか」だけでなく、AIを「いかにエージェントとしてシステムに組み込むか」が重要になったことを示しています。新人エンジニアの皆さんは、これらの新しいAPIやエージェントの概念をぜひ積極的にキャッチアップしてみてください。

引用元: https://deepmind.google/blog/googles-year-in-review-8-areas-with-research-breakthroughs-in-2025/


- [GPT-1 から GPT-5.2 まで: LLM の特殊トークン徹底解説【2025年12月最新】](https://blog.asial.co.jp/6429/)  


本記事は、大規模言語モデル（LLM）の内部で重要な役割を果たす「特殊トークン」の変遷について、黎明期のGPT-1から最新のGPT-5.2までを技術的に解説したドキュメントです。新人エンジニアの方に向けて、LLMがどのように進化し、どのような仕組みで私たちの意図を汲み取っているのかを解き明かします。

### 1. 特殊トークンとは何か
LLMはテキストを「トークン」という単位に分割して処理しますが、通常の単語以外に「ここから会話開始」「ここで思考を終了」といった**制御情報をモデルに伝えるための予約されたトークン**が存在します。これが特殊トークンです。

### 2. トークナイズ方式の基礎知識
モデルが文字を認識する仕組みには、以下の主要な方式があります。
- **WordPiece**: BERT等で採用。単語をサブワードに分解する。
- **BPE (Byte Pair Encoding)**: GPT系で採用。出現頻度の高い文字の組み合わせを統合する。
- **バイトレベル BPE**: GPT-2以降で採用。UTF-8のバイト単位で処理するため、未知語（語彙にない文字）が発生しません。

### 3. 歴史に見る特殊トークンの進化
*   **黎明期（GPT-1, BERT）**: 文末を示す`<|endoftext|>`や、BERTの穴埋め問題用`[MASK]`など、文章構造を示すシンプルなものでした。
*   **チャットの登場（GPT-3.5）**: 単なる「続きの予測」から「会話」へ進化。OpenAIは**ChatML**を導入し、`<|im_start|>user`のように「誰の発言か」を区別する特殊トークンを定義しました。
*   **マルチモーダル（GPT-4, LLaVA）**: 画像情報の入力を示す`<image>`トークンが登場。画像データを576個程度の連続した「画像トークン」として扱い、テキストと同じ次元で処理します。
*   **Reasoning（思考）モデル（o1, DeepSeek-R1, GPT-5）**: 2024年後半からのトレンドです。回答を出す前に「じっくり考える」プロセスを制御します。
    *   **DeepSeek-R1**: `<think>`と`</think>`で思考プロセスを囲む明快な設計。
    *   **gpt-oss / GPT-5**: 「チャンネル」という概念を導入。`analysis`（思考用）と`final`（回答用）を特殊トークンで切り替えています。

### 4. 最新のGPT-5.2と今後の展望
最新のGPT-5.2では、思考の途中で「今すぐ回答する」ボタンが実装されました。これは技術的に、特殊トークンを強制挿入して思考モードを終了させる制御を行っていると推測されます。今後は「感情トークン」や「エージェント間通信用トークン」など、より高度な概念を伝えるための特殊トークンが追加され、LLMの表現力はさらに向上していくでしょう。

### 新人エンジニアへのメッセージ
LLMの進化は「いかに人間と同じようなコンテキスト（文脈）をモデルに理解させるか」の歴史でもあります。特殊トークンを学ぶことは、一見魔法のように見えるAIの「裏側の制御」を理解する第一歩です。今後新しいモデルが登場した際は、ぜひその「トークナイザ（語彙リスト）」を覗いてみてください。そこに次世代のAIのヒントが隠されています。

引用元: https://blog.asial.co.jp/6429/


- [NVFP4: 4ビットの浮動小数点でLLMを学習する仕組み](https://www.m3tech.blog/entry/4bit_float_llm)  


近年の大規模言語モデル（LLM）の学習には、天文学的な計算リソースとコストが必要です。この課題に対し、計算に用いる数値の精度を下げることで、ハードウェアの性能を最大限に引き出す手法が注目されています。本記事では、NVIDIAの最新アーキテクチャ「Blackwell」で導入された4ビット浮動小数点「NVFP4」の仕組みと、その優位性について解説しています。

■数値精度と演算速度のトレードオフ
GPUの演算性能は、扱う数値の精度（ビット数）を下げるほど向上します。例えばNVIDIAのB200 GPUでは、FP16に比べてFP4（4ビット）を用いることで、理論上4倍の演算速度が得られます。計算コストを1/4に抑えられる可能性を秘めていますが、FP4はわずか「16通り」の値しか表現できないため、単純に適用すると計算誤差が大きすぎて学習が破綻するという大きな課題がありました。

■精度を維持するための工夫：スケーリング
表現幅の狭いFP4で精度を保つため、以下のステップで技術が進展してきました。

1. スケーリングの導入
量子化したい数値をFP4の最大値に合わせて拡大（スケーリング）し、計算後に元に戻す手法です。しかし、LLMのパラメータには極端に大きな「外れ値」が含まれることがあり、一つの大きな値に合わせると他の微小な値がすべて「0」に丸められてしまう欠点がありました。

2. ブロック単位スケーリング（Block-wise Scaling）
数値を小さな「ブロック」に分割し、ブロックごとにスケーリング定数を計算する手法です。これにより、外れ値の影響を特定のブロック内に限定でき、全体の精度を維持しやすくなります。LLMは一部の外れ値が重要な役割を果たすため、この手法は非常に効果的です。

■NVFP4の革新性
Blackwellで採用された「NVFP4」は、これをさらに洗練させた「2段階スケーリング」を採用しています。
・テンソル全体の基準（FP32）
・16要素ごとの微調整（FP8）
これらを掛け合わせることで、メモリへの負荷を最小限に抑えつつ、4ビットとは思えない高い精度で数値を表現します。

■今後の展望
シミュレーション結果からも、NVFP4は従来の量子化手法より大幅に誤差が少ないことが示されています。2025年に入り、FP4を用いた事前学習の事例も報告され始めており、今後は4ビットネイティブな学習が主流となることで、さらなるモデルの大規模化や学習の高速化が進むと期待されます。低精度演算は現代のAIインフラを支える核となる技術であり、エンジニアにとって必須の知識と言えるでしょう。

引用元: https://www.m3tech.blog/entry/4bit_float_llm



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

（株式会社ずんだもんは架空の登場組織です）
