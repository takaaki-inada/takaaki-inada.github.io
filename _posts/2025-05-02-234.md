---
actor_ids:
  - お嬢様ずんだもん
audio_file_path: https://storage.googleapis.com/podcast-zund-arm-on-tech/audio/私立ずんだもん女学園放送部_podcast_20250502.mp3
audio_file_size: 0
date: 2025-05-02 05:00:00 +0900
description: 'ClineとDDDと私、AIエージェントを使って実際にアプリ開発→リリースした経験・知見を共有する、Gemini 2.5 Proと取り組んだデータ分析のリアルな道のり、WebGPU対応のThree.jsのはじめ方'
duration: "00:00"
layout: article
title: 私立ずんだもん女学園放送部 podcast 20250502
image_url: https://zund-arm-on.com/images/ojousama_zundamon_square.jpg
thumbnail_url: https://zund-arm-on.com/images/ojousama_zundamon_thumbnail.jpg
---

## 関連リンク


- [ClineとDDDと私](https://tech.codmon.com/entry/2025/05/01/132700)  


この記事は、「AIちょっと苦手おじさん」だった筆者が、AIエージェント「Cline」を使い始めた経験と、開発現場での具体的な活用方法について紹介しています。特に、DDD（ドメイン駆動設計）などの考え方を取り入れた保守性の高いコードベースが、AI活用にいかに重要であるかを強調しています。

筆者は、VSCode拡張機能のClineを、GitHub Copilot経由でClaude 3.5 Sonnetモデルと組み合わせて使用しています。最初はタスクの指示の粒度や効率的な進め方に悩みましたが、試行錯誤の結果、効果的な使い方が見えてきました。

AIに開発タスクを任せる上で、なぜ保守性の高いコードが重要なのでしょうか。それは、AIが正確なアウトプットを出すために必要な「コンテキスト情報」を小さく抑えることができるからです。複雑な全体像を知らなくても、特定の小さな部分だけを理解すれば作業できるようなコード（「コンテキストの局所化」）は、AIにとっても扱いやすいのです。DDDやクリーンアーキテクチャは、関心事を分離し、このコンテキストを局所化するのに役立ちます。

また、AIは自然言語で学習しているため、クラス名やメソッド名から振る舞いが予想しやすい、いわゆる「自然言語としての可読性が高い」コードは、AIにとっても理解しやすく、期待通りのコードを生成する可能性が高まります。現時点では、「AIフレンドリーなコード」は「人間が読みやすいコード」とほぼ同じだと言えるでしょう。

具体的なタスク分担の例として、新規機能開発におけるバックエンド（SpringBoot/Kotlin）とフロントエンド（React/TypeScript）でのClineの活用が紹介されています。

バックエンドでは、
- UseCase（アプリケーションロジック）のユニットテスト実装
- Controller（UI層とUseCaseの連携）の実装
- Repository（ドメイン層とインフラ層の連携）の実装
といった、定型的な作業やモック設定が面倒な部分をClineに任せています。特に、DDDにおけるレイヤードアーキテクチャによって関心事が分離されていることが、AIに任せやすいタスクの切り出しに繋がっています。

フロントエンドでは、
- デザインシステムに基づいたコンポーネントライブラリの実装
- アプリケーション固有の個別コンポーネント（API連携やフォーム処理など）の実装
などに活用しています。React/TypeScriptはVSCodeとの連携やAIモデルの学習量の多さから、より良いコード生成が期待できるようです。

Clineを効果的に使うためのTIPSとして、以下の点が挙げられています。
- 参考にしたい既存コードをVSCodeで開いておく（AIが参照しやすくなる）
- Plan（計画）段階でAIの応答を確認し、指示を調整してからAct（実行）する
- `.clinerules`ファイルにプロジェクト共通のルールを記述しておく
- Clineとのやり取りを記録・共有する（振り返りやナレッジ共有のため）
- Clineに「キャラ付け」して楽しく使う

まとめとして、DDDのようなレイヤードアーキテクチャによる関心事の分離は、AIが必要とするコンテキストを減らし、効率的なタスク分担を可能にすることが筆者の経験から分かったそうです。AIの進化にも期待しつつ、今後も人間が理解しやすい「ヒューマンリーダブルなコードベース」を維持していくことの重要性を改めて認識しています。新人エンジニアの皆さんも、こういったAIツールを使いこなし、開発をより効率的で楽しいものにしていきましょう。

引用元: https://tech.codmon.com/entry/2025/05/01/132700


- [AIエージェントを使って実際にアプリ開発→リリースした経験・知見を共有する](https://qiita.com/koher/items/503cd5a9811ab5683a85)  


この記事では、AIエージェント「Claude Code」を使ってiOSアプリ「電光石火」を実際に開発・リリースした経験から得られた知見が共有されています。AIによるコーディングツールは増えていますが、プロダクトとして完成させた例はまだ少ないため、実践的な情報として参考になります。

著者が感じたAIエージェントの最大のメリットは、開発速度の向上です。体感で通常の3〜5倍速く開発できたとのこと。簡単な指示で動くコードをすぐに生成できるため、ゼロから考えるよりも、まずAIにたたき台となるコードを書かせ、それをベースに作業を進めるのが非常に効率的です。AIが間違ったコードを書くこともありますが、生成コストが低いため、失敗しても大きな問題にはなりません。

一方で、AIエージェントは複雑な構造設計や高度な「共通化・抽象化」がまだ苦手です。後々のメンテナンス性を考えると、全体的な設計や、異なる要素をうまくまとめて扱う仕組み作りは人間が行うべき部分です。しかし、最初にAIにいくつかコードを書かせて具体例を作ってから人間が設計を考えると、よりスムーズに進められます。一度設計が固まれば、AIはその方針に沿ったコードを書くのが得意になります。

AIエージェントと「テストファースト」開発は相性が良い方法です。AIはテストコードを書くのが得意で、テスト結果をフィードバックとして自身のコードを修正し、学習していくことができます。これにより、単なるチャットでコードを生成させるよりも賢く振る舞うように感じられるそうです。ただし、AIがテストコードを都合よく変えてしまうこともあるため、テストの内容は人間が確認することが大切です。

また、使用するプログラミング言語の特性もAIエージェントの効率に影響を与える可能性があります。Swiftのような静的型付け言語は、コンパイラからの明確なエラーフィードバックが多いため、AIが問題を自己解決しやすいと考えられます。

AIエージェントを使う際は、「単なるツール」ではなく「チームメンバー」として捉えることが重要です。ジュニアエンジニアに接するように、AIのコードに完璧を求めすぎず、ある程度任せることで全体の速度が上がります。ただし、セキュリティやパフォーマンスに関わる部分、あるいはデータの構造など、プロダクトの基盤となる重要な部分は、人間がしっかりと設計・レビューする必要があります。

AIエージェントの真価を引き出すには、チームのコミュニケーションやプロダクトの仕様自体も考慮に入れるべきです。AIはコーディングが速い分、人間側の指示待ち時間や、AIが苦手な仕様に時間をかけることがボトルネックになり得ます。AIが作りやすいシンプルな仕様にしたり、指示を出す人が素早く判断できるような体制がAI活用には有利です。

AIエージェントは、これからの開発スタイルを大きく変える可能性を持っています。その能力を最大限に活かすためには、AIの得意・不得意を理解し、試行錯誤しながら、自分の開発に最適なAIとの連携方法を見つけていくことが重要です。

引用元: https://qiita.com/koher/items/503cd5a9811ab5683a85


- [Gemini 2.5 Proと取り組んだデータ分析のリアルな道のり](https://nealle-dev.hatenablog.com/entry/2025/05/01/102722)  


この記事では、エンジニアがGemini 2.5 Proを強力な「副操縦士」として活用し、自由記述テキストのマルチラベル分類タスクに挑んだリアルな体験談が紹介されています。

データ分析の目的は、大量のテキストデータに含まれる要望を自動で識別し、複数のタグ（ラベル）を付けるモデルを、業務で使える精度で構築することでした。

プロジェクトは、まずアプローチの模索から始まりました。最初は簡単な教師なし学習を試みましたが、期待する結果が得られませんでした。そこでGeminiに相談したところ、「特定のラベルを正確に付けたいなら教師あり学習が適している」と、その理由とともに代替案が提示されました。このように、**うまくいかなかった結果をAIにフィードバックすることで、次の一手のヒントを得られる**ことが学びとなりました。

次に、分析に必要な教師データの品質を確認しました。データの欠損やラベルの種類、表記揺れなどをチェックする定型的なコード（Python/pandas）は、Geminiに具体的な指示を与えることで瞬時に生成してもらえました。これにより、**コードを手書きしたり調べたりする時間を削減し、データの「中身」の確認に集中できた**といいます。

データが整い、いよいよモデル構築です。テキスト前処理にTF-IDF、分類器にロジスティック回帰を使ったマルチラベル分類モデルの基本的なコードをGeminiに依頼すると、必要な処理の流れを含むパイプラインコードがすぐに入手できました。**標準的な機械学習モデル構築の土台となるコード（ボイラープレートコード）の生成**も、Geminiを使えば効率的に行えます。

しかし、最初のモデルの精度は実用には遠く、特に少数派ラベルの予測精度が低いという課題が見つかりました。ここからが、**Geminiが壁打ち相手として真価を発揮**するフェーズでした。評価結果をAIに見せて課題を投げかけると、Geminiは考えられる原因（日本語の前処理不足、データ数の偏りなど）を分析し、具体的な改善策（形態素解析の導入、不均衡データ対策としての`class_weight`オプション利用、別のモデル提案など）を複数提示してくれました。**なぜその対策が有効なのかという根拠も示してくれる**ため、納得感を持って改善を試みることができ、効率的にモデルの精度を高めることができました。

モデルの精度が向上した後も、実際の運用で起こりうる問題（例：予測結果が空になる、特定のラベルが異常に多く予測される）への対応が必要です。**予期せぬモデルの挙動に対する原因分析や、それを解決するためのビジネスルールをコードに落とし込む作業**でもGeminiは役立ちました。

最終的に、大量データへの予測結果を集計・可視化するレポーティング作業も、Geminiにコード生成を依頼することでスムーズに完了しました。

この経験を通じて、著者は生成AIがデータ分析作業の多くの「手足」となる部分を効率化・代行できるだけでなく、**分析を進める上で行き詰まった際の相談相手として、原因分析や具体的な解決策の提案においても非常に有用である**ことを強く実感したそうです。AIの回答を鵜呑みにせず適切に評価することは必要ですが、生成AIを上手に活用することで、データ分析のハードルを下げ、**組織全体のデータ活用能力向上にも繋がる可能性**がある、と記事は締めくくられています。

新人エンジニアの皆さんにとって、データ分析プロジェクトの具体的な進め方や、生成AIをどのように活用すれば効率的に作業を進め、課題を乗り越えられるかのヒントがたくさん詰まった記事です。ぜひ、皆さんの日々の業務でも生成AIを「副操縦士」として活用することを試してみてください。

引用元: https://nealle-dev.hatenablog.com/entry/2025/05/01/102722


- [WebGPU対応のThree.jsのはじめ方](https://ics.media/entry/250501/)  


この記事は、ウェブ上で次世代のグラフィックスAPIであるWebGPUと、3D表現ライブラリとして広く使われているThree.jsの連携について、新人エンジニアにも分かりやすく解説しています。

WebGPUは、従来のWebGLよりも低レベルでGPU（グラフィックス処理を行う部品）に効率的にアクセスできる新しい技術です。Three.jsは、難しいGPUの操作を抽象化して、JavaScriptで手軽に3Dコンテンツを作成できるライブラリです。

Three.jsでは現在、WebGPUに対応するための「`WebGPURenderer`」という新しいレンダラー（描画を行う機能）の開発が進められています。これはまだ開発途中（WIP: Work in Progress）ですが、既存の`WebGLRenderer`とほぼ同じ使い方で、WebGPUのメリットを活かせるように設計されています。Three.jsを使うことで、WebGPUの低レベルな部分を知らなくても、その高いパフォーマンスや将来性を活かせる可能性があります。

Three.jsでWebGPUを導入するのは比較的簡単です。従来の`WebGLRenderer`を使っていた場合、Three.jsのWebGPU対応ビルドを読み込み、レンダラーを`WebGPURenderer`に切り替えて、初期化時に`await renderer.init()`を呼び出すのが主な変更点です。多くの既存コンテンツは、このレンダラーの切り替えだけで動作すると筆者は述べています。ただし、一部の特殊な表現（粒子や線）や、自分でシェーダー（描画の計算を行うプログラム）を書いている場合は修正が必要になることがあります。

現在のWebGPUはまだすべてのブラウザで正式にサポートされているわけではなく、主にChromeとEdgeのみです（2025年5月時点）。しかし、`WebGPURenderer`には、WebGPUが使えないブラウザでは自動的にWebGL 2.0に切り替えて描画する「フォールバック機能」があります。これにより、新しい技術を使いつつ、より多くのユーザー環境で表示できるのが大きなメリットです。

ただし、現状（Three.jsのr176時点）では、`WebGPURenderer`が`WebGLRenderer`より必ずしも高速というわけではなく、特に大量の細かい描画を行う場合など、WebGLの方が良い性能を示すこともあるようです。WebGPU本来の性能を引き出すための最適化は現在も進行中であり、今後のアップデートで改善されていくことが期待されます。

また、Three.jsではWebGPU時代に向けた新しいシェーダー記述方法として「TSL（Three.js Shading Language）」が導入されています。これはJavaScriptを使ってシェーダーの処理を定義できるもので、書いたコードはThree.jsが自動的にWebGPU用のWGSLやWebGL用のGLSLに変換してくれます。これにより、異なるシェーダー言語の違いを意識せずに、単一のコードでWebGPUとWebGLの両方に対応できるようになります。TSLはノードベースで直感的に記述できるという特徴もあります。

まとめると、Three.jsの`WebGPURenderer`は開発が進んでおり、将来性のあるWebGPUを手軽に試せる方法を提供しています。新しいTSLを使えばシェーダー開発も効率的になります。まだ開発途中のため、本格導入前には動作確認が必要ですが、小規模なプロジェクトやプロトタイプで触ってみることで、WebGPU時代の新しい表現力を体験する価値は大きいでしょう。今後のThree.jsの進化に注目していきましょう。

引用元: https://ics.media/entry/250501/



- [お便り投稿フォーム](https://forms.gle/ffg4JTfqdiqK62qf9)

VOICEVOX:ずんだもん
