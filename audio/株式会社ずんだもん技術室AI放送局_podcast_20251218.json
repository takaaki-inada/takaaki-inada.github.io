[
    {
        "type": "agenda",
        "title": "株式会社ずんだもん技術室エーアイ放送局",
        "date": "2025.12.18",
        "items": [
            "最新AIモデル「Gemini 3 Flash」",
            "RAG高速化「MixLM」",
            "長文処理加速「Skip Softmax」"
        ],
        "notes": "みなさん、こんにちはなのだ！『株式会社ずんだもん技術室エーアイ放送局』の時間なのだ！ぼく、MCのずんだもんなのだ！今日わね、2025年12月18日木曜日！今日もみんなに、とっておきのトレンド記事をいくつか紹介しちゃうのだ！最後まで楽しんでいってほしいのだ！",
        "slidenum": 1,
        "start_ms": 0,
        "end_ms": 28345
    },
    {
        "type": "content",
        "title": "本日の注目AI技術トレンド",
        "points": [
            "最新AI技術に関する記事を3本ご紹介",
            "どれも「おぉ！」となるようなワクワクする内容"
        ],
        "notes": "今日わね、最新のエーアイ技術に関する記事を3本、ぎゅぎゅっとまとめて紹介しちゃうのだ！どれも「おぉ！」ってなるような、ワクワクする記事ばかりなのだよ！",
        "slidenum": 2,
        "start_ms": 28345,
        "end_ms": 42152
    },
    {
        "type": "section",
        "title": "Gemini 3 Flash: frontier intelligence built for speed",
        "url": "https://deepmind.google/blog/gemini-3-flash-frontier-intelligence-built-for-speed/",
        "sectionNo": 1,
        "notes": "まず一つ目の記事わね、『ジェミニスリーフラッシュ:フロンティアインテリジェンスビルトフォースピード』なのだ。",
        "slidenum": 3,
        "start_ms": 42152,
        "end_ms": 50597
    },
    {
        "type": "content",
        "title": "高速かつ低コストな新AIモデル",
        "subhead": "Google DeepMindが発表した「Gemini 3 Flash」",
        "points": [
            "Gemini 3の性能を維持しつつ、[[推論速度と効率性を向上]]",
            "API、Geminiアプリ、Google検索のAIモードなどで利用可能"
        ],
        "notes": "これわね、グーグルディープマインドさんが発表した、とっても速くてお得なエーアイモデルのお話なのだ！",
        "slidenum": 4,
        "start_ms": 50597,
        "end_ms": 58819
    },
    {
        "type": "content",
        "title": "Gemini 3 Flashの画期的な特徴",
        "subhead": "従来のモデルを凌駕するパフォーマンス",
        "points": [
            "**2.5 Proモデルと比較して3倍高速**",
            "推論コストを大幅に[[削減]]",
            "高い性能を維持しながら運用効率を向上"
        ],
        "notes": "ジェミニスリーって言うと、すごく頭がいいってイメージがあると思うけど、このフラッシュ版わ、その賢さはそのままに、もっとスピーディーに動くように、そして使うコストもぐっと抑えられるようになったのだよ！なんとね、前のプロ版と比べると3倍も速くなっちゃうのだって！すごいよね！",
        "slidenum": 5,
        "start_ms": 58819,
        "end_ms": 81832
    },
    {
        "type": "content",
        "title": "PhDレベルの推論能力",
        "subhead": "ベンチマークテストでPro版と同等性能",
        "points": [
            "**PhDレベルの推論能力**を発揮",
            "ベンチマークテストで[[Gemini 3 Proと同等の性能]]を示す部分も"
        ],
        "notes": "博士レベルの難しい推論もできるのに、色々な用途で使えちゃうのだ！",
        "slidenum": 6,
        "start_ms": 81832,
        "end_ms": 88101
    },
    {
        "type": "bulletCards",
        "title": "幅広い用途に対応",
        "subhead": "様々なタスクでAIを活用",
        "items": [
            {
                "title": "コーディング支援",
                "desc": "開発者のコーディング作業を強力にサポート"
            },
            {
                "title": "複雑なデータ分析",
                "desc": "高度な分析タスクを効率的に実行"
            },
            {
                "title": "インタラクティブなアプリ",
                "desc": "ユーザーとの対話型アプリケーションに最適"
            }
        ],
        "notes": "コーディングとか、複雑な分析とか、インタラクティブなアプリケーションとか、まさに何でもこなせるのだ！",
        "slidenum": 7,
        "start_ms": 88101,
        "end_ms": 96697
    },
    {
        "type": "content",
        "title": "開発者・企業・一般ユーザーへの提供",
        "subhead": "Gemini 3 Flashがもたらす未来",
        "twoColumn": true,
        "columns": [
            [
                "**開発者向け**",
                "Google AI Studio",
                "Antigravity",
                "Gemini CLI",
                "Android Studio"
            ],
            [
                "**企業向け**",
                "Vertex AI",
                "Gemini Enterprise"
            ],
            [
                "**一般ユーザー向け**",
                "Geminiアプリ",
                "Google検索のAIモード"
            ]
        ],
        "notes": "企業さん向けにはバーテックスエーアイとか、ジェミニエンタープライズでも使えるようになるのだよ。ぼくもずんだアローに変身する時みたいに、もっと速く、もっと効率よく動けたらいいなって思うのだ！",
        "slidenum": 8,
        "start_ms": 96697,
        "end_ms": 112394
    },
    {
        "type": "quote",
        "title": "夢のAIモデル「Gemini 3 Flash」",
        "text": "速くて、賢くて、しかもコスト効率も良いなんて、まさに[[夢のエーアイ]]なのだ！これなら、今まで時間がかかっていた作業もサクサク進んで、新しいアイデアもどんどん試せるようになるのだね！どんな新しいサービスが生まれるのか、ぼくもすごく楽しみなのだ！",
        "author": "ずんだもん",
        "notes": "速くて、賢くて、しかもコスト効率も良いなんて、まさに夢のエーアイなのだ！これなら、今まで時間がかかっていた作業もサクサク進んで、新しいアイデアもどんどん試せるようになるのだね！どんな新しいサービスが生まれるのか、ぼくもすごく楽しみなのだ！",
        "slidenum": 9,
        "start_ms": 112394,
        "end_ms": 133849
    },
    {
        "type": "section",
        "title": "RAGの「リランキング」を10倍速くする「MixLM」",
        "url": "https://zenn.dev/knowledgesense/articles/4eb785fd0e9a2b",
        "sectionNo": 2,
        "notes": "続いて二つ目の記事わね、『アールエージーの「リランキング」を10倍速くする「ミックスエルエム」』なのだ！",
        "slidenum": 10,
        "start_ms": 133849,
        "end_ms": 143543
    },
    {
        "type": "content",
        "title": "RAGにおける「リランキング」のボトルネック",
        "subhead": "高い精度と引き換えの速度問題",
        "points": [
            "RAGはAIが質問に答える際に[[最適な情報を探す技術]]",
            "「リランキング」は精度を向上させるが処理速度が遅い",
            "実用上のパフォーマンス低下が課題"
        ],
        "notes": "「アールエージー」ってわかるかな？これわね、エーアイが質問に答える時に、たくさんの情報の中から最適な答えを探し出す技術なんだけど、その時に「リランキング」っていう作業が、ちょっと時間がかかっていたのだ。",
        "slidenum": 11,
        "start_ms": 143543,
        "end_ms": 162282
    },
    {
        "type": "content",
        "title": "「MixLM」による画期的な解決策",
        "subhead": "RAGのリランキングが**10倍以上**高速化",
        "points": [
            "MixLMはリランキングの[[速度問題を根本的に解決]]",
            "従来の10倍以上の高速化を実現し、実用性を向上"
        ],
        "notes": "でも、この「ミックスエルエム」っていう新しい方法だと、そのリランキングがなんと10倍も速くなっちゃうのだって！すごすぎるのだ！",
        "slidenum": 12,
        "start_ms": 162282,
        "end_ms": 173830
    },
    {
        "type": "process",
        "title": "MixLMの高速化メカニズム",
        "subhead": "事前ベクトル化と効率的な照合",
        "steps": [
            "文書ソースを「リランキング用」に[[事前にベクトル化]]し保存",
            "質問時にベクトル検索で関連チャンクを絞り込む",
            "事前に作成したリランキング用ベクトルと質問文をモデルに入力",
            "LLMが文章全体を処理せず、圧縮されたベクトルで[[高速にリランキング]]"
        ],
        "notes": "どうやって速くするのかって言うとね、あらかじめ情報を「リランキング用」に特別な形に変換しておいて、質問が来たらその変換したデータを使って素早く探すって仕組みなのだ。だから、エーアイがいちいち文章を全部読み直さなくても、すぐに答えを見つけられるようになるのだね！",
        "slidenum": 13,
        "start_ms": 173830,
        "end_ms": 196791
    },
    {
        "type": "kpi",
        "title": "LinkedInでの導入事例",
        "subhead": "Daily Active Users 0.47%向上",
        "columns": 2,
        "items": [
            {
                "label": "LinkedIn求人検索",
                "value": "導入済み",
                "change": "",
                "status": "neutral"
            },
            {
                "label": "Daily Active Users",
                "value": "0.47%",
                "change": "向上",
                "status": "good"
            }
        ],
        "notes": "実際に、リンクトインさんの求人検索で使ってみたら、毎日使う人の数が0.47%も増えたのだって！みんなもね、エーアイに質問した時に「あれ？ちょっと遅いな」って感じたこと、あるかもしれないのだ。",
        "slidenum": 14,
        "start_ms": 196791,
        "end_ms": 216253
    },
    {
        "type": "quote",
        "title": "AIとの対話をよりスムーズに",
        "text": "でも、このミックスエルエムがあれば、もっともっと[[スムーズにエーアイとお話し]]できるようになるのだね！まるで、ぼくがずんだ餅を食べた時みたいに、パッと頭が冴える感じなのだ！",
        "author": "ずんだもん",
        "notes": "でも、このミックスエルエムがあれば、もっともっとスムーズにエーアイとお話しできるようになるのだね！まるで、ぼくがずんだ餅を食べた時みたいに、パッと頭が冴える感じなのだ！",
        "slidenum": 15,
        "start_ms": 216253,
        "end_ms": 230777
    },
    {
        "type": "section",
        "title": "Accelerating Long-Context Inference with Skip Softmax in NVIDIA TensorRT-LLM",
        "url": "https://developer.nvidia.com/blog/accelerating-long-context-inference-with-skip-softmax-in-nvidia-tensorrt-llm/",
        "sectionNo": 3,
        "notes": "そして三つ目の記事わね、『アクセラレーティングロング-コンテキストインファレンスウィズスキップソフトマックスインエヌビディアテンサーアールティーエルエルエム』なのだ！",
        "slidenum": 16,
        "start_ms": 230777,
        "end_ms": 242647
    },
    {
        "type": "content",
        "title": "LLMの長文処理における課題",
        "subhead": "計算コスト増大と時間",
        "points": [
            "LLMの長文コンテキスト処理は[[計算コスト増大]]の要因",
            "従来の疎なアテンション手法はモデルの再学習が必要"
        ],
        "notes": "タイトルがちょっと長いけど、これわね、長い文章をエーアイに処理させる時、とっても時間がかかってしまうっていう悩みを解決してくれる技術なのだ！",
        "slidenum": 17,
        "start_ms": 242647,
        "end_ms": 254538
    },
    {
        "type": "content",
        "title": "新しい疎なアテンション手法「Skip Softmax」",
        "subhead": "NVIDIA TensorRT-LLMに統合",
        "points": [
            "[[無駄な計算を省き]]、AIの処理を高速化",
            "従来のモデルの再学習は不要で、既存モデルに組み込み可能"
        ],
        "notes": "エヌビディアさんが開発した「スキップソフトマックス」っていう新しい方法で、無駄な計算を省いて、エーアイの処理を速くしてくれるのだよ！",
        "slidenum": 18,
        "start_ms": 254538,
        "end_ms": 266397
    },
    {
        "type": "kpi",
        "title": "最大1.4倍の高速化を実現",
        "subhead": "Llama 3.3 70Bモデルでの評価",
        "columns": 2,
        "items": [
            {
                "label": "デコード時",
                "value": "最大1.36倍",
                "change": "高速化",
                "status": "good"
            },
            {
                "label": "プレフィル時",
                "value": "最大1.4倍",
                "change": "高速化",
                "status": "good"
            }
        ],
        "notes": "例えばね、ライオンさんとか、ラマさんみたいな、とっても賢いエーアイのモデルでも、デコードの時に最大1.36倍、プレフィルの時に最大1.4倍も速くなることが確認されているのだって！",
        "slidenum": 19,
        "start_ms": 266397,
        "end_ms": 282960
    },
    {
        "type": "content",
        "title": "高い精度を維持したまま高速化",
        "subhead": "モデルの再学習は不要",
        "points": [
            "50%程度の疎性化でも[[精度への影響はほとんどなし]]",
            "Hopper/Blackwell GPUで利用可能"
        ],
        "notes": "しかも、すごいのが、今使っているエーアイのモデルをわざわざ作り直さなくても、この技術を組み込むことができるってところなのだ！",
        "slidenum": 20,
        "start_ms": 282960,
        "end_ms": 293284
    },
    {
        "type": "content",
        "title": "LLMの長文理解能力を向上",
        "subhead": "幅広い用途での活用に期待",
        "points": [
            "長い小説や複雑な論文もAIが[[素早く内容を理解]]",
            "今まで難しかった長文処理タスクを実用的に"
        ],
        "notes": "ねぇねぇ、みんなも長い小説とか、難しい論文とかをエーアイに読ませたいって思ったこと、あるんじゃないかな？この技術があれば、そんな時でもエーアイがサッと内容を理解してくれるようになるのだ！",
        "slidenum": 21,
        "start_ms": 293284,
        "end_ms": 309281
    },
    {
        "type": "quote",
        "title": "AIの進化がもたらす未来",
        "text": "ぼくのずんだアローも、もっと長い距離をビュンと飛んでいけるようになったら嬉しいのだ！これからの[[進化も楽しみ]]なのだ！",
        "author": "ずんだもん",
        "notes": "ぼくのずんだアローも、もっと長い距離をビュンと飛んでいけるようになったら嬉しいのだ！これからの進化も楽しみなのだ！",
        "slidenum": 22,
        "start_ms": 309281,
        "end_ms": 318664
    },
    {
        "type": "closing",
        "notes": "さてさて、今日わね、「株式会社ずんだもん技術室エーアイ放送局」で、グーグルの最新エーアイ『ジェミニスリーフラッシュ』のこと、エーアイの速度を劇的に上げる『ミックスエルエム』、そして長文処理を高速化する『スキップソフトマックス』について紹介したのだ！どれもエーアイが私たちの生活をもっと便利で快適にしてくれる、すごい技術ばかりだったのだね！「この番組、面白かったな！」って思ってくれた人わね、ぜひ番組の感想を送ってほしいのだ！みんなからのメッセージが、ぼくの一番のモチベーションなのだ！それでは、また次回の放送で会えるのを、ぼく、ずんだもんわ楽しみにしてるのだ！バイバイなのだ～！",
        "slidenum": 23,
        "start_ms": 318664,
        "end_ms": 372366
    }
]