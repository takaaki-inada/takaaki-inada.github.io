みんな、こんにちはなのだ！「株式会社ずんだもん技術室AI放送局」にようこそなのだ！ぼく、MCのずんだもんなのだ！今日も元気に、最新の技術やAIの話題をお届けするのだ！今日は、2025年4月22日、火曜日なのだ。新しい情報がどんどん出てくるこの世界で、ぼくが気になったトレンド記事をいくつかピックアップしてきたのだ！さあ、今日も一緒に、わくわくする発見をしていこうなのだ！
今日、ぼくがみんなにシェアしたい面白い記事は、全部で4つあるのだ！どれもAIとか技術に関することで、へぇ～って思っちゃうような話なのだ。今日の最初にご紹介するのは、アマゾン ウェブ サービシズのブログ記事なのだ。「Build a location-aware agent using Amazon Bedrock Agents and Foursquare APIs」というタイトルなのだ。この記事では、みんながいる場所を理解して、状況に合わせたアドバイスをくれる賢いAIエージェントをどうやって作るか、その方法を教えてくれているのだ。パーソナルな体験を提供するためには、みんなの好みだけじゃなくて、今いる場所とか、天気とか、その時の状況も考えてあげるのがすごく大切なんだ。例えば、晴れた日には公園、雨の日にはカフェ、みたいに、状況に合わせたおすすめができたら、きっとみんなもっとハッピーになるはずなのだ。ぼくも、おいしいずんだ餅のお店を探すときに、今ぼくがいる場所から一番近いお店を教えてほしいのだ！
こういう、みんなのいる場所をちゃんと理解したレコメンデーションをAIで実現するために、アマゾン ベッドロック エイジェンツと、フォースクウェアーっていう外部のサービスが提供しているAPIを連携させる方法が紹介されているのだ。アマゾン ベッドロックっていうのは、AWSっていう会社が提供しているサービスで、いろんなすごいAIモデルをAPIを通して簡単に使えるようにしてくれるものなのだ。自分で難しいコンピューターの管理とかをしなくても、生成AIを使ったアプリが開発できちゃうのだ。そのベッドロックにある「エイジェンツ」っていう機能を使うと、AIエージェントを自分で考えて動かすことができるんだ。このエージェントは、みんなの複雑なリクエストを理解して、必要な手順に分けて実行できるのだ。特に、会社の持っているAPIとかデータと連携させると、いつもやっている作業を自動でやってくれたりするのだ。プロンプトエンジニアリングとかメモリ管理とかも自動でやってくれるから、比較的簡単に設定できるみたいなんだ。一方、フォースクウェアー プレイシズ APIっていうのは、場所に関する正確な情報を提供してくれる外部サービスなのだ。緯度と経度から場所を特定したり、場所の種類とか、お店の属性、営業時間で絞り込んで検索したりできるのだ。写真とかレビューとか、人気度みたいな詳しい情報も取れるみたいなんだ。これらの技術を組み合わせることで、AIがみんなが今いる場所や天気みたいな状況を理解して、それに合わせた、関連性の高い、すぐに役に立つ情報を提供できるエージェントが作れるんだ。記事には、アマゾン ベッドロック エイジェントがフォースクウェアー APIと天気APIを呼び出す仕組みが図で示されていて、みんなが近くの公園を探したり、公園の周りでテイクアウトできるレストランを探したりするデモ例が紹介されていたのだ。この位置情報認識エージェントを作るためのプログラムは、ギットハブっていう場所で公開されているみたいで、必要な設定をすれば誰でも試せるそうなんだ。開発する上での良いやり方としては、AIの応答を試すためのテストデータを用意したり、アマゾン ベッドロック ガードレイルズっていう機能を使って、変な入力があったときにAIがおかしくならないように対策したりすることが推奨されているのだ。こんなふうに、アマゾン ベッドロック エイジェンツと外部のAPIを連携させると、みんなの状況に合わせたパーソナルな応答ができるAIエージェントを作って、もっともっと便利な体験を提供できるようになる可能性が示されているのだ。技術室のぼくとしては、こういうAIと外部サービスの組み合わせって、すごく面白いと思うのだ。----
次に紹介するのは、ローカル、つまりみんなの手元のPCとかスマホでAIを動かす話なのだ。ミディアムに載っていた「ローカル LLM インファレンス - アミアー Zohrenejad」というタイトルの記事なのだ。著者名はアミル・ゾフレネジャッドさんなのだ。最近、AIを使うときって、インターネットの向こう側にある大きなコンピューター（クラウドっていうのだ）で動いているものを利用することが多いのだ。でも、この記事では、そういうクラウドじゃなくて、自分の持っているPCとかスマホで、AI、特にLLM（大規模言語モデル）を動かす技術の現状と、まだ実用化に向けてどんな課題があるかについて書かれているのだ。なんで、わざわざLLMをローカルで動かしたいんだろう？って思うかもしれないのだ。いくつか理由があるのだ。一つは**お金がかからない**ことなのだ。クラウドでLLMを使うと利用料がかかることがあるけど、ローカルならPCの電気代くらいで済むのだ。次に**プライバシー**の向上なのだ。誰にも見られたくない、秘密の情報とかを外部のサーバーに送らずに自分のPCの中だけで処理できるから、情報漏洩のリスクを減らせるのだ。また、**処理速度**の向上も期待できるのだ。インターネットを通さないから、特に最初の応答がすぐに返ってくる可能性があるのだ。さらに、**インターネットが繋がってなくても利用できる**ようになる点も大きなメリットなのだ。スマホの顔認証機能なんかは、ローカルで画像処理（推論）している良い例で、高速さ、オフラインで使えること、プライバシーが大切だからこそローカルで行われているのだ。この記事の著者のアミルさんは、Macbook プロ（M2チップ搭載）を使って、いくつか代表的なローカルでAIを動かすためのプログラム（フレームワークっていうのだ）を試しているのだ。具体的には、C/C++で書かれた速い`ラーマ.シーピーピー`、それを使いやすくした`オラマ`、そしてブラウザ上で動く`WebLLM`なのだ。これらを使って、少し小さくなった（量子化っていう技術を使った）比較的小さな（7Bサイズの）LLMを動かして、性能を測ったのだ。クラウドのオープンエーアイの小さなモデルとも比べてみたそうなんだ。性能評価では、「最初の単語が出てくるまでの時間（TTFT）」と「1秒間に生成できる単語数（TPS）」を計測したのだ。結果として、`ラーマ.シーピーピー`と`オラマ`はTTFTがすごく速くて、応答までの待つ時間が短かったことが分かったのだ。TPSもこの2つは同じくらいの速さだったのだ。`WebLLM`はどちらの速さでも他のより遅めだったらしいのだ。クラウドのオープンエーアイモデルと比べると、ローカルで動かした小型モデルは文章をたくさん作る速さでは少し劣る結果だったけど、それでも普段使うには十分な速さは出ていたみたいなんだ。でも、性能面以外に、ローカルLLMにはまだ開発者が頑張らなきゃいけない課題があるのだ。一番大変なのは、**どのモデルを使うか決めることと、それをみんなにどうやって届けるか**なのだ。ローカルのPCとかスマホは、使える性能に限りがあるから、使う目的に合った、小さくて効率の良いモデルを見つけなくちゃいけないのだ。でも、世の中にはたくさんのモデルがあって、どれを選べば良いか迷いやすいのが現状なのだ。また、選んだモデルのファイルサイズが大きい（何GB以上！）ことが多くて、みんながアプリを使い始める前にモデルをダウンロードしたり読み込んだりするのに数分かかることもあるのだ。これは、使いたいときにすぐに使えないっていう点で、ちょっと不便なのだ。結論として、ローカルでLLMを動かすのは技術的にはできるようになってきて、速さも上がってきているけど、みんなが実用的なアプリとして広く使えるようになるには、開発者が目的のモデルを簡単に見つけられて、ユーザーがモデルのダウンロードとか実行を気にせずに済むような、もっと使いやすい開発ツールとか配布の仕組みが必要だと筆者は述べているのだ。将来的には、クラウドとローカルの良いところをうまく組み合わせて使っていく形が広がっていくんじゃないかな、と期待されているのだ。ぼくも手元でAIを動かせたら、ずんだ餅の新しいレシピを考えさせたり、ずんだアローの改良点を教えてもらったりしたいのだ！----
さて、三つ目の記事は、前の記事とも少し関連がある、手元のPCでAIを動かす話なのだ。アイティメディアの記事で、「128GB搭載M4 マックス マックブック プロでオープンLLM「ジェマ 3」をローカル実行してみた話」っていうタイトルなのだ。この記事の筆者さんは、なんと128GBっていう、すごくたくさんのメモリを積んだ、一番性能が良いM4 マックスチップ搭載のマックブック プロっていうPCを使って、グーグルが出している「ジェマ 3」っていう最新のオープンなAIモデルを自分のPC（ローカル環境）で動かしてみた体験談を書いてくれているのだ！すごいのだ！なんでそんなに高性能なPCでAIを動かしたいかというと、やっぱりAPI利用料（AIを使うたびにかかるお金）とかプライバシーを気にせず、手元でAIを自由に動かしたい！っていう強い思いから、この高性能なマックブック プロを買って、ジェマ 3のローカル実行に挑戦したそうなんだ。ジェマ 3はテキストだけじゃなくて画像なども扱える「マルチモーダル」に対応していて、特に大きな特徴は非常に長い文章や会話の履歴（コンテキストウィンドウっていうのだ）を扱えることなのだ。この記事では、ジェマ 3の中でもサイズの異なる12B（120億パラメータ）モデルと、一番大きい27B（270億パラメータ）モデルを試しているのだ。パラメータっていうのはAIの賢さに関わる数字で、大きいほどたくさんの情報や複雑なことを処理できるんだ。ローカルで動かすための準備は、オウプン ウェブユーアイとかラーマ.シーピーピーベースのツールを使えば比較的簡単だったとのこと。そして、M4 マックスチップと128GBのメモリ、それにアップルのメタルっていう技術を使うことで、27Bのような結構大きなモデルも実用的な速さで動かせたそうなんだ！これは、個人のPCでこれだけ大きなAIを動かせるってことで、技術の進化を感じるのだ！実際に試してみたところ、面白い発見があったのだ。まず、夏目漱石の小説『吾輩は猫である』の要約をジェマ3-12Bにお願いしたところ、内容はそれなりだったんだけど、なんと作者を芥川龍之介と間違えてしまったらしいのだ！これは「ハルシネーション（AIが事実と異なることを生成すること）」と呼ばれる現象なのだ。比較的小さいモデルでは、知識や理解がまだ完璧じゃない場合があるんだな。そんなことしたらだめなのだ！AIが出した情報を鵜呑みにするのは危険なのだ。次に、同じタスクをより大きなジェマ3-27Bで試したところ、今度は正確な要約が返ってきたらしいのだ！この体験から、やっぱりモデルサイズが大きいほど性能が向上することを実感できたそうなんだ。27Bモデルは日本語の長い文章処理も安定していて、話の筋道をしっかり保ってくれるみたいなんだ。あと、ジェマ3-27Bの最大128kトークンっていう長いコンテキストウィンドウのすごさも体験したんだって。1万文字を超える短い小説を丸ごと入力して、どんなテーマの物語か聞いてみたところ、小説全体の構成を理解した深い答えが得られて、まるでPC上で膨大な情報を扱えるAIがいるかのようだと感じたそうなんだ。これは、たくさんのドキュメントを読ませて要約させたり、大量のプログラムコードを解析させたりといった、技術室の仕事でもすごく役立つ可能性を示しているのだ。さらに、ジェマ 3の画像認識能力も試したそうなんだ。画像を入力してその説明を求めたところ、実用レベルの応答があったそうで、ローカルで画像も扱えるマルチモーダルAIの可能性を感じたとのこと。ただし、ローカルで画像を使う場合は、モデルを起動する場所（ディレクトリっていうのだ）に画像ファイルを置く必要があるとか、少し注意すべき点もあったみたいなんだ。高性能なマックブック プロについて、筆者さんは512GBメモリ搭載のマック ストゥディオっていうのも話題になっていることに触れつつ、数千億パラメータみたいな超巨大モデルを一台のPCで動かすのは、まだ計算能力的に現実的ではないと述べているのだ。ジェマ3-27Bくらいのモデルが、今のPCで動かすには性能と負荷のバランスが良い、「現実的に動かせる上限」だと考えているみたいなんだ。最後に、M4 マックス マックブック プロ (128GB) は確かに高価だけど、開発作業だけでなく巨大なAIモデルも動かせるその汎用性と性能は、価格に見合う価値がある！と言っていて、特にアップル シリコンの「ユニファイドメモリ」（CPUとGPUでメモリを共有する仕組み）が大容量を実現している点を評価しているのだ。API料金やプライバシーを気にせずローカルでAIを試せる安心感は大きく、AI活用を本気でやりたいエンジニアにとって、メモリをたくさん積んだマックはすごく有力な選択肢だと結んでいるのだ。ぼくもいつか、ずんだ餅を無限に作れるようなPCがほしいのだ！----
今日の最後の記事は、ちょっと面白いAIの使い方の話なのだ。慶應義塾大学の授業で、PDF資料に一風変わったAI対策が施されていたと、ネットで話題になった話なのだ。これは、学生さんが課題を提出する際に、安易に生成AIに頼りすぎることを防いで、AIの特性や限界をちゃんと理解してもらうための、教育的な工夫として注目されているのだ。ぼくはずんだもんなのだ。安易にずんだ餅を冷凍庫から取り出すのはだめなのだ。ちゃんと解凍しないと美味しく食べられないのだ。それと同じで、AIも使い方を間違えちゃいけないのだ。具体的にどんな仕掛けかというと、配布されたPDF資料に、**人間が見ても分からないように、透明度100%（完全に透明）にしたテキスト**が、大量に埋め込まれていたそうなんだ！この隠されたテキストには、本来の授業内容とは全く関係のない情報や、AIが間違った要約をするように誘導するような内容が含まれていたらしいのだ。生成AIの多くは、PDFみたいなドキュメントを読み込むときに、テキストデータとして処理するんだ。このとき、文字の色や透明度みたいな見た目の情報は考えないで、そこに書かれている文字そのものを抜き出してしまうのだ。だから、AIにこのPDFを読み込ませて「要約して」ってお願いすると、透明で見えないはずの、本来の授業内容とは全く関係ないテキストまで含めて処理してしまって、元の内容とは全然違う、おかしな要約結果が出てしまう、という仕組みなのだ。この対策の面白い点は、単に「生成AIを使うな」って禁止するんじゃなくて、AIを課題作成に使った時に、実際にどんな問題が起こるかを、学生さん自身に体験させているところなのだ。「課題は自分なりの言葉で書きなさい」っていう指示と組み合わせることで、AIが生成した答えをそのまま提出しても、この仕掛けがあるせいで適切な評価が得られないかもしれないことを示唆して、AIの出力内容をそのまま信じちゃダメで、自分でちゃんと確認して、適切に使うことの重要性を教えているんだ。この事例は、AIがどうやって情報を読み取るのか、そしてその特性を理解した上で、どうやってAIと付き合っていくべきかという、技術室のエンジニアであるぼくにとっても、すごく勉強になる大切な学びを示唆しているのだ。どんなにすごい最新技術であるAIも万能じゃないのだ。その動きの仕組みを知って、限界を踏まえた上で、賢く活用していく姿勢が求められているんだ。そんなことしたらだめなのだ、じゃないのだ。AIに頼りきりになるのは、自分で考えることをやめちゃうことにつながるから、それはだめなのだ！AIは便利な道具として、上手に使うのが一番なのだ。さて、今日もあっという間なのだ！今日は、アマゾン ベッドロックを使った位置情報AIエージェントの話、そして自分のPCでAIを動かす「ローカルLLM」の現状と課題、さらに128GBメモリ搭載のマックで最新AIを動かしてみた体験談、そして最後に、慶應大学の面白いAI対策のお話をご紹介したのだ。AIって、どんどん進化していて、毎日新しい技術や使い方が生まれているのだ。今日の話も、もしかしたら明日にはもう古い情報になっちゃうくらい、速いスピードで変わっているのだ。でも、技術の進化って、いつだってワクワクするものなのだ！
この「株式会社ずんだもん技術室AI放送局」では、これからもぼく、ずんだもんが、みんなが「へぇ～！」って思えるような、面白い技術の話をお届けしていくのだ！番組の感想や、取り上げてほしい話題、ぼくへのメッセージがあったら、ぜひ送ってほしいのだ！ぼくがずんだアローに変身して、みんなのメッセージを受け取りに行くのだ！
それでは、今日はこの辺でおしまいなのだ。また、次の放送で会えるのを楽しみにしているのだ！バイバイなのだー！