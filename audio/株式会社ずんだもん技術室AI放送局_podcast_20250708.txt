みんな、こんにちはなのだ！ぼくはずんだもんだなのだ。「株式会社ずんだもん技術室AI放送局」の時間だよ。今日も、みんなにAIの最新技術や、ぼくたちの世界を楽しくする情報を届けていくのだ。
今日は2025年7月8日、火曜日なのだ。梅雨のじめじめも吹き飛ばすような、とっておきのテクノロジーニュースをたくさん持ってきたのだ！今回はね、最新のAI記事を4本、ぎゅぎゅっとまとめて紹介していくのだ。
どれもとっても興味深い内容だから、最後まで楽しんでくれると嬉しいなのだ！----それじゃあ、さっそく1つ目の記事を紹介するのだ！タイトルは「マーキュリー:ウルトゥラ-ファースト ラングエッジ モデルズ ベイスド on ディフィゥジョン」なのだ。
これはね、新しいタイプのAIモデル「拡散モデル」を使った、すっごく速い大規模言語モデル、LLMの「マーキュリー」のお話なのだ。これまでのLLMって、一つずつ言葉を生成してたんだけど、マーキュリーはなんと、複数の言葉やコードの断片を同時に予測して作れるんだって！これが本当にすごい技術で、信じられないくらい高速に動くらしいのだ。
特に注目されてるのが、プログラミングコードを作ることに特化した「マーキュリー コウダー」というモデルなのだ。このモデル、「ミニ」と「スモール」って2つのサイズがあるんだけど、独立した機関のテストでは、「マーキュリー コウダー ミニ」が1秒間に1109トークン、「マーキュリー コウダー スモール」が1秒間に737トークンも処理したらしいのだ！これって、今ある速いLLMと比べても、なんと平均で最大10倍も速いのに、生成されるコードの品質は同じくらいなんだって！信じられないのだ！しかもね、実際に開発者さんが使ってる「コパイロット アリーナ」っていう評価プラットフォームでも、マーキュリー コウダーは品質で2位、速度では全モデルの中で最速を記録したのだ！論文の中だけじゃなくて、現場でも認められてるってことなのだ。
この「マーキュリー コウダー」は、無料で試せるプレイグラウンドも公開されてるから、新人エンジニアのみんなはぜひ試してみてほしいのだ！コーディングがめちゃくちゃ効率的になるし、AIを使った開発のスピードがぐんと上がる可能性があるのだ。
これはもう、AI開発の未来を大きく変える、まさに「ブレイクスルー」って言ってもいいんじゃないかなって、ぼくは思うのだ！いやー、こんな速いAIがあったら、ぼくもずんだアローでコードを書き換えられちゃうかもなのだ！----続いて2つ目の記事なのだ！タイトルは「LLM インファレンス Benchmarking:パフォーマンス チューニング ウィズ テンサーRT-LLM」なのだ。
みんながLLMを実際に使う時って、どれだけ速く動くか、どれだけたくさんの人からのリクエストを処理できるかって、とっても大事なのだ。ユーザーさんの体験やサービスの効率に、すごく影響するからね。
この記事では、NVIDIAが出してる「テンサーRT-LLM」っていうオープンソースのAI推論エンジンを使って、LLMの性能を最大限に引き出すためのベンチマークとチューニング方法が、新人エンジニアにも分かりやすく解説されてるのだ。
まず、LLMの性能を測るためのツール「`trtllm-ベンチ`」っていうのが紹介されてるのだ。これを使えば、LLMを実際に動かさなくても、モデルの性能を簡単に測って分析できるんだって。
ベンチマークをやるには、GPUの環境を準備して、テスト用のデータを用意するのだ。ベンチマークを実行すると、いろいろな性能のデータが得られるんだけど、特に注目すべきなのが、「リクウェスト スループト（1秒あたりのリクエスト処理数）」とか、「トータル アウトプト スループト（1秒あたりの出力トークン数）」、あとは、ユーザーさんの体験に直結する「アベレイジ タイム-to-ファースト-トウクン（最初のトークンが出るまでの時間）」とか「アベレイジ タイム-パー-アウトプト-トウクン（トークンごとの生成時間）」なのだ。
これらの数字を分析して、自分のアプリケーションに合うように調整するのが、性能を良くするカギなんだって。例えば、記事ではデータの精度をちょっと落とす代わりに処理を速くする「FP8量子化」されたモデルと、標準の「FP16」モデルを比べて、FP8モデルの方がたくさんのユーザーさんを同時に処理できる例が示されてたのだ。
こうやって「`trtllm-ベンチ`」を使えば、いろんな設定を試して、どの設定が一番効率的か、グラフで目で見て確認できるのだ。一番いい設定が見つかったら、それを「`trtllm-サーブ`」っていうツールを使って、LLMを動かすサーバーに適用するのだ。
この「`trtllm-サーブ`」は、オープンエーアイ互換のAPIを提供してるから、調整されたLLMをアプリケーションから簡単に呼んで使えるようになるんだって。テンサーRT-LLMは、LLMの性能を測るところから、一番いい設定で使えるようにするところまで、全部サポートしてくれる強力なツールなのだ。
これを使えば、開発者さんはLLMの性能を最適化して、ユーザーさんに最高の体験を届けられることに集中できるのだ。うーん、技術的な話は奥深いのだ！でも、これって、まさに技術室のぼくたちが日々考えていることにつながる話なのだ！こういうツールで効率が上がるのは、本当に素晴らしいのだ！----さあ、3つ目の記事なのだ！タイトルは「LLMを本番品質に育てるプロンプトオプス：”100回の試行錯誤”を支えた仕組みと文化」なのだ。
この記事はね、株式会社ELYZAさんと株式会社マイナビさんが一緒に開発した「マイナビAI ペンシル」っていうのの開発を通じて分かった、LLMを実際のビジネスで使えるレベルにするための「プロンプトエンジニアリング」と、それを支える「プロンプトオプス」っていう仕組みについて解説されてるのだ。
LLMって、とっても賢いんだけど、その出力って、そのまま本番で使うにはちょっと物足りないことがあるのだ。期待通りの品質をいつも安定して出すためには、細かい調整がどうしても必要になるのだ。
この「なんか微妙」っていう課題を、具体的な指示に変えてプロンプト、つまりLLMへのお願いする文章を磨き上げていく作業が「プロンプトエンジニアリング」なのだ。例えば、ユーザーさんの自己PR文章を作る時に、もっと魅力的にするために、100パターンもプロンプトを試したりするような、地道な努力が必要になるんだって！でもね、こういうプロンプトを調整するスキルが、特定の人だけの「職人技」になっちゃうと、なかなか改善が進まなかったり、せっかくの知識がチームに広まらなかったりする問題が起きちゃうのだ。
そこで大事になるのが、「プロンプトオプス」っていう仕組みなのだ。これは、プロンプトエンジニアリングを組織全体でやる活動として、継続的に良くしていくための運用基盤のことなんだって。
ELYZAさんでは、プロンプトオプスを実現するために、こんなことをしてるのだ。1つ目は、エンジニアさんだけじゃなくて、プロジェクトマネージャーさんとか、ビジネスサイドの人たちも、プロンプトのレビューや改善に加わる「開かれたプロンプト改善」なのだ。
プロンプトの質って、技術的な正しさだけじゃなくて、お客さんの業務や業界の知識もすっごく大事だから、みんなで協力するんだって。2つ目は、「バージョン管理」なのだ。ソフト開発で使う「Semヴァー（セマンティックバージョニング）」っていうルールを参考にして、プロンプトにもバージョン番号を付けて、「いつ、誰が、どうして」変えたかを記録するのだ。
これですごく分かりやすくなるし、前の良いプロンプトもチームの「財産」として使えるようになるのだ。3つ目は、「客観的なデータに基づく実験管理と高速な改善サイクル」なのだ。
勘じゃなくて、データに基づいてプロンプトの性能を評価するのだ。ちょっとのデータで試す「クイック改善」、もっとたくさんのデータで評価する「定性・定量評価」、そしてお客さんに実際の出力を確認してもらう「顧客レビュー」っていうサイクルを早く回して、一番いいプロンプトを見つけるのだ。
これらの仕組みのおかげで、プロンプト管理の混乱がなくなって、いろんな職種の人が協力して開発できるようになったんだって。今後は、手作業を自動化したり、最先端のツールを使ったりして、もっと開発を効率良くしていくらしいのだ。
新人エンジニアのみんなも、LLMを使った開発をする時は、プロンプトをソフトみたいにバージョン管理して、変更履歴をしっかり残すこと、それから長いプロンプトを管理する時は「TOML」っていう形式が読み書きしやすくて便利だよ、っていうのをぜひ参考にしてみてほしいのだ！ぼくも技術室でプロンプトをいじる時は、気をつけなくちゃなのだ！----さあ、いよいよ最後の記事なのだ！タイトルは「自分の10年越えツイッターログが超記憶として対話可能に！Twilog専用MCPサーバーが使えるようになりました。
」なのだ。これはね、みんなも知ってるTwilogが、初めてのAI機能「Twilog専用MCPサーバー」をリリースしたっていうニュースなのだ！MCPっていうのは、AIが外部のサービスデータ、この場合はTwilogのX（旧ツイッター）のログを、自分で見て、使えるようにする仕組みなんだって。
これによって、みんなが過去にXに投稿した内容とか、いいねした投稿、ブックマークした投稿っていう、膨大なログをAIが読み込んで、自然な言葉で質問できるようになるんだって！例えば、「去年のラーメンの投稿をまとめて」って聞けば、AIが自動でログの中からラーメンの投稿を探して、まとめてくれるらしいのだ。
すごいのだ！Twilogが、みんなの「超記憶装置」になって、ログの中から必要な情報を簡単に引き出せる、新しい体験を提供してくれるってことなのだ。これって、昔の投稿を振り返るのがもっと楽しくなるし、あの時のあの情報どこだっけ？って時にパッと見つけられるようになるってことなのだ。
ぼくも、ずんだ餅を食べた記録とか、ずんだアローで変身した時の思い出とかを全部記録して、後からAIに聞きたいのだ！きっと楽しいのだ！----さて、あっという間にエンディングの時間なのだ。
今日は、超高速LLMの「マーキュリー」や、LLMの推論性能を最適化する「テンサーRT-LLM」、LLMを本番品質にするための「プロンプトオプス」、そしてみんなのXログを「超記憶」にしてくれる「Twilog専用MCPサーバー」について紹介したのだ。
どれも、今のAI技術の進化を物語る、わくわくするような話だったのだ！みんなの毎日を、もっと楽しく、もっと便利にするヒントが見つかったら嬉しいなのだ！「株式会社ずんだもん技術室AI放送局」では、番組の感想や、ずんだもんへの質問、こんな話題を扱ってほしいな、っていうリクエストも募集しているのだ。
ぼく、ずんだもんが、みんなからのお便りを首を長くして待ってるのだ！それでは、またこの番組で会えるのを楽しみにしてるのだ！今日も聞いてくれて、本当にありがとうなのだ！ずんだもんがお届けしました！バイバイなのだ！
